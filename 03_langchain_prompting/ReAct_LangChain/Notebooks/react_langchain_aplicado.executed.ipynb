{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479d4f61",
   "metadata": {},
   "source": [
    "# ReAct con LangChain (Avanzado)\n",
    "\n",
    "## Definición\n",
    "ReAct combina razonamiento + acción: el agente decide, ejecuta herramientas y usa observaciones para el siguiente paso.\n",
    "\n",
    "## Cuándo usarlo\n",
    "- Cuando necesitas herramientas reales (auditoría, búsqueda, validación).\n",
    "- Cuando la trazabilidad de pasos es requisito de negocio.\n",
    "\n",
    "## Cuándo NO usarlo\n",
    "- Si no hay tools que agreguen valor.\n",
    "- Si una llamada directa ya cumple calidad/costo/latencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb53ce1",
   "metadata": {},
   "source": [
    "## Flujo conceptual\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "A[State] --> B[Decide Action]\n",
    "B --> C[ANALIZAR_PERFIL]\n",
    "C --> D[GENERAR_MENSAJE]\n",
    "D --> E[AUDITAR_RESPETO]\n",
    "E --> F[FINAL_ANSWER]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bd899d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:04:57.022200Z",
     "iopub.status.busy": "2026-02-10T11:04:57.022079Z",
     "iopub.status.idle": "2026-02-10T11:04:57.031465Z",
     "shell.execute_reply": "2026-02-10T11:04:57.031036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/carlosdaniel/Documents/Projects/labor_projects/Henry/2026/01-introduction_ai_engineering/ai_engineering_henry\n"
     ]
    }
   ],
   "source": [
    "# Setup de notebook\n",
    "from __future__ import annotations\n",
    "\n",
    "import importlib.util\n",
    "import inspect\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def find_repo_root(start: Path) -> Path:\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "    raise RuntimeError(\"No se encontro raiz del repo\")\n",
    "\n",
    "\n",
    "def load_module(path: Path, module_name: str):\n",
    "    spec = importlib.util.spec_from_file_location(module_name, path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    if spec is None or spec.loader is None:\n",
    "        raise RuntimeError(f\"No se pudo cargar modulo: {path}\")\n",
    "    sys.modules[module_name] = module\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "ROOT = find_repo_root(Path.cwd())\n",
    "print('Repo root:', ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b6c484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:04:57.033167Z",
     "iopub.status.busy": "2026-02-10T11:04:57.033075Z",
     "iopub.status.idle": "2026-02-10T11:04:57.335907Z",
     "shell.execute_reply": "2026-02-10T11:04:57.335410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script: /Users/carlosdaniel/Documents/Projects/labor_projects/Henry/2026/01-introduction_ai_engineering/ai_engineering_henry/03_langchain_prompting/ReAct_LangChain/Notebooks/01_react_langchain_avanzado.py\n",
      "Firma: (profile: 'dict | None' = None, verbose: 'bool' = True) -> 'dict'\n"
     ]
    }
   ],
   "source": [
    "script = ROOT / '03_langchain_prompting/ReAct_LangChain/Notebooks/01_react_langchain_avanzado.py'\n",
    "module = load_module(script, 'react_langchain_nb')\n",
    "run_fn = module.run_react_langchain\n",
    "print('Script:', script)\n",
    "print('Firma:', inspect.signature(run_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982adbce",
   "metadata": {},
   "source": [
    "## Código fuente principal (visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d64eec02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:04:57.336824Z",
     "iopub.status.busy": "2026-02-10T11:04:57.336759Z",
     "iopub.status.idle": "2026-02-10T11:04:57.339535Z",
     "shell.execute_reply": "2026-02-10T11:04:57.339254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def run_react_langchain(profile: dict | None = None, verbose: bool = True) -> dict:\n",
      "    root = find_repo_root(Path.cwd())\n",
      "    load_dotenv(root / \".env\")\n",
      "\n",
      "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "    if not api_key:\n",
      "        raise RuntimeError(\"OPENAI_API_KEY no esta definida en .env\")\n",
      "\n",
      "    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
      "    build_context_packet = load_context_builder(root)\n",
      "    llm = ChatOpenAI(model=model, temperature=0.4, api_key=api_key)\n",
      "\n",
      "    if profile is None:\n",
      "        profile = {\n",
      "            \"tipo_persona\": \"arquitecta apasionada por fotografia urbana\",\n",
      "            \"gustos\": [\"cafes tranquilos\", \"jazz\", \"viajes cortos\", \"cafes tranquilos\"],\n",
      "            \"estilo\": \"intelectual pero relajado\",\n",
      "            \"contexto\": \"match reciente, primera interaccion\",\n",
      "        }\n",
      "\n",
      "    context_packet = build_context_packet(\n",
      "        profile=profile,\n",
      "        module=\"03_langchain_prompting\",\n",
      "        strategy=\"react_langchain\",\n",
      "    )\n",
      "\n",
      "    analizar_tool = StructuredTool.from_function(\n",
      "        func=tool_analizar_perfil,\n",
      "        name=\"ANALIZAR_PERFIL\",\n",
      "        description=\"Extrae insights accionables desde un context packet JSON.\",\n",
      "    )\n",
      "    mensaje_tool = StructuredTool.from_function(\n",
      "        func=tool_generar_mensaje,\n",
      "        name=\"GENERAR_MENSAJE\",\n",
      "        description=\"Genera opener y follow_up a partir de analysis JSON.\",\n",
      "    )\n",
      "    auditar_tool = StructuredTool.from_function(\n",
      "        func=tool_auditar_respeto,\n",
      "        name=\"AUDITAR_RESPETO\",\n",
      "        description=\"Audita respeto y ausencia de presion.\",\n",
      "    )\n",
      "\n",
      "    tool_map = {\n",
      "        \"ANALIZAR_PERFIL\": analizar_tool,\n",
      "        \"GENERAR_MENSAJE\": mensaje_tool,\n",
      "        \"AUDITAR_RESPETO\": auditar_tool,\n",
      "    }\n",
      "\n",
      "    decision_prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            (\n",
      "                \"system\",\n",
      "                \"Eres un agente ReAct disciplinado. Decide una accion por iteracion y respeta el protocolo canonico.\",\n",
      "            ),\n",
      "            (\n",
      "                \"human\",\n",
      "                \"Estado actual del agente:\\n{state_json}\\n\\n\"\n",
      "                \"Acciones validas: ANALIZAR_PERFIL, GENERAR_MENSAJE, AUDITAR_RESPETO, FINAL_ANSWER. \"\n",
      "                \"Devuelve thought, action y action_input.\",\n",
      "            ),\n",
      "        ]\n",
      "    )\n",
      "    decision_chain = decision_prompt | llm.with_structured_output(\n",
      "        AgentStep,\n",
      "        method=\"function_calling\",\n",
      "    )\n",
      "\n",
      "    state = {\n",
      "        \"context_packet\": context_packet,\n",
      "        \"analysis\": None,\n",
      "        \"draft\": None,\n",
      "        \"audit\": None,\n",
      "        \"trace\": [],\n",
      "    }\n",
      "\n",
      "    expected_sequence = [\"ANALIZAR_PERFIL\", \"GENERAR_MENSAJE\", \"AUDITAR_RESPETO\", \"FINAL_ANSWER\"]\n",
      "\n",
      "    for idx in range(6):\n",
      "        step = decision_chain.invoke({\"state_json\": json.dumps(state, ensure_ascii=False, indent=2)})\n",
      "        expected_action = expected_sequence[min(idx, len(expected_sequence) - 1)]\n",
      "        chosen_action = step.action\n",
      "\n",
      "        if chosen_action != expected_action:\n",
      "            state[\"trace\"].append(\n",
      "                {\n",
      "                    \"thought\": step.thought,\n",
      "                    \"action\": f\"override:{chosen_action}->{expected_action}\",\n",
      "                    \"observation\": \"Guardrail aplicado para mantener orden canonico.\",\n",
      "                }\n",
      "            )\n",
      "            chosen_action = expected_action\n",
      "\n",
      "        if chosen_action == \"ANALIZAR_PERFIL\":\n",
      "            observation = tool_map[chosen_action].invoke(\n",
      "                {\"context_packet_json\": json.dumps(context_packet, ensure_ascii=False)}\n",
      "            )\n",
      "            state[\"analysis\"] = json.loads(observation)\n",
      "        elif chosen_action == \"GENERAR_MENSAJE\":\n",
      "            observation = tool_map[chosen_action].invoke(\n",
      "                {\"analysis_json\": json.dumps(state[\"analysis\"], ensure_ascii=False)}\n",
      "            )\n",
      "            state[\"draft\"] = json.loads(observation)\n",
      "        elif chosen_action == \"AUDITAR_RESPETO\":\n",
      "            observation = tool_map[chosen_action].invoke(\n",
      "                {\"message_json\": json.dumps(state[\"draft\"], ensure_ascii=False)}\n",
      "            )\n",
      "            state[\"audit\"] = json.loads(observation)\n",
      "        else:\n",
      "            state[\"trace\"].append(\n",
      "                {\n",
      "                    \"thought\": step.thought,\n",
      "                    \"action\": \"FINAL_ANSWER\",\n",
      "                    \"observation\": \"Listo para responder.\",\n",
      "                }\n",
      "            )\n",
      "            break\n",
      "\n",
      "        state[\"trace\"].append(\n",
      "            {\n",
      "                \"thought\": step.thought,\n",
      "                \"action\": chosen_action,\n",
      "                \"observation\": observation,\n",
      "            }\n",
      "        )\n",
      "\n",
      "    final_prompt = ChatPromptTemplate.from_messages(\n",
      "        [\n",
      "            (\"system\", \"Genera la respuesta final usando el estado consolidado del agente.\"),\n",
      "            (\n",
      "                \"human\",\n",
      "                \"Estado consolidado:\\n{state_json}\\n\\n\"\n",
      "                \"Devuelve opener, follow_up, why_it_works y trace_summary.\",\n",
      "            ),\n",
      "        ]\n",
      "    )\n",
      "    final_chain = final_prompt | llm.with_structured_output(\n",
      "        FinalAnswer,\n",
      "        method=\"function_calling\",\n",
      "    )\n",
      "    final_answer = final_chain.invoke({\"state_json\": json.dumps(state, ensure_ascii=False, indent=2)})\n",
      "\n",
      "    flow_mermaid = (\n",
      "        \"graph TD\\n\"\n",
      "        \"A[State] --> B[Decide Action]\\n\"\n",
      "        \"B --> C[ANALIZAR_PERFIL]\\n\"\n",
      "        \"C --> D[GENERAR_MENSAJE]\\n\"\n",
      "        \"D --> E[AUDITAR_RESPETO]\\n\"\n",
      "        \"E --> F[FINAL_ANSWER]\"\n",
      "    )\n",
      "\n",
      "    trace_preview = [\n",
      "        {\n",
      "            \"action\": step[\"action\"],\n",
      "            \"observation_excerpt\": str(step[\"observation\"])[:140],\n",
      "        }\n",
      "        for step in state[\"trace\"]\n",
      "    ]\n",
      "    payload = {\n",
      "        \"__model\": model,\n",
      "        \"__context_hash\": context_packet[\"context_hash\"],\n",
      "        \"__flow_mermaid\": flow_mermaid,\n",
      "        \"context_packet\": context_packet,\n",
      "        \"state\": state,\n",
      "        \"final_answer\": final_answer.model_dump(),\n",
      "        \"trace_preview\": trace_preview,\n",
      "    }\n",
      "\n",
      "    if verbose:\n",
      "        print(\"=\" * 80)\n",
      "        print(f\"Modelo: {model}\")\n",
      "        print(\"Context packet aplicado (context engineering):\")\n",
      "        print(json.dumps(context_packet, ensure_ascii=False, indent=2))\n",
      "\n",
      "        print(\"\\nFinal answer:\")\n",
      "        print(final_answer.model_dump_json(indent=2, ensure_ascii=False))\n",
      "        print(\"\\nTrace preview:\")\n",
      "        print(json.dumps(trace_preview, ensure_ascii=False, indent=2))\n",
      "\n",
      "        print(\"\\n[Autocritica]\")\n",
      "        print(\"- El guardrail de secuencia mejora predictibilidad y reduce deriva del agente.\")\n",
      "        print(\"- El agente depende de la calidad del context packet (GIGO sigue aplicando).\")\n",
      "        print(\"- Estas tools son pedagogicas; en produccion hay que endurecer validaciones y timeouts.\")\n",
      "\n",
      "    return payload\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(run_fn)[:7000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a580eb",
   "metadata": {},
   "source": [
    "## Ejemplo 1: caso base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842f376a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:04:57.340379Z",
     "iopub.status.busy": "2026-02-10T11:04:57.340329Z",
     "iopub.status.idle": "2026-02-10T11:05:06.817201Z",
     "shell.execute_reply": "2026-02-10T11:05:06.816350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: gpt-4o-mini\n",
      "Context hash: 12995565f4f0\n",
      "Final answer:\n",
      "{\n",
      "  \"opener\": \"Vi que te mueves en arquitecta apasionada por fotografia urbana, que tema te entusiasma tanto que podrias hablar horas sin aburrirte?\",\n",
      "  \"follow_up\": \"Si te parece, yo comparto uno y comparamos notas.\",\n",
      "  \"why_it_works\": [\n",
      "    \"Conecta con identidad e intereses.\",\n",
      "    \"Mantiene tono curioso y respetuoso.\"\n",
      "  ],\n",
      "  \"trace_summary\": [\n",
      "    \"Analizar el perfil de la persona para entender sus intereses.\",\n",
      "    \"Generar un mensaje atractivo acorde a sus intereses y estilo.\",\n",
      "    \"Auditar el mensaje para asegurar que respeta el contexto y los intereses de la persona.\"\n",
      "  ]\n",
      "}\n",
      "Trace preview:\n",
      "[\n",
      "  {\n",
      "    \"action\": \"ANALIZAR_PERFIL\",\n",
      "    \"observation_excerpt\": \"{\\\"persona\\\": \\\"arquitecta apasionada por fotografia urbana\\\", \\\"estilo_preferido\\\": \\\"intelectual pero relajado\\\", \\\"insights\\\": [\\\"Intereses detectad\"\n",
      "  },\n",
      "  {\n",
      "    \"action\": \"GENERAR_MENSAJE\",\n",
      "    \"observation_excerpt\": \"{\\\"opener\\\": \\\"Vi que te mueves en arquitecta apasionada por fotografia urbana, que tema te entusiasma tanto que podrias hablar horas sin aburr\"\n",
      "  },\n",
      "  {\n",
      "    \"action\": \"AUDITAR_RESPETO\",\n",
      "    \"observation_excerpt\": \"{\\\"ok\\\": true, \\\"flags\\\": [], \\\"suggestion\\\": \\\"Mantener pregunta abierta y evitar intensidad prematura.\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"action\": \"FINAL_ANSWER\",\n",
      "    \"observation_excerpt\": \"Listo para responder.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "result_1 = run_fn(verbose=False)\n",
    "print('Modelo:', result_1['__model'])\n",
    "print('Context hash:', result_1['__context_hash'])\n",
    "print('Final answer:')\n",
    "print(json.dumps(result_1['final_answer'], ensure_ascii=False, indent=2))\n",
    "print('Trace preview:')\n",
    "print(json.dumps(result_1['trace_preview'], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc15d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:05:06.820842Z",
     "iopub.status.busy": "2026-02-10T11:05:06.820455Z",
     "iopub.status.idle": "2026-02-10T11:05:06.826439Z",
     "shell.execute_reply": "2026-02-10T11:05:06.825926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Flujo real usado en ejecución"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```mermaid\n",
       "graph TD\n",
       "A[State] --> B[Decide Action]\n",
       "B --> C[ANALIZAR_PERFIL]\n",
       "C --> D[GENERAR_MENSAJE]\n",
       "D --> E[AUDITAR_RESPETO]\n",
       "E --> F[FINAL_ANSWER]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown('## Flujo real usado en ejecución'))\n",
    "display(Markdown('```mermaid\\n' + result_1['__flow_mermaid'] + '\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebc990",
   "metadata": {},
   "source": [
    "## Ejemplo 2: coqueteo (asistente latino experto en enamorar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9fdc50a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-10T11:05:06.828030Z",
     "iopub.status.busy": "2026-02-10T11:05:06.827906Z",
     "iopub.status.idle": "2026-02-10T11:05:15.914251Z",
     "shell.execute_reply": "2026-02-10T11:05:15.913519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context hash coqueteo: 6f918a97a9d4\n",
      "Final answer coqueteo:\n",
      "{\n",
      "  \"opener\": \"Vi que te mueves en asistente latino experto en enamorar con elegancia, que tema te entusiasma tanto que podrias hablar horas sin aburrirte?\",\n",
      "  \"follow_up\": \"Si te parece, yo comparto uno y comparamos notas.\",\n",
      "  \"why_it_works\": [\n",
      "    \"Conecta con identidad e intereses.\",\n",
      "    \"Mantiene tono curioso y respetuoso.\"\n",
      "  ],\n",
      "  \"trace_summary\": [\n",
      "    \"Analizar el perfil del agente para entender su contexto y gustos.\",\n",
      "    \"Generar un mensaje de coqueteo carismático y auténtico.\",\n",
      "    \"Auditar el respeto en el mensaje para asegurar que cumple con el estilo deseado.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "profile_coqueteo = {\n",
    "  \"tipo_persona\": \"asistente latino experto en enamorar con elegancia\",\n",
    "  \"gustos\": [\n",
    "    \"salsa romantica\",\n",
    "    \"cafes bohemios\",\n",
    "    \"poesia urbana\",\n",
    "    \"paseos nocturnos\"\n",
    "  ],\n",
    "  \"estilo\": \"carismatico, coqueto, respetuoso, humor picante sutil\",\n",
    "  \"contexto\": \"quiere iniciar una conversacion de coqueteo sin frases prefabricadas\"\n",
    "}\n",
    "result_2 = run_fn(profile=profile_coqueteo, verbose=False)\n",
    "print('Context hash coqueteo:', result_2['__context_hash'])\n",
    "print('Final answer coqueteo:')\n",
    "print(json.dumps(result_2['final_answer'], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13db3f0b",
   "metadata": {},
   "source": [
    "## Errores típicos en producción\n",
    "- Tools con contratos ambiguos: el agente se vuelve impredecible.\n",
    "- Sin guardrails de secuencia: deriva de acciones y debugging costoso.\n",
    "- Sin auditoría de respeto: riesgo reputacional en outputs sensibles.\n",
    "- Sobrecargar de herramientas irrelevantes: más latencia, menos precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd571ba2",
   "metadata": {},
   "source": [
    "## Autocrítica\n",
    "- ReAct aporta control operativo, pero cuesta más que CoT simple.\n",
    "- La calidad sigue dependiendo del contexto; herramientas malas no rescatan un mal diseño."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
