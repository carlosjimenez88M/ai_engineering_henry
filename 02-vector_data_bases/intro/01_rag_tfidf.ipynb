{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a6c3157",
   "metadata": {},
   "source": "![Henry Logo](https://www.soyhenry.com/_next/static/media/HenryLogo.bb57fd6f.svg)\n\n# De Texto Completo a Agentic RAG: Busqueda Semantica sobre Don Quijote\n\n---\n\n**Curso:** AI Engineering | **Modulo:** Bases de Datos Vectoriales\n\n---\n\n## Objetivos de Aprendizaje\n\nAl finalizar este notebook, seras capaz de:\n\n1. **Entender** por que existen las bases de datos vectoriales (limites de context stuffing).\n2. **Explicar** TF-IDF: formula, intuicion y calculo manual.\n3. **Comparar** representaciones sparse (TF-IDF) vs dense (embeddings de OpenAI).\n4. **Usar ChromaDB** como base de datos vectorial real con embeddings densos.\n5. **Implementar** un pipeline Simple RAG con retrieval semantico.\n6. **Construir** un Agentic RAG con LangGraph (grading + re-query loops).\n7. **Evaluar** cuantitativamente los 3 enfoques sobre un corpus real (~15K palabras).\n\n---\n\n### Resumen de los 3 Enfoques\n\n| Enfoque | Idea central | Tokens de contexto | Costo aprox. por query |\n|---------|-------------|-------------------|----------------------|\n| **Context Stuffing** | Pasar TODO el texto al LLM | ~20,000 | ~$0.003 |\n| **Simple RAG** | Retrieval + top-3 chunks | ~500 | ~$0.0001 |\n| **Agentic RAG** | LangGraph con reasoning loops | ~500-1500 | ~$0.0005 |\n\n> **Nota:** Este notebook usa la API de OpenAI. Se requiere `OPENAI_API_KEY` configurada."
  },
  {
   "cell_type": "markdown",
   "id": "d6aabd2d",
   "metadata": {},
   "source": [
    "## Reproducibilidad\n",
    "\n",
    "```bash\n",
    "cd 02-vector_data_bases\n",
    "uv sync                    # instala dependencias en .venv/\n",
    "cp ../.env .env            # o configurar OPENAI_API_KEY directamente\n",
    "```\n",
    "\n",
    "Abrir esta notebook en VS Code (el `.vscode/settings.json` apunta al venv correcto) o ejecutar:\n",
    "\n",
    "```bash\n",
    "make run-notebook          # ejecucion headless\n",
    "```\n",
    "\n",
    "**Dependencias clave:** `chromadb`, `langchain-openai`, `langgraph`, `scikit-learn`, `tiktoken`, `matplotlib`, `seaborn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SETUP: Importaciones y configuracion del entorno\n",
    "# ==============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import time\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from typing import TypedDict\n",
    "\n",
    "import chromadb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tiktoken\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import END, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- Configuracion visual ---\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "\n",
    "# --- Cargar API key ---\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# --- Modelos ---\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489448bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CORPUS: Descargar Don Quijote de Project Gutenberg (capitulos I-VIII)\n",
    "# ==============================================================================\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/2000/pg2000.txt\"\n",
    "print(f\"Descargando desde {url} ...\")\n",
    "\n",
    "raw = urllib.request.urlopen(url).read().decode(\"utf-8\")\n",
    "print(f\"Texto completo descargado: {len(raw):,} caracteres\")\n",
    "\n",
    "# --- Extraer capitulos I-VIII ---\n",
    "# Buscamos desde \"En un lugar de la Mancha\" hasta el inicio del capitulo IX\n",
    "start_match = re.search(r\"En un lugar de la Mancha\", raw)\n",
    "end_match = re.search(r\"Cap[ií]tulo\\s+(IX|noveno|9)\", raw, re.IGNORECASE)\n",
    "\n",
    "if start_match and end_match:\n",
    "    texto_quijote = raw[start_match.start() : end_match.start()].strip()\n",
    "else:\n",
    "    # Fallback: tomar desde \"En un lugar\" + ~80K caracteres\n",
    "    start = raw.find(\"En un lugar de la Mancha\")\n",
    "    texto_quijote = raw[start : start + 80000].strip()\n",
    "\n",
    "# --- Limpieza basica ---\n",
    "texto_quijote = re.sub(r\"\\r\\n\", \"\\n\", texto_quijote)\n",
    "texto_quijote = re.sub(r\"\\n{3,}\", \"\\n\\n\", texto_quijote)\n",
    "\n",
    "# --- Estadisticas ---\n",
    "palabras = texto_quijote.split()\n",
    "tokens = enc.encode(texto_quijote)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CORPUS: Don Quijote, Capitulos I-VIII\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Palabras:       {len(palabras):,}\")\n",
    "print(f\"Tokens:         {len(tokens):,}\")\n",
    "print(f\"Caracteres:     {len(texto_quijote):,}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nPreview (primeros 300 chars):\")\n",
    "print(f\"  {texto_quijote[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f1957",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 1: El Problema -- Context Stuffing\n",
    "\n",
    "### Por que necesitamos bases de datos vectoriales?\n",
    "\n",
    "Cuando un texto es largo, pasar TODO el contenido como contexto al LLM tiene problemas serios:\n",
    "\n",
    "1. **Costo en tokens**: Cada query cuesta O(|T|) tokens de input. Con gpt-4o-mini a $0.15/1M input tokens, un texto de 20K tokens cuesta ~$0.003 por query. Parece poco, pero a 1000 queries/dia = **$3/dia solo en contexto**.\n",
    "\n",
    "2. **Latencia**: Mas tokens = mas tiempo de procesamiento. La latencia crece linealmente.\n",
    "\n",
    "3. **\"Lost in the middle\"** (Liu et al., 2023): Los LLMs pierden informacion que esta en la mitad del contexto largo. Solo atienden bien al principio y al final.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│           CONTEXT STUFFING: ~20K tokens             │\n",
    "│                                                     │\n",
    "│  Query ──► [TODO el texto de 8 capitulos] ──► LLM   │\n",
    "│                                                     │\n",
    "│  Problema: caro, lento, pierde info del medio       │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Pregunta clave**: Y si pudieramos encontrar SOLO los parrafos relevantes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33a9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONTEXT STUFFING: Pasar TODO el texto como contexto al LLM\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def context_stuffing_answer(query: str, context: str) -> dict:\n",
    "    \"\"\"Pasa todo el texto como contexto al LLM y mide tokens/costo/latencia.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    prompt = (\n",
    "        f\"Contexto completo:\\n{context}\\n\\n\"\n",
    "        f\"Pregunta: {query}\\n\\n\"\n",
    "        \"Responde de forma concisa basandote unicamente en el contexto.\"\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    latency_s = time.perf_counter() - t0\n",
    "\n",
    "    # Conteo real de tokens con tiktoken\n",
    "    input_tokens = len(enc.encode(prompt))\n",
    "    output_tokens = len(enc.encode(response.content))\n",
    "\n",
    "    # Costos gpt-4o-mini: $0.15/1M input, $0.60/1M output\n",
    "    cost_input = input_tokens * 0.15 / 1_000_000\n",
    "    cost_output = output_tokens * 0.60 / 1_000_000\n",
    "    cost_total = cost_input + cost_output\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": response.content,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"cost_usd\": cost_total,\n",
    "        \"latency_s\": latency_s,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Test ---\n",
    "test_query = \"Que piensa Cervantes sobre los libros de caballeria?\"\n",
    "cs_result = context_stuffing_answer(test_query, texto_quijote)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CONTEXT STUFFING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query:          {cs_result['query']}\")\n",
    "print(f\"Answer:         {cs_result['answer'][:200]}...\")\n",
    "print(f\"Input tokens:   {cs_result['input_tokens']:,}\")\n",
    "print(f\"Output tokens:  {cs_result['output_tokens']:,}\")\n",
    "print(f\"Costo:          ${cs_result['cost_usd']:.6f}\")\n",
    "print(f\"Latencia:       {cs_result['latency_s']:.1f}s\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Proyeccion\n",
    "daily_cost = cs_result[\"cost_usd\"] * 1000\n",
    "print(\"\\nProyeccion: Si haces 1,000 queries/dia:\")\n",
    "print(f\"  Costo diario:  ${daily_cost:.2f}\")\n",
    "print(f\"  Costo mensual: ${daily_cost * 30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918581ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Resultado**: Funciona, pero gasta ~20K tokens por query solo en contexto. Necesitamos una forma de **convertir texto en numeros** para poder **buscar** los fragmentos relevantes en vez de pasar todo.\n",
    "\n",
    "Eso es exactamente lo que hace **TF-IDF** (y despues, los **embeddings densos**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2c1bd3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 2: TF-IDF -- De Texto a Numeros\n",
    "\n",
    "### Intuicion\n",
    "\n",
    "Una palabra es **importante en un documento** si:\n",
    "- Es **frecuente** en ese documento (Term Frequency)\n",
    "- Es **rara** en el resto del corpus (Inverse Document Frequency)\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{veces que } t \\text{ aparece en } d}{\\text{total de palabras en } d}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{\\text{docs que contienen } t}\\right)\n",
    "$$\n",
    "\n",
    "### Ejemplo Conceptual\n",
    "\n",
    "| Palabra | TF (en cap. 1) | IDF (8 capitulos) | TF-IDF |\n",
    "|---------|----------------|-------------------|--------|\n",
    "| \"de\" | Alta (muy frecuente) | Bajo (aparece en todos) | **Bajo** |\n",
    "| \"Quijote\" | Media | Medio-alto | **Medio-alto** |\n",
    "| \"Rocinante\" | Baja (solo algunos caps) | Alto (raro en el corpus) | **Alto si aparece** |\n",
    "\n",
    "> **Clave**: TF-IDF automaticamente reduce el peso de las \"stop words\" (de, el, la, los) y amplifica las palabras discriminativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1763f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TF-IDF: Calculo manual con 3 oraciones cortas\n",
    "# ==============================================================================\n",
    "\n",
    "# Mini corpus para demostrar TF-IDF\n",
    "mini_corpus = [\n",
    "    \"el hidalgo tenia un rocin flaco\",\n",
    "    \"el rocin del hidalgo era viejo\",\n",
    "    \"sancho montaba un burro gris\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CALCULO MANUAL DE TF-IDF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- TF manual ---\n",
    "print(\"\\n1. Term Frequency (TF) por documento:\")\n",
    "print(\"-\" * 60)\n",
    "for i, doc in enumerate(mini_corpus):\n",
    "    words = doc.split()\n",
    "    counts = Counter(words)\n",
    "    tf = {w: c / len(words) for w, c in counts.items()}\n",
    "    top3 = sorted(tf.items(), key=lambda x: -x[1])[:5]\n",
    "    print(f\"  Doc {i}: {doc}\")\n",
    "    print(f\"         TF top-5: {dict(top3)}\")\n",
    "\n",
    "# --- Sklearn TfidfVectorizer para comparar ---\n",
    "print(\"\\n2. Matriz TF-IDF completa (sklearn):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(mini_corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "df_tfidf = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=feature_names,\n",
    "    index=[f\"Doc {i}\" for i in range(len(mini_corpus))],\n",
    ")\n",
    "\n",
    "# Mostrar la matriz completa\n",
    "print(df_tfidf.round(3).to_string())\n",
    "\n",
    "# --- Observacion ---\n",
    "print(\"\\n3. Observacion clave:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"  'el' tiene score BAJO (aparece en doc 0 y doc 1 -> IDF bajo)\")\n",
    "print(\"  'burro' tiene score ALTO (solo aparece en doc 2 -> IDF alto)\")\n",
    "print(\"  'hidalgo' tiene score MEDIO (aparece en 2 de 3 docs)\")\n",
    "\n",
    "# Mostrar scores especificos\n",
    "for word in [\"el\", \"burro\", \"hidalgo\", \"rocin\"]:\n",
    "    if word in feature_names:\n",
    "        idx = list(feature_names).index(word)\n",
    "        scores = tfidf_matrix.toarray()[:, idx]\n",
    "        max_score = max(scores)\n",
    "        print(f\"  '{word}': max TF-IDF = {max_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZACION: Heatmap de la matriz TF-IDF del ejemplo mini\n",
    "# ==============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3.5))\n",
    "\n",
    "sns.heatmap(\n",
    "    df_tfidf,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    ax=ax,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"label\": \"TF-IDF Score\"},\n",
    ")\n",
    "\n",
    "ax.set_title(\"Matriz TF-IDF: 3 oraciones del Quijote\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlabel(\"Palabras\")\n",
    "ax.set_ylabel(\"Documentos\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Las celdas mas oscuras indican palabras mas discriminativas.\")\n",
    "print(\"'el' es claro (poco informativo), 'burro' y 'gris' son oscuros (muy informativos).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa5f94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Limitacion de TF-IDF: No Entiende Semantica\n",
    "\n",
    "TF-IDF es una representacion **sparse** (la mayoria de valores son 0). Tiene un problema fundamental:\n",
    "\n",
    "```\n",
    "\"rocin flaco\"  vs  \"caballo delgado\"\n",
    "     │                    │\n",
    "     ▼                    ▼\n",
    " TF-IDF score = 0.0  (NO comparten palabras!)\n",
    "```\n",
    "\n",
    "Aunque significan lo mismo, TF-IDF les asigna **similitud cero** porque no comparten ningun termino.\n",
    "\n",
    "### Sparse vs Dense: Tabla Comparativa\n",
    "\n",
    "| Dimension | TF-IDF (Sparse) | Embeddings (Dense) |\n",
    "|-----------|-----------------|-------------------|\n",
    "| Representacion | Vector de |V| dims, mayoria 0 | Vector de ~1536 dims, todos != 0 |\n",
    "| Captura semantica | No | Si |\n",
    "| \"rocin\" ≈ \"caballo\" | No (score = 0) | Si (vectores cercanos) |\n",
    "| Velocidad | Muy rapida (solo multiplicar) | Requiere modelo de embeddings |\n",
    "| Costo | Gratis (local) | API call (OpenAI) |\n",
    "| Mejor para | Busqueda por keywords exactas | Busqueda por significado |\n",
    "\n",
    "**Siguiente paso**: Necesitamos embeddings **densos** que capturen el significado. Para eso usamos un modelo de embeddings (OpenAI) y una base de datos vectorial (ChromaDB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CHUNKING: Dividir el texto en fragmentos con RecursiveCharacterTextSplitter\n",
    "# ==============================================================================\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(texto_quijote)\n",
    "\n",
    "# --- Estadisticas ---\n",
    "chunk_tokens = [len(enc.encode(c)) for c in chunks]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHUNKING DEL TEXTO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Chunks generados:      {len(chunks)}\")\n",
    "print(f\"Tokens promedio/chunk: {np.mean(chunk_tokens):.0f}\")\n",
    "print(f\"Tokens min/max:        {min(chunk_tokens)} / {max(chunk_tokens)}\")\n",
    "print(f\"Tokens totales:        {sum(chunk_tokens):,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ejemplo de 2 chunks\n",
    "for i in [0, len(chunks) // 2]:\n",
    "    print(f\"\\nChunk {i} ({chunk_tokens[i]} tokens):\")\n",
    "    print(f\"  {chunks[i][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fe896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TF-IDF: Busqueda sparse sobre los chunks reales del Quijote\n",
    "# ==============================================================================\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n",
    "tfidf_matrix_real = tfidf_vectorizer.fit_transform(chunks)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TF-IDF SOBRE CHUNKS REALES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sparse matrix shape:  {tfidf_matrix_real.shape}\")\n",
    "total_elements = tfidf_matrix_real.shape[0] * tfidf_matrix_real.shape[1]\n",
    "nonzero = tfidf_matrix_real.nnz\n",
    "sparsity = (1 - nonzero / total_elements) * 100\n",
    "print(f\"Sparsity:             {sparsity:.1f}%\")\n",
    "print(f\"Non-zero elements:    {nonzero:,} / {total_elements:,}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def tfidf_search(query: str, k: int = 3) -> list[dict]:\n",
    "    \"\"\"Busqueda TF-IDF sobre los chunks del Quijote.\"\"\"\n",
    "    q_vec = tfidf_vectorizer.transform([query])\n",
    "    sims = cosine_similarity(q_vec, tfidf_matrix_real)[0]\n",
    "    top_idx = np.argsort(sims)[::-1][:k]\n",
    "    return [\n",
    "        {\"chunk_id\": int(i), \"score\": float(sims[i]), \"text\": chunks[int(i)]}\n",
    "        for i in top_idx\n",
    "    ]\n",
    "\n",
    "\n",
    "# --- Test ---\n",
    "test_results = tfidf_search(\"libros de caballeria\")\n",
    "print(\"\\nTest: 'libros de caballeria'\")\n",
    "for r in test_results:\n",
    "    print(f\"  Chunk {r['chunk_id']} (score={r['score']:.3f}): {r['text'][:80]}...\")\n",
    "\n",
    "# --- SVD 2D projection ---\n",
    "n_display = min(30, len(chunks))\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "coords = svd.fit_transform(tfidf_matrix_real[:n_display])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.scatter(\n",
    "    coords[:, 0], coords[:, 1], s=60, alpha=0.7,\n",
    "    c=range(n_display), cmap=\"viridis\",\n",
    ")\n",
    "for i, (x, y) in enumerate(coords):\n",
    "    ax.annotate(f\"c{i}\", (x, y), fontsize=7, alpha=0.8)\n",
    "ax.set_title(\n",
    "    f\"Proyeccion SVD 2D de {n_display} chunks (TF-IDF)\",\n",
    "    fontsize=13, fontweight=\"bold\",\n",
    ")\n",
    "ax.set_xlabel(\"Componente 1\")\n",
    "ax.set_ylabel(\"Componente 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aceba4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 3: ChromaDB -- Base de Datos Vectorial Real\n",
    "\n",
    "### Que es ChromaDB?\n",
    "\n",
    "ChromaDB es una base de datos vectorial open-source que almacena **documentos + embeddings + metadata** y permite busqueda por similitud semantica.\n",
    "\n",
    "### Que almacena cada documento?\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────┐\n",
    "│              ChromaDB Collection                     │\n",
    "│                                                      │\n",
    "│  ┌────────────────────────────────────────────────┐  │\n",
    "│  │  ID:        \"chunk_042\"                        │  │\n",
    "│  │  Document:  \"En un lugar de la Mancha...\"      │  │\n",
    "│  │  Embedding: [0.12, -0.45, 0.78, ..., 0.33]    │  │\n",
    "│  │  Metadata:  {capitulo: 1, posicion: 0}         │  │\n",
    "│  └────────────────────────────────────────────────┘  │\n",
    "│                                                      │\n",
    "│  Indice HNSW para busqueda rapida (O(log n))         │\n",
    "└─────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Flujo: Agregar y Buscar\n",
    "\n",
    "```\n",
    "AGREGAR:                          BUSCAR:\n",
    "\n",
    "documents ──► embed ──► add()     query ──► embed ──► collection.query()\n",
    "                  │                              │\n",
    "                  ▼                              ▼\n",
    "            ┌──────────┐                 Similitud coseno\n",
    "            │ ChromaDB │                 contra todos los\n",
    "            │  (HNSW)  │                 vectores indexados\n",
    "            └──────────┘                        │\n",
    "                                                ▼\n",
    "                                          Top-k documentos\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1657ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CHROMADB: Crear coleccion + agregar chunks con embeddings de OpenAI\n",
    "# ==============================================================================\n",
    "\n",
    "# Cliente in-memory (para desarrollo/prototipado)\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Crear coleccion\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"quijote_caps_1_8\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# --- Generar embeddings con OpenAI ---\n",
    "print(\"Generando embeddings para todos los chunks con OpenAI...\")\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# Batch embedding (max ~2000 por batch para OpenAI)\n",
    "batch_size = 500\n",
    "all_embeddings = []\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i : i + batch_size]\n",
    "    batch_embs = embeddings_model.embed_documents(batch)\n",
    "    all_embeddings.extend(batch_embs)\n",
    "    print(f\"  Batch {i // batch_size + 1}: {len(batch)} chunks embebidos\")\n",
    "\n",
    "embedding_time = time.perf_counter() - t0\n",
    "\n",
    "# --- Agregar a ChromaDB ---\n",
    "ids = [f\"chunk_{i:04d}\" for i in range(len(chunks))]\n",
    "metadatas = [{\"chunk_id\": i, \"n_tokens\": len(enc.encode(c))} for i, c in enumerate(chunks)]\n",
    "\n",
    "collection.add(\n",
    "    ids=ids,\n",
    "    documents=chunks,\n",
    "    embeddings=all_embeddings,\n",
    "    metadatas=metadatas,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHROMADB: COLECCION CREADA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Coleccion:       {collection.name}\")\n",
    "print(f\"Documentos:      {collection.count()}\")\n",
    "print(f\"Embedding dims:  {len(all_embeddings[0])}\")\n",
    "print(f\"Tiempo:          {embedding_time:.1f}s\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# COMPARACION: TF-IDF vs ChromaDB para las mismas queries\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def chroma_search(query: str, k: int = 3) -> list[dict]:\n",
    "    \"\"\"Busqueda semantica en ChromaDB con embeddings de OpenAI.\"\"\"\n",
    "    q_embedding = embeddings_model.embed_query(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[q_embedding],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"],\n",
    "    )\n",
    "    return [\n",
    "        {\n",
    "            \"chunk_id\": results[\"metadatas\"][0][i][\"chunk_id\"],\n",
    "            \"score\": 1 - results[\"distances\"][0][i],  # cosine similarity\n",
    "            \"text\": results[\"documents\"][0][i],\n",
    "        }\n",
    "        for i in range(len(results[\"documents\"][0]))\n",
    "    ]\n",
    "\n",
    "\n",
    "# --- Queries de prueba ---\n",
    "test_queries = [\n",
    "    \"Que piensa Cervantes sobre los libros de caballeria?\",\n",
    "    \"Describe al caballo delgado del protagonista\",  # semantica: rocin flaco\n",
    "    \"Que papel juega la locura en la historia?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARACION: TF-IDF vs ChromaDB (Top-1)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in test_queries:\n",
    "    tfidf_top = tfidf_search(q, k=1)[0]\n",
    "    chroma_top = chroma_search(q, k=1)[0]\n",
    "\n",
    "    print(f\"\\nQuery: '{q}'\")\n",
    "    print(f\"  TF-IDF  (chunk {tfidf_top['chunk_id']}, score={tfidf_top['score']:.3f}):\")\n",
    "    print(f\"    {tfidf_top['text'][:100]}...\")\n",
    "    print(f\"  ChromaDB (chunk {chroma_top['chunk_id']}, score={chroma_top['score']:.3f}):\")\n",
    "    print(f\"    {chroma_top['text'][:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Observa: Para 'caballo delgado', ChromaDB encuentra chunks\")\n",
    "print(\"sobre 'rocin flaco' (sinonimo semantico). TF-IDF no puede.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8c4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZACION DUAL: TF-IDF similarity vs Dense embedding similarity\n",
    "# ==============================================================================\n",
    "\n",
    "n_display = 20  # Primeros 20 chunks\n",
    "\n",
    "# --- TF-IDF cosine similarity ---\n",
    "tfidf_sim = cosine_similarity(tfidf_matrix_real[:n_display]).astype(float)\n",
    "\n",
    "# --- Dense embedding similarity ---\n",
    "dense_embs = np.array(all_embeddings[:n_display])\n",
    "dense_sim = cosine_similarity(dense_embs).astype(float)\n",
    "\n",
    "# --- Plot dual ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5.5))\n",
    "\n",
    "sns.heatmap(\n",
    "    tfidf_sim, ax=axes[0], cmap=\"Blues\", vmin=0, vmax=1,\n",
    "    xticklabels=5, yticklabels=5, square=True,\n",
    "    cbar_kws={\"label\": \"Similitud coseno\"},\n",
    ")\n",
    "axes[0].set_title(\"TF-IDF (Sparse)\", fontsize=13, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Chunk ID\")\n",
    "axes[0].set_ylabel(\"Chunk ID\")\n",
    "\n",
    "sns.heatmap(\n",
    "    dense_sim, ax=axes[1], cmap=\"Oranges\", vmin=0, vmax=1,\n",
    "    xticklabels=5, yticklabels=5, square=True,\n",
    "    cbar_kws={\"label\": \"Similitud coseno\"},\n",
    ")\n",
    "axes[1].set_title(\"Embeddings Densos (OpenAI)\", fontsize=13, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Chunk ID\")\n",
    "axes[1].set_ylabel(\"Chunk ID\")\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Similitud entre los primeros {n_display} chunks: Sparse vs Dense\",\n",
    "    fontsize=14, fontweight=\"bold\", y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Los embeddings densos capturan similitud semantica entre chunks\")\n",
    "print(\"que no comparten palabras exactas (bloques mas calidos fuera de la diagonal).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e0d12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 4: Simple RAG -- Retrieval + Generation\n",
    "\n",
    "### La idea central\n",
    "\n",
    "En vez de pasar TODO el texto (~20K tokens), solo pasamos los chunks mas relevantes (~500 tokens):\n",
    "\n",
    "```\n",
    "CONTEXT STUFFING:\n",
    "  Query ──► [TODO el texto: ~20,000 tokens] ──► LLM ──► Respuesta\n",
    "\n",
    "SIMPLE RAG:\n",
    "  Query ──► Embed ──► ChromaDB ──► Top-3 chunks (~500 tokens) ──► LLM ──► Respuesta\n",
    "\n",
    "                          ~40x menos tokens!\n",
    "```\n",
    "\n",
    "### Ventajas\n",
    "\n",
    "- **~40x reduccion** en tokens de contexto\n",
    "- **Menor costo** por query\n",
    "- **Menor latencia** (menos tokens para procesar)\n",
    "- **Foco**: El LLM solo ve informacion relevante, evitando \"lost in the middle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SIMPLE RAG: Retrieval desde ChromaDB + generacion con LLM\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def rag_simple(query: str, k: int = 3) -> dict:\n",
    "    \"\"\"Simple RAG: busqueda en ChromaDB + generacion con LLM.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Retrieval\n",
    "    retrieved = chroma_search(query, k=k)\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[Chunk {r['chunk_id']} | sim={r['score']:.3f}] {r['text']}\"\n",
    "        for r in retrieved\n",
    "    )\n",
    "\n",
    "    # Generation\n",
    "    prompt = (\n",
    "        f\"Contexto recuperado:\\n{context}\\n\\n\"\n",
    "        f\"Pregunta: {query}\\n\\n\"\n",
    "        \"Responde de forma concisa basandote unicamente en el contexto.\"\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    latency_s = time.perf_counter() - t0\n",
    "\n",
    "    input_tokens = len(enc.encode(prompt))\n",
    "    output_tokens = len(enc.encode(response.content))\n",
    "    cost = input_tokens * 0.15 / 1_000_000 + output_tokens * 0.60 / 1_000_000\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": response.content,\n",
    "        \"retrieved\": retrieved,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"cost_usd\": cost,\n",
    "        \"latency_s\": latency_s,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Test + comparacion directa ---\n",
    "rag_result = rag_simple(test_query, k=3)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SIMPLE RAG vs CONTEXT STUFFING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query: {test_query}\")\n",
    "print()\n",
    "cs_in = cs_result[\"input_tokens\"]\n",
    "rag_in = rag_result[\"input_tokens\"]\n",
    "cs_cost = cs_result[\"cost_usd\"]\n",
    "rag_cost = rag_result[\"cost_usd\"]\n",
    "cs_lat = cs_result[\"latency_s\"]\n",
    "rag_lat = rag_result[\"latency_s\"]\n",
    "\n",
    "header = f\"{'Metrica':<20} {'Context Stuffing':>18} {'Simple RAG':>12} {'Reduccion':>12}\"\n",
    "print(header)\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Input tokens':<20} {cs_in:>18,} {rag_in:>12,} {cs_in / rag_in:>11.0f}x\")\n",
    "print(f\"{'Costo (USD)':<20} ${cs_cost:>17.6f} ${rag_cost:>11.6f} {cs_cost / rag_cost:>11.0f}x\")\n",
    "print(f\"{'Latencia (s)':<20} {cs_lat:>18.1f} {rag_lat:>12.1f} {cs_lat / rag_lat:>11.1f}x\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nRespuesta RAG: {rag_result['answer'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27341137",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 5: Agentic RAG con LangGraph\n",
    "\n",
    "### Motivacion\n",
    "\n",
    "Simple RAG tiene una limitacion: **no sabe si los chunks recuperados son relevantes**. Si la query es ambigua o los chunks no contienen la respuesta, el LLM igual genera algo -- potencialmente una alucinacion.\n",
    "\n",
    "**Agentic RAG** agrega un loop de razonamiento:\n",
    "1. Recupera chunks\n",
    "2. **Evalua** si son relevantes (grading)\n",
    "3. Si no son relevantes, **reformula** la query y reintenta\n",
    "4. Genera la respuesta\n",
    "5. **Evalua** si la respuesta esta fundamentada en el contexto\n",
    "6. Si tiene alucinaciones, **regenera**\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────┐     ┌────────────────┐\n",
    "│ analyze_query│────►│ retrieve │────►│ grade_context  │\n",
    "└──────────────┘     └──────────┘     └───────┬────────┘\n",
    "       ▲                                       │\n",
    "       │ (irrelevant)                 relevant │\n",
    "       │                                       ▼\n",
    "  increment_retry                      ┌──────────┐\n",
    "                                       │ generate │\n",
    "                                       └────┬─────┘\n",
    "                                            │\n",
    "                                            ▼\n",
    "                                    ┌──────────────┐\n",
    "                                    │ grade_answer │\n",
    "                                    └──────┬───────┘\n",
    "                                           │\n",
    "                              grounded ────►  END\n",
    "                              hallucination ► re-generate\n",
    "```\n",
    "\n",
    "Ref: Self-RAG (Asai et al., 2023) -- el sistema reflexiona sobre sus propios outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AGENTIC RAG: Modelos Pydantic + Graders con structured output\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "class ContextGrade(BaseModel):\n",
    "    \"\"\"Evalua si el contexto recuperado es relevante para la query.\"\"\"\n",
    "\n",
    "    relevant: bool = Field(description=\"True si el contexto es relevante para la query\")\n",
    "    reasoning: str = Field(description=\"Breve explicacion del grado asignado\")\n",
    "\n",
    "\n",
    "class AnswerGrade(BaseModel):\n",
    "    \"\"\"Evalua si la respuesta esta fundamentada en el contexto (sin alucinacion).\"\"\"\n",
    "\n",
    "    grounded: bool = Field(\n",
    "        description=\"True si la respuesta esta completamente respaldada por el contexto\"\n",
    "    )\n",
    "    reasoning: str = Field(description=\"Breve explicacion del grado asignado\")\n",
    "\n",
    "\n",
    "context_grader = llm.with_structured_output(ContextGrade)\n",
    "answer_grader = llm.with_structured_output(AnswerGrade)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"GRADERS CONFIGURADOS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"context_grader: evalua relevancia del contexto recuperado\")\n",
    "print(\"answer_grader:  evalua si la respuesta tiene alucinaciones\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AGENTIC RAG: State + todos los nodos del grafo\n",
    "# ==============================================================================\n",
    "\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "\n",
    "class AgenticRAGState(TypedDict):\n",
    "    query: str\n",
    "    refined_query: str\n",
    "    docs: list[dict]\n",
    "    context_relevant: bool\n",
    "    context_reasoning: str\n",
    "    answer: str\n",
    "    answer_grounded: bool\n",
    "    answer_reasoning: str\n",
    "    retry_count: int\n",
    "    steps_trace: list[str]\n",
    "    total_tokens: int\n",
    "\n",
    "\n",
    "def analyze_query(state: AgenticRAGState) -> dict:\n",
    "    \"\"\"Analiza la query y opcionalmente la reformula para mejor retrieval.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    refined = state.get(\"refined_query\", \"\")\n",
    "    trace = list(state.get(\"steps_trace\", []))\n",
    "    total_tokens = state.get(\"total_tokens\", 0)\n",
    "\n",
    "    if refined and state.get(\"retry_count\", 0) > 0:\n",
    "        prompt_text = (\n",
    "            f\"La siguiente pregunta no obtuvo contexto relevante: '{query}'.\\n\"\n",
    "            f\"Reformulala para mejorar la busqueda. Responde SOLO con la nueva pregunta.\"\n",
    "        )\n",
    "        resp = llm.invoke(prompt_text)\n",
    "        refined = resp.content.strip()\n",
    "        total_tokens += len(enc.encode(prompt_text)) + len(enc.encode(refined))\n",
    "        trace.append(f\"analyze_query: reformulated -> '{refined}'\")\n",
    "    else:\n",
    "        refined = query\n",
    "        trace.append(f\"analyze_query: using original -> '{refined}'\")\n",
    "\n",
    "    return {\"refined_query\": refined, \"steps_trace\": trace, \"total_tokens\": total_tokens}\n",
    "\n",
    "\n",
    "def retrieve_node(state: AgenticRAGState) -> dict:\n",
    "    \"\"\"Recupera documentos de ChromaDB.\"\"\"\n",
    "    refined = state[\"refined_query\"]\n",
    "    docs = chroma_search(refined, k=3)\n",
    "    trace = list(state.get(\"steps_trace\", []))\n",
    "    trace.append(f\"retrieve: got {len(docs)} docs for '{refined}'\")\n",
    "    return {\"docs\": docs, \"steps_trace\": trace}\n",
    "\n",
    "\n",
    "def build_context(docs: list[dict]) -> str:\n",
    "    \"\"\"Construye el contexto a partir de los documentos recuperados.\"\"\"\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[Chunk {d['chunk_id']} | sim={d['score']:.3f}] {d['text']}\" for d in docs\n",
    "    )\n",
    "\n",
    "\n",
    "def grade_context_node(state: AgenticRAGState) -> dict:\n",
    "    \"\"\"Evalua si el contexto recuperado es relevante.\"\"\"\n",
    "    context = build_context(state[\"docs\"])\n",
    "    query = state[\"refined_query\"]\n",
    "    prompt_text = (\n",
    "        f\"Query: {query}\\n\\nContexto recuperado:\\n{context}\\n\\n\"\n",
    "        \"Es este contexto relevante para responder la query?\"\n",
    "    )\n",
    "    grade = context_grader.invoke(prompt_text)\n",
    "    trace = list(state.get(\"steps_trace\", []))\n",
    "    trace.append(f\"grade_context: relevant={grade.relevant} ({grade.reasoning})\")\n",
    "    total_tokens = state.get(\"total_tokens\", 0)\n",
    "    total_tokens += len(enc.encode(prompt_text))\n",
    "    return {\n",
    "        \"context_relevant\": grade.relevant,\n",
    "        \"context_reasoning\": grade.reasoning,\n",
    "        \"steps_trace\": trace,\n",
    "        \"total_tokens\": total_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_context_grade(state: AgenticRAGState) -> str:\n",
    "    if state.get(\"context_relevant\", False):\n",
    "        return \"generate\"\n",
    "    if state.get(\"retry_count\", 0) >= MAX_RETRIES:\n",
    "        return \"generate\"\n",
    "    return \"re_query\"\n",
    "\n",
    "\n",
    "def increment_retry(state: AgenticRAGState) -> dict:\n",
    "    return {\"retry_count\": state.get(\"retry_count\", 0) + 1}\n",
    "\n",
    "\n",
    "def generate_node(state: AgenticRAGState) -> dict:\n",
    "    \"\"\"Genera la respuesta a partir del contexto.\"\"\"\n",
    "    context = build_context(state[\"docs\"])\n",
    "    query = state[\"query\"]\n",
    "    prompt_text = (\n",
    "        f\"Contexto recuperado:\\n{context}\\n\\n\"\n",
    "        f\"Pregunta: {query}\\n\\n\"\n",
    "        \"Responde de forma concisa basandote unicamente en el contexto.\"\n",
    "    )\n",
    "    resp = llm.invoke(prompt_text)\n",
    "    trace = list(state.get(\"steps_trace\", []))\n",
    "    trace.append(\"generate: produced answer\")\n",
    "    total_tokens = state.get(\"total_tokens\", 0)\n",
    "    total_tokens += len(enc.encode(prompt_text)) + len(enc.encode(resp.content))\n",
    "    return {\"answer\": resp.content, \"steps_trace\": trace, \"total_tokens\": total_tokens}\n",
    "\n",
    "\n",
    "def grade_answer_node(state: AgenticRAGState) -> dict:\n",
    "    \"\"\"Evalua si la respuesta esta fundamentada en el contexto.\"\"\"\n",
    "    context = build_context(state[\"docs\"])\n",
    "    answer = state[\"answer\"]\n",
    "    prompt_text = (\n",
    "        f\"Contexto:\\n{context}\\n\\nRespuesta generada: {answer}\\n\\n\"\n",
    "        \"Esta la respuesta completamente respaldada por el contexto?\"\n",
    "    )\n",
    "    grade = answer_grader.invoke(prompt_text)\n",
    "    trace = list(state.get(\"steps_trace\", []))\n",
    "    trace.append(f\"grade_answer: grounded={grade.grounded} ({grade.reasoning})\")\n",
    "    total_tokens = state.get(\"total_tokens\", 0)\n",
    "    total_tokens += len(enc.encode(prompt_text))\n",
    "    return {\n",
    "        \"answer_grounded\": grade.grounded,\n",
    "        \"answer_reasoning\": grade.reasoning,\n",
    "        \"steps_trace\": trace,\n",
    "        \"total_tokens\": total_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "def route_after_answer_grade(state: AgenticRAGState) -> str:\n",
    "    if state.get(\"answer_grounded\", False):\n",
    "        return \"end\"\n",
    "    if state.get(\"retry_count\", 0) >= MAX_RETRIES:\n",
    "        return \"end\"\n",
    "    return \"re_generate\"\n",
    "\n",
    "\n",
    "def increment_retry_gen(state: AgenticRAGState) -> dict:\n",
    "    trace = list(state.get(\"steps_trace\", []))\n",
    "    trace.append(\"re_generate: retrying generation\")\n",
    "    return {\"retry_count\": state.get(\"retry_count\", 0) + 1, \"steps_trace\": trace}\n",
    "\n",
    "\n",
    "print(\"Todos los nodos definidos: analyze_query, retrieve, grade_context,\")\n",
    "print(\"generate, grade_answer, increment_retry, increment_retry_gen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11860085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AGENTIC RAG: Compilar el StateGraph\n",
    "# ==============================================================================\n",
    "\n",
    "graph = StateGraph(AgenticRAGState)\n",
    "\n",
    "# Nodos\n",
    "graph.add_node(\"analyze_query\", analyze_query)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"grade_context\", grade_context_node)\n",
    "graph.add_node(\"increment_retry\", increment_retry)\n",
    "graph.add_node(\"generate\", generate_node)\n",
    "graph.add_node(\"grade_answer\", grade_answer_node)\n",
    "graph.add_node(\"increment_retry_gen\", increment_retry_gen)\n",
    "\n",
    "# Edges\n",
    "graph.set_entry_point(\"analyze_query\")\n",
    "graph.add_edge(\"analyze_query\", \"retrieve\")\n",
    "graph.add_edge(\"retrieve\", \"grade_context\")\n",
    "graph.add_conditional_edges(\n",
    "    \"grade_context\",\n",
    "    route_after_context_grade,\n",
    "    {\"generate\": \"generate\", \"re_query\": \"increment_retry\"},\n",
    ")\n",
    "graph.add_edge(\"increment_retry\", \"analyze_query\")\n",
    "graph.add_edge(\"generate\", \"grade_answer\")\n",
    "graph.add_conditional_edges(\n",
    "    \"grade_answer\",\n",
    "    route_after_answer_grade,\n",
    "    {\"end\": END, \"re_generate\": \"increment_retry_gen\"},\n",
    ")\n",
    "graph.add_edge(\"increment_retry_gen\", \"generate\")\n",
    "\n",
    "agentic_app = graph.compile()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AGENTIC RAG: GRAFO COMPILADO\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Nodos: analyze_query -> retrieve -> grade_context ->\")\n",
    "print(\"       generate -> grade_answer -> END\")\n",
    "print(\"Loops: re_query (max 2), re_generate (max 2)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbd931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AGENTIC RAG: Visualizar el grafo\n",
    "# ==============================================================================\n",
    "\n",
    "try:\n",
    "    from IPython.display import Image, display\n",
    "\n",
    "    img_bytes = agentic_app.get_graph().draw_mermaid_png()\n",
    "    display(Image(img_bytes))\n",
    "except Exception:\n",
    "    # Fallback: print Mermaid text\n",
    "    print(\"Diagrama Mermaid (renderizar en mermaid.live):\")\n",
    "    print(agentic_app.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f86ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# AGENTIC RAG: Wrapper + test\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def rag_agentic(query: str) -> dict:\n",
    "    \"\"\"Ejecuta el pipeline Agentic RAG con trace completo.\"\"\"\n",
    "    t0 = time.perf_counter()\n",
    "    initial_state: AgenticRAGState = {\n",
    "        \"query\": query,\n",
    "        \"refined_query\": \"\",\n",
    "        \"docs\": [],\n",
    "        \"context_relevant\": False,\n",
    "        \"context_reasoning\": \"\",\n",
    "        \"answer\": \"\",\n",
    "        \"answer_grounded\": False,\n",
    "        \"answer_reasoning\": \"\",\n",
    "        \"retry_count\": 0,\n",
    "        \"steps_trace\": [],\n",
    "        \"total_tokens\": 0,\n",
    "    }\n",
    "    result = agentic_app.invoke(initial_state)\n",
    "    latency_s = time.perf_counter() - t0\n",
    "\n",
    "    # Estimar tokens totales (input + output)\n",
    "    total_tokens = result.get(\"total_tokens\", 0)\n",
    "    cost = total_tokens * 0.15 / 1_000_000  # Approx (mayoria input)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"retrieved\": result[\"docs\"],\n",
    "        \"steps_trace\": result[\"steps_trace\"],\n",
    "        \"retry_count\": result[\"retry_count\"],\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost_usd\": cost,\n",
    "        \"latency_s\": latency_s,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Test ---\n",
    "agentic_result = rag_agentic(test_query)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AGENTIC RAG: RESULTADO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Query:       {agentic_result['query']}\")\n",
    "print(f\"Answer:      {agentic_result['answer'][:200]}...\")\n",
    "print(f\"Retries:     {agentic_result['retry_count']}\")\n",
    "print(f\"Tokens:      {agentic_result['total_tokens']:,}\")\n",
    "print(f\"Costo:       ${agentic_result['cost_usd']:.6f}\")\n",
    "print(f\"Latencia:    {agentic_result['latency_s']:.1f}s\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTrace de pasos:\")\n",
    "for step in agentic_result[\"steps_trace\"]:\n",
    "    print(f\"  -> {step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4f1967",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 6: Evaluacion Comparativa\n",
    "\n",
    "Ahora comparamos los 3 enfoques sistematicamente con un conjunto de **6 preguntas** que abarcan capitulos I-VIII del Quijote.\n",
    "\n",
    "Cada pregunta tiene **keywords de ground truth** -- medimos cuantas keywords aparecen en la respuesta (**precision proxy = keyword recall**).\n",
    "\n",
    "> **Nota**: Keyword recall NO es una metrica robusta para produccion (ver analisis critico al final). Pero es suficiente para ilustrar las diferencias entre enfoques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EVALUACION: Definir eval_set con ground truth keywords\n",
    "# ==============================================================================\n",
    "\n",
    "eval_set = [\n",
    "    {\n",
    "        \"query\": \"Que comia el hidalgo y como era su dieta semanal?\",\n",
    "        \"keywords\": [\"olla\", \"vaca\", \"carnero\", \"salpicon\", \"lentejas\", \"palomino\", \"duelos\", \"quebrantos\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Como se llama el caballo del protagonista y como era?\",\n",
    "        \"keywords\": [\"rocinante\", \"rocin\", \"flaco\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Quien es Dulcinea y que relacion tiene con Don Quijote?\",\n",
    "        \"keywords\": [\"dulcinea\", \"toboso\", \"aldonza\", \"lorenzo\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Que ocurre en el episodio de los molinos de viento?\",\n",
    "        \"keywords\": [\"molinos\", \"viento\", \"gigantes\", \"lanza\", \"aspas\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Que hicieron el cura y el barbero con los libros de Don Quijote?\",\n",
    "        \"keywords\": [\"libros\", \"quemaron\", \"fuego\", \"cura\", \"barbero\", \"escrutinio\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Como se incorpora Sancho Panza a las aventuras?\",\n",
    "        \"keywords\": [\"sancho\", \"panza\", \"escudero\", \"labrador\", \"insula\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def precision_proxy(answer: str, keywords: list[str]) -> float:\n",
    "    \"\"\"Keyword recall: fraccion de keywords esperadas presentes en la respuesta.\"\"\"\n",
    "    answer_lower = answer.lower()\n",
    "    hits = sum(1 for k in keywords if k in answer_lower)\n",
    "    return hits / len(keywords) if keywords else 0.0\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EVAL SET DEFINIDO\")\n",
    "print(\"=\" * 60)\n",
    "for i, ex in enumerate(eval_set):\n",
    "    print(f\"  Q{i+1}: {ex['query']}\")\n",
    "    print(f\"       Keywords: {ex['keywords']}\")\n",
    "print(f\"\\nTotal: {len(eval_set)} preguntas\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a29357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# EVALUACION: Ejecutar 3 enfoques en todas las queries\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Ejecutando evaluacion comparativa...\")\n",
    "print(\"Esto toma ~2-3 minutos (multiples LLM calls)\\n\")\n",
    "\n",
    "rows = []\n",
    "for i, ex in enumerate(eval_set):\n",
    "    q = ex[\"query\"]\n",
    "    kw = ex[\"keywords\"]\n",
    "    print(f\"  Q{i+1}/{len(eval_set)}: {q[:50]}...\")\n",
    "\n",
    "    # Context Stuffing\n",
    "    cs = context_stuffing_answer(q, texto_quijote)\n",
    "\n",
    "    # Simple RAG\n",
    "    sr = rag_simple(q, k=3)\n",
    "\n",
    "    # Agentic RAG\n",
    "    ar = rag_agentic(q)\n",
    "\n",
    "    rows.append({\n",
    "        \"query\": q[:40] + \"...\",\n",
    "        \"cs_precision\": precision_proxy(cs[\"answer\"], kw),\n",
    "        \"rag_precision\": precision_proxy(sr[\"answer\"], kw),\n",
    "        \"agentic_precision\": precision_proxy(ar[\"answer\"], kw),\n",
    "        \"cs_tokens\": cs[\"input_tokens\"],\n",
    "        \"rag_tokens\": sr[\"input_tokens\"],\n",
    "        \"agentic_tokens\": ar[\"total_tokens\"],\n",
    "        \"cs_cost\": cs[\"cost_usd\"],\n",
    "        \"rag_cost\": sr[\"cost_usd\"],\n",
    "        \"agentic_cost\": ar[\"cost_usd\"],\n",
    "        \"cs_latency\": cs[\"latency_s\"],\n",
    "        \"rag_latency\": sr[\"latency_s\"],\n",
    "        \"agentic_latency\": ar[\"latency_s\"],\n",
    "        \"agentic_retries\": ar[\"retry_count\"],\n",
    "    })\n",
    "\n",
    "report = pd.DataFrame(rows)\n",
    "print(\"\\nEvaluacion completada!\")\n",
    "\n",
    "# --- Mostrar tabla resumen ---\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TABLA RESUMEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"Metrica\": [\"Precision promedio\", \"Tokens promedio\", \"Costo promedio (USD)\", \"Latencia promedio (s)\"],\n",
    "    \"Context Stuffing\": [\n",
    "        f\"{report['cs_precision'].mean():.2f}\",\n",
    "        f\"{report['cs_tokens'].mean():,.0f}\",\n",
    "        f\"${report['cs_cost'].mean():.6f}\",\n",
    "        f\"{report['cs_latency'].mean():.1f}\",\n",
    "    ],\n",
    "    \"Simple RAG\": [\n",
    "        f\"{report['rag_precision'].mean():.2f}\",\n",
    "        f\"{report['rag_tokens'].mean():,.0f}\",\n",
    "        f\"${report['rag_cost'].mean():.6f}\",\n",
    "        f\"{report['rag_latency'].mean():.1f}\",\n",
    "    ],\n",
    "    \"Agentic RAG\": [\n",
    "        f\"{report['agentic_precision'].mean():.2f}\",\n",
    "        f\"{report['agentic_tokens'].mean():,.0f}\",\n",
    "        f\"${report['agentic_cost'].mean():.6f}\",\n",
    "        f\"{report['agentic_latency'].mean():.1f}\",\n",
    "    ],\n",
    "})\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47594a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VISUALIZACION: Bar chart 3-panel de la evaluacion\n",
    "# ==============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "x = np.arange(len(report))\n",
    "width = 0.25\n",
    "labels = [f\"Q{i+1}\" for i in range(len(report))]\n",
    "\n",
    "# --- Panel 1: Tokens de contexto ---\n",
    "axes[0].bar(x - width, report[\"cs_tokens\"], width, label=\"Context Stuffing\", color=\"#e74c3c\")\n",
    "axes[0].bar(x, report[\"rag_tokens\"], width, label=\"Simple RAG\", color=\"#3498db\")\n",
    "axes[0].bar(x + width, report[\"agentic_tokens\"], width, label=\"Agentic RAG\", color=\"#2ecc71\")\n",
    "axes[0].set_title(\"Tokens de contexto\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(labels)\n",
    "axes[0].set_ylabel(\"Tokens\")\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# --- Panel 2: Precision (keyword recall) ---\n",
    "axes[1].bar(x - width, report[\"cs_precision\"], width, label=\"Context Stuffing\", color=\"#e74c3c\")\n",
    "axes[1].bar(x, report[\"rag_precision\"], width, label=\"Simple RAG\", color=\"#3498db\")\n",
    "axes[1].bar(x + width, report[\"agentic_precision\"], width, label=\"Agentic RAG\", color=\"#2ecc71\")\n",
    "axes[1].set_title(\"Precision (keyword recall)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(labels)\n",
    "axes[1].set_ylim(0, 1.15)\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "# --- Panel 3: Latencia ---\n",
    "axes[2].bar(x - width, report[\"cs_latency\"], width, label=\"Context Stuffing\", color=\"#e74c3c\")\n",
    "axes[2].bar(x, report[\"rag_latency\"], width, label=\"Simple RAG\", color=\"#3498db\")\n",
    "axes[2].bar(x + width, report[\"agentic_latency\"], width, label=\"Agentic RAG\", color=\"#2ecc71\")\n",
    "axes[2].set_title(\"Latencia (s)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(labels)\n",
    "axes[2].set_ylabel(\"Segundos\")\n",
    "axes[2].legend(fontsize=8)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Evaluacion Comparativa: Context Stuffing vs Simple RAG vs Agentic RAG\",\n",
    "    fontsize=14, fontweight=\"bold\",\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PROYECCION DE COSTOS: queries/dia vs costo mensual\n",
    "# ==============================================================================\n",
    "\n",
    "queries_per_day = [10, 50, 100, 500, 1000, 5000, 10000]\n",
    "\n",
    "avg_cs_cost = report[\"cs_cost\"].mean()\n",
    "avg_rag_cost = report[\"rag_cost\"].mean()\n",
    "avg_agentic_cost = report[\"agentic_cost\"].mean()\n",
    "\n",
    "cs_monthly = [q * avg_cs_cost * 30 for q in queries_per_day]\n",
    "rag_monthly = [q * avg_rag_cost * 30 for q in queries_per_day]\n",
    "agentic_monthly = [q * avg_agentic_cost * 30 for q in queries_per_day]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(queries_per_day, cs_monthly, \"o-\", label=\"Context Stuffing\", color=\"#e74c3c\", linewidth=2)\n",
    "ax.plot(queries_per_day, rag_monthly, \"s-\", label=\"Simple RAG\", color=\"#3498db\", linewidth=2)\n",
    "ax.plot(queries_per_day, agentic_monthly, \"^-\", label=\"Agentic RAG\", color=\"#2ecc71\", linewidth=2)\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Queries por dia\", fontsize=12)\n",
    "ax.set_ylabel(\"Costo mensual (USD)\", fontsize=12)\n",
    "ax.set_title(\"Proyeccion de costos mensuales por enfoque\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, which=\"both\", alpha=0.3)\n",
    "\n",
    "# Anotar el punto de 1000 queries/dia\n",
    "for i, q in enumerate(queries_per_day):\n",
    "    if q == 1000:\n",
    "        ax.annotate(\n",
    "            f\"CS: ${cs_monthly[i]:.1f}/mes\",\n",
    "            (q, cs_monthly[i]),\n",
    "            textcoords=\"offset points\", xytext=(10, 10), fontsize=9,\n",
    "        )\n",
    "        ax.annotate(\n",
    "            f\"RAG: ${rag_monthly[i]:.2f}/mes\",\n",
    "            (q, rag_monthly[i]),\n",
    "            textcoords=\"offset points\", xytext=(10, -15), fontsize=9,\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PROYECCION A 1,000 QUERIES/DIA\")\n",
    "print(\"=\" * 60)\n",
    "idx_1k = queries_per_day.index(1000)\n",
    "print(f\"  Context Stuffing: ${cs_monthly[idx_1k]:.2f}/mes\")\n",
    "print(f\"  Simple RAG:       ${rag_monthly[idx_1k]:.2f}/mes\")\n",
    "print(f\"  Agentic RAG:      ${agentic_monthly[idx_1k]:.2f}/mes\")\n",
    "print(f\"  Ahorro RAG vs CS: {cs_monthly[idx_1k] / rag_monthly[idx_1k]:.0f}x\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b7a86d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 7: Analisis Critico\n",
    "\n",
    "### Fortalezas de este notebook\n",
    "\n",
    "- **Texto real**: ~15K palabras del Quijote, no un parrafo de juguete.\n",
    "- **ChromaDB real**: Base de datos vectorial con embeddings de OpenAI, no solo TF-IDF.\n",
    "- **Metricas cuantitativas**: Tokens, costo, latencia y precision medidos en cada enfoque.\n",
    "- **Agentic con reasoning loops**: Grading de contexto y respuesta, no solo heuristicas.\n",
    "\n",
    "### Limitaciones\n",
    "\n",
    "- **Keyword recall no es robusta**: Una respuesta correcta puede no usar las keywords exactas. En produccion usar RAGAS, ARES, o evaluacion humana.\n",
    "- **Corpus unico**: Solo Don Quijote. Generalizar requiere evaluar con multiples corpus.\n",
    "- **ChromaDB in-memory**: No persiste entre ejecuciones. Produccion requiere persistencia.\n",
    "- **Self-grading circular**: El mismo LLM que genera la respuesta evalua su calidad. Sesgo inherente.\n",
    "\n",
    "### Cuando usar cada enfoque\n",
    "\n",
    "| Enfoque | Mejor para | Evitar cuando |\n",
    "|---------|-----------|---------------|\n",
    "| **Context Stuffing** | Textos cortos (< 4K tokens), prototipado rapido | Corpus grandes, costos importan |\n",
    "| **Simple RAG** | Corpus medianos, latencia predecible, produccion | Se necesita autocorreccion |\n",
    "| **Agentic RAG** | Queries complejas, calidad critica, autocorreccion | Latencia critica, presupuesto limitado |\n",
    "\n",
    "### Roadmap a produccion\n",
    "\n",
    "1. **Vector DB escalable**: FAISS para batch, Pinecone/Qdrant para produccion cloud.\n",
    "2. **Golden dataset**: Evaluacion con scorecards humanas, no solo keyword recall.\n",
    "3. **Observabilidad**: LangSmith, Phoenix, o Weights & Biases para trazas y costos.\n",
    "4. **Guardrails**: Validacion de fuentes, deteccion de PII, limites de costo por query.\n",
    "5. **Chunking strategy**: Experimentar con semantic chunking, parent-child, o agentic chunking.\n",
    "\n",
    "### Referencias\n",
    "\n",
    "- Chip Huyen (2024). *AI Engineering: Building Applications with Foundation Models*. O'Reilly.\n",
    "- Asai, A., Wu, Z., Wang, Y., Sil, A., & Hajishirzi, H. (2023). *Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection*. arXiv:2310.11511.\n",
    "- Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). *Lost in the Middle: How Language Models Use Long Contexts*. arXiv:2307.03172."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d049a83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checklist de Consolidacion\n",
    "\n",
    "Antes de avanzar, asegurate de poder responder estas preguntas:\n",
    "\n",
    "- [ ] **1.** Por que context stuffing no escala? Calcula el costo mensual de pasar 20K tokens por query a 1000 queries/dia con gpt-4o-mini.\n",
    "\n",
    "- [ ] **2.** Explica la formula TF-IDF con tus palabras. Por que \"de\" tiene score bajo y \"Rocinante\" tiene score alto?\n",
    "\n",
    "- [ ] **3.** Cual es la diferencia fundamental entre TF-IDF (sparse) y embeddings de OpenAI (dense)? Da un ejemplo donde TF-IDF falla y embeddings aciertan.\n",
    "\n",
    "- [ ] **4.** Que almacena ChromaDB para cada documento? Explica el flujo completo de `collection.add()` y `collection.query()`.\n",
    "\n",
    "- [ ] **5.** Dibuja el diagrama del Agentic RAG. En que casos se dispara el loop de re-query? Y el de re-generate?\n",
    "\n",
    "- [ ] **6.** Keyword recall como metrica tiene un sesgo importante. Cual es? Que metrica usarias en produccion y por que?\n",
    "\n",
    "---\n",
    "\n",
    "*Fin del Notebook: De Texto Completo a Agentic RAG*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vector-data-bases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}