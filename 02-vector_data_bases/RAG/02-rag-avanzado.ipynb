{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Henry Logo](https://www.soyhenry.com/_next/static/media/HenryLogo.bb57fd6f.svg)\n",
    "\n",
    "# RAG Avanzado - Patrones de Produccion\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar esta notebook seras capaz de:\n",
    "\n",
    "1. Implementar **Multi-Query RAG** para mejorar el recall del retrieval\n",
    "2. Usar **Ensemble Retriever** combinando busqueda semantica + keyword (BM25)\n",
    "3. Aplicar **Contextual Compression** para extraer solo lo relevante de cada chunk\n",
    "4. Construir un **RAG conversacional** con memoria de sesion\n",
    "5. Evaluar la calidad del RAG con metricas cuantitativas\n",
    "6. Comparar todos los patrones side-by-side\n",
    "\n",
    "### Prerequisitos\n",
    "\n",
    "Esta notebook asume que completaste `01-rag-fundamentos.ipynb` y entiendes:\n",
    "- El pipeline basico: documentos -> chunks -> embeddings -> retrieval -> generacion\n",
    "- LCEL chains con pipe operator\n",
    "- ChromaDB como vector store\n",
    "\n",
    "### Nota sobre implementacion\n",
    "\n",
    "En esta notebook implementamos los patrones avanzados **desde los primitivos** de LangChain (prompts, runnables, retrievers). Esto permite entender exactamente que hace cada patron y adaptarlo a tu caso de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"Todas las dependencias cargadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset: NovaTech Solutions\n",
    "\n",
    "Reutilizamos la base de conocimiento de NovaTech con datos que el LLM no puede adivinar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novatech_docs_raw = [\n",
    "    \"\"\"Politica de Vacaciones de NovaTech Solutions. Todos los empleados de tiempo completo \n",
    "    tienen derecho a 22 dias habiles de vacaciones al anio, acumulables hasta un maximo de \n",
    "    44 dias. Los primeros 6 meses son periodo de prueba y solo se acumulan 1.5 dias por mes. \n",
    "    Las vacaciones deben solicitarse con al menos 15 dias de anticipacion a traves del sistema \n",
    "    interno VacaTrack v3.2. El periodo de vacaciones colectivas es del 23 de diciembre al 2 de \n",
    "    enero. Los dias no utilizados se pueden compensar economicamente al 75% del valor diario \n",
    "    del salario base. El director de cada area debe aprobar las solicitudes en un plazo maximo \n",
    "    de 3 dias habiles.\"\"\",\n",
    "\n",
    "    \"\"\"Plan de Salud Corporativo NovaTech. La empresa ofrece cobertura medica a traves de \n",
    "    MediPlus Premium para todos los empleados y sus dependientes directos (conyuge e hijos \n",
    "    menores de 25 anios). La cobertura incluye: consultas generales y especialistas con copago \n",
    "    de $150 MXN, hospitalizacion con cobertura al 90%, medicamentos con descuento del 40% en \n",
    "    FarmaRed, dental basico incluido y dental estetico con 50% de cobertura. El tope anual de \n",
    "    cobertura es de $2,000,000 MXN por beneficiario. La empresa absorbe el 80% de la prima \n",
    "    mensual y el empleado el 20% restante via nomina.\"\"\",\n",
    "\n",
    "    \"\"\"Proceso de Onboarding en NovaTech Solutions. El programa de integracion dura 4 semanas \n",
    "    y sigue la metodologia NOVA-Start. Semana 1: Setup tecnico (laptop Dell XPS 15, cuentas, \n",
    "    VPN Cisco AnyConnect, acceso a GitLab), reunion con HR y firma de NDA. Semana 2: Sesiones \n",
    "    con cada area del producto, introduccion a la arquitectura de DataPulse y al stack \n",
    "    tecnologico. Semana 3: Asignacion de buddy (mentor) y primer ticket de practica en el \n",
    "    sprint actual. Semana 4: Evaluacion de onboarding con el tech lead y definicion de OKRs \n",
    "    del primer trimestre. Cada nuevo empleado recibe $5,000 MXN en credito para setup de \n",
    "    home office.\"\"\",\n",
    "\n",
    "    \"\"\"Stack Tecnologico de NovaTech Solutions. Backend: FastAPI 0.109 sobre Python 3.12, con \n",
    "    PostgreSQL 16 como base de datos principal y Redis 7.2 para cache y colas. Frontend: \n",
    "    React 18 con TypeScript 5.3, Next.js 14 para SSR, y TailwindCSS 3.4 para estilos. \n",
    "    Infraestructura: AWS con EKS (Kubernetes 1.28), Terraform para IaC, ArgoCD para GitOps \n",
    "    y Datadog para observabilidad. CI/CD: GitLab CI con runners dedicados en m6i.xlarge, \n",
    "    pipelines de ~8 minutos promedio. El monorepo tiene 340,000 lineas de codigo y 94% de \n",
    "    cobertura de tests. Arquitectura de microservicios con 23 servicios comunicados via gRPC \n",
    "    y eventos en Apache Kafka 3.6.\"\"\",\n",
    "\n",
    "    \"\"\"Sistema de Evaluaciones de NovaTech. Las evaluaciones de desempeno se realizan cada 6 \n",
    "    meses usando la plataforma NovaPerf v2.1. El proceso incluye: autoevaluacion (20% del \n",
    "    peso), evaluacion del manager directo (40%), evaluacion de peers (30%) y evaluacion de \n",
    "    reportes directos si aplica (10%). La escala es de 1 a 5: 1-Necesita mejora, 2-En \n",
    "    desarrollo, 3-Cumple expectativas, 4-Supera expectativas, 5-Excepcional. Los empleados \n",
    "    con calificacion 4+ son elegibles para el programa de Fast Track que incluye aumento \n",
    "    salarial del 15-20% y acceso a proyectos estrategicos.\"\"\",\n",
    "\n",
    "    \"\"\"Politica de Trabajo Remoto NovaTech. La empresa opera bajo modelo hibrido 3-2: 3 dias \n",
    "    en oficina (martes, miercoles, jueves) y 2 dias remotos (lunes, viernes). Las oficinas \n",
    "    principales estan en Ciudad de Mexico (Torre NOVA, piso 12-15, Santa Fe) y Guadalajara \n",
    "    (WeWork Midtown). Se permite trabajo 100% remoto temporal por hasta 4 semanas al anio \n",
    "    con aprobacion del VP. Subsidio mensual de internet: $800 MXN. Silla ergonomica Herman \n",
    "    Miller Aeron en comodato. Reuniones de all-hands los miercoles a las 10:00 AM CST.\"\"\",\n",
    "\n",
    "    \"\"\"Programa de Capacitacion y Desarrollo NovaTech. Cada empleado cuenta con un presupuesto \n",
    "    anual de $25,000 MXN para capacitacion profesional. Esto cubre: cursos online (Udemy, \n",
    "    Coursera, Platzi), conferencias (hasta 2 al anio con viaticos incluidos), certificaciones \n",
    "    tecnicas (AWS, GCP, Kubernetes). La plataforma interna LearnHub tiene 150+ cursos creados \n",
    "    por empleados senior. Los viernes de 2-4 PM son 'Learning Hours' protegidas. El programa \n",
    "    de mentoria NOVA-Grow conecta juniors con seniors por 6 meses con sesiones quincenales.\"\"\",\n",
    "\n",
    "    \"\"\"DataPulse - Producto Principal de NovaTech Solutions. DataPulse es una plataforma de \n",
    "    analytics en tiempo real para empresas de e-commerce y fintech. Procesa en promedio 2.3 \n",
    "    millones de eventos por minuto con una latencia p99 de 45ms. El SLA garantizado es de \n",
    "    99.95% de disponibilidad mensual, con creditos automaticos del 10% de la factura por cada \n",
    "    0.01% por debajo del SLA. DataPulse tiene 3 tiers: Starter ($2,500 USD/mes, hasta 10M \n",
    "    eventos/dia), Business ($8,000 USD/mes, hasta 100M eventos/dia) y Enterprise (precio \n",
    "    custom, eventos ilimitados, soporte 24/7). Actualmente 847 clientes en 12 paises.\"\"\",\n",
    "\n",
    "    \"\"\"Protocolo de Manejo de Incidentes NovaTech. Los incidentes se clasifican en 4 niveles: \n",
    "    SEV1 (caida total del servicio, respuesta en 5 minutos, war room inmediato), SEV2 \n",
    "    (degradacion significativa, respuesta en 15 minutos), SEV3 (impacto menor, respuesta en \n",
    "    1 hora) y SEV4 (cosmetico, siguiente sprint). El on-call rota semanalmente entre 12 \n",
    "    ingenieros senior con compensacion de $8,000 MXN por semana de guardia. Post-mortem \n",
    "    obligatorio para SEV1 y SEV2 dentro de 48 horas. El MTTR objetivo es de 30 minutos \n",
    "    para SEV1. Se usa PagerDuty para alertas y Slack #incidents para coordinacion.\"\"\",\n",
    "\n",
    "    \"\"\"Beneficios Adicionales NovaTech Solutions. Ademas del salario base, los empleados \n",
    "    reciben: vales de despensa por $3,500 MXN mensuales (TotalPass), fondo de ahorro con \n",
    "    aportacion patronal del 8%, seguro de vida por 48 meses de sueldo, bono anual basado \n",
    "    en desempeno (target del 15% del salario anual), programa de stock options despues de \n",
    "    2 anios de antiguedad (vesting de 4 anios con cliff de 1 anio), descuento del 30% en \n",
    "    gimnasios (Sport City, Smart Fit), y acceso a la app de salud mental TherapyChat con \n",
    "    4 sesiones mensuales incluidas. Employee Referral Bonus: $15,000 MXN.\"\"\",\n",
    "\n",
    "    \"\"\"Arquitectura Tecnica de DataPulse. El sistema usa una arquitectura event-driven con \n",
    "    Apache Kafka como backbone de mensajeria. Los eventos ingresan via API Gateway (Kong) y \n",
    "    se procesan en 3 pipelines: real-time (Apache Flink para agregaciones sub-segundo), \n",
    "    near-real-time (Apache Spark Structured Streaming para enriquecimiento) y batch (Apache \n",
    "    Spark para reportes historicos). Almacenamiento: lakehouse con Delta Lake sobre S3, \n",
    "    ClickHouse para queries OLAP y Redis para dashboards en tiempo real. ML pipeline con \n",
    "    MLflow para experiment tracking y Kubeflow para training. Los modelos de deteccion de \n",
    "    anomalias se reentrenan cada 24 horas con los ultimos 30 dias de datos.\"\"\",\n",
    "\n",
    "    \"\"\"Proceso de Desarrollo en NovaTech. Usamos Scrum con sprints de 2 semanas. Cada equipo \n",
    "    (squad) tiene 5-7 personas: 1 PM, 1 tech lead, 3-4 developers y 1 QA. Hay 8 squads: \n",
    "    Ingest, Processing, Storage, Analytics, API, Frontend, Platform e Infra. Definition of \n",
    "    Done: code review aprobado por 2 peers, tests unitarios >90% cobertura, tests de \n",
    "    integracion, documentacion en Notion, y validacion de QA en staging. Deployments \n",
    "    automaticos via ArgoCD con canary releases (5% -> 25% -> 100%) y rollback automatico \n",
    "    si el error rate supera 0.1%.\"\"\",\n",
    "\n",
    "    \"\"\"Politica de Seguridad Informatica NovaTech. Todos los empleados completan el \n",
    "    entrenamiento SecureNova cada 6 meses. Contrasenas de minimo 16 caracteres, rotacion \n",
    "    cada 90 dias. MFA obligatoria con YubiKey 5 NFC para sistemas criticos. Escaneo de \n",
    "    vulnerabilidades semanal con Qualys, findings criticas se resuelven en 48 horas. \n",
    "    Pentest externos trimestrales con CyberGuard MX. El equipo de seguridad (5 personas) \n",
    "    reporta al CTO. Se prohibe el uso de USB externos y todo acceso a produccion requiere \n",
    "    aprobacion via Teleport con grabacion de sesion.\"\"\",\n",
    "]\n",
    "\n",
    "# Chunking\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "documents = []\n",
    "for i, doc_text in enumerate(novatech_docs_raw):\n",
    "    titulo = doc_text.strip().split('.')[0]\n",
    "    chunks = text_splitter.split_text(doc_text)\n",
    "    for chunk in chunks:\n",
    "        documents.append(Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\"source\": titulo, \"doc_id\": i}\n",
    "        ))\n",
    "\n",
    "print(f\"Documentos originales: {len(novatech_docs_raw)}\")\n",
    "print(f\"Chunks generados: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vectorstore base (lo reutilizaremos en todos los patrones)\n",
    "vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"Vectorstore creado con {vectorstore._collection.count()} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## RAG Baseline\n",
    "\n",
    "Primero creamos el RAG basico como punto de comparacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_base = \"\"\"Responde la pregunta basandote UNICAMENTE en el contexto proporcionado.\n",
    "Si la informacion no esta en el contexto, di \\\"No tengo esa informacion\\\".\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "\n",
    "prompt_base = ChatPromptTemplate.from_template(template_base)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Baseline chain\n",
    "baseline_chain = (\n",
    "    {\"context\": base_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_base\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test rapido\n",
    "resp = baseline_chain.invoke(\"cuantos dias de vacaciones tienen los empleados?\")\n",
    "print(\"Baseline RAG:\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Seccion 1: Multi-Query RAG\n",
    "\n",
    "### El problema\n",
    "\n",
    "Una sola query puede no capturar todos los angulos de la pregunta del usuario. Por ejemplo:\n",
    "\n",
    "- Query: *\"que beneficios economicos ofrece NovaTech\"*\n",
    "- Documentos relevantes estan en: vacaciones (compensacion), salud (copagos), beneficios (vales, bonos), capacitacion (presupuesto)\n",
    "\n",
    "Con una sola query, el retriever puede perder algunos de estos documentos.\n",
    "\n",
    "### La solucion\n",
    "\n",
    "**Multi-Query** usa el LLM para generar multiples versiones de la pregunta, hace retrieval con cada una, y combina los resultados (union de documentos unicos).\n",
    "\n",
    "```\n",
    "Query original\n",
    "      |\n",
    "      v\n",
    "  LLM genera 3 queries\n",
    "      |\n",
    "      +---> Query 1 ---> Retrieval ---> Docs A, B\n",
    "      +---> Query 2 ---> Retrieval ---> Docs B, C\n",
    "      +---> Query 3 ---> Retrieval ---> Docs C, D\n",
    "      |\n",
    "      v\n",
    "  Union: Docs A, B, C, D  (mas recall)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacion de Multi-Query desde cero\n",
    "multi_query_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Tu tarea es generar 3 versiones diferentes de la pregunta del usuario \n",
    "para mejorar la busqueda en una base de datos de documentos. \n",
    "Cada version debe abordar la pregunta desde un angulo distinto.\n",
    "\n",
    "Pregunta original: {question}\n",
    "\n",
    "Genera exactamente 3 preguntas alternativas, una por linea, sin numeros ni viÃ±etas:\"\"\"\n",
    ")\n",
    "\n",
    "generate_queries_chain = multi_query_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def multi_query_retrieve(question: str) -> list[Document]:\n",
    "    \"\"\"Genera multiples queries y combina los resultados de retrieval.\"\"\"\n",
    "    # Generar queries alternativas\n",
    "    alt_queries_text = generate_queries_chain.invoke({\"question\": question})\n",
    "    alt_queries = [q.strip() for q in alt_queries_text.strip().split(\"\\n\") if q.strip()]\n",
    "\n",
    "    print(f\"  Queries generadas:\")\n",
    "    for q in alt_queries:\n",
    "        print(f\"    - {q}\")\n",
    "\n",
    "    # Retrieval con cada query (incluyendo la original)\n",
    "    all_queries = [question] + alt_queries\n",
    "    seen_contents = set()\n",
    "    unique_docs = []\n",
    "\n",
    "    for q in all_queries:\n",
    "        docs = base_retriever.invoke(q)\n",
    "        for doc in docs:\n",
    "            if doc.page_content not in seen_contents:\n",
    "                seen_contents.add(doc.page_content)\n",
    "                unique_docs.append(doc)\n",
    "\n",
    "    return unique_docs\n",
    "\n",
    "\n",
    "# Probar\n",
    "query_mq = \"que beneficios economicos ofrece NovaTech a sus empleados\"\n",
    "print(f\"Query original: '{query_mq}'\")\n",
    "docs_mq = multi_query_retrieve(query_mq)\n",
    "\n",
    "print(f\"\\nDocumentos recuperados: {len(docs_mq)}\")\n",
    "print(\"=\" * 60)\n",
    "sources = set()\n",
    "for i, doc in enumerate(docs_mq, 1):\n",
    "    source = doc.metadata.get('source', 'N/A')\n",
    "    sources.add(source)\n",
    "    print(f\"  {i}. [{source}] {doc.page_content[:80]}...\")\n",
    "\n",
    "print(f\"\\nFuentes unicas: {len(sources)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar cobertura: baseline vs multi-query\n",
    "docs_baseline = base_retriever.invoke(query_mq)\n",
    "\n",
    "sources_baseline = set(d.metadata.get('source', '') for d in docs_baseline)\n",
    "sources_mq = set(d.metadata.get('source', '') for d in docs_mq)\n",
    "\n",
    "print(\"Comparacion de cobertura:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Baseline (k=3): {len(docs_baseline)} docs, {len(sources_baseline)} fuentes\")\n",
    "print(f\"  Fuentes: {sources_baseline}\")\n",
    "print(f\"\\nMulti-Query: {len(docs_mq)} docs, {len(sources_mq)} fuentes\")\n",
    "print(f\"  Fuentes: {sources_mq}\")\n",
    "print(f\"\\nFuentes adicionales con Multi-Query: {sources_mq - sources_baseline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Query chain\n",
    "def mq_retriever_fn(question: str) -> str:\n",
    "    docs = multi_query_retrieve(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "\n",
    "mq_chain = (\n",
    "    {\"context\": RunnableLambda(mq_retriever_fn), \"question\": RunnablePassthrough()}\n",
    "    | prompt_base\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "resp_mq = mq_chain.invoke(query_mq)\n",
    "print(\"\\nMulti-Query RAG:\")\n",
    "print(resp_mq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Seccion 2: Ensemble Retriever (Semantica + BM25)\n",
    "\n",
    "### El problema\n",
    "\n",
    "La busqueda semantica es excelente para capturar significado, pero puede fallar con:\n",
    "- Nombres propios: \"VacaTrack\", \"NovaPerf\", \"DataPulse\"\n",
    "- Numeros especificos: \"99.95%\", \"$25,000\"\n",
    "- Acronimos: \"SEV1\", \"MTTR\", \"OKRs\"\n",
    "\n",
    "**BM25** es un algoritmo de busqueda por keywords (similar a TF-IDF) que es excelente en estos casos.\n",
    "\n",
    "### La solucion\n",
    "\n",
    "El **Ensemble Retriever** combina multiples retrievers con **Reciprocal Rank Fusion (RRF)**:\n",
    "\n",
    "```\n",
    "Query\n",
    "  |\n",
    "  +---> Semantic Retriever ---> Ranking semantico\n",
    "  +---> BM25 Retriever    ---> Ranking por keywords\n",
    "  |\n",
    "  v\n",
    "  RRF: score(doc) = sum(1 / (k + rank_i))  ---> Docs combinados\n",
    "```\n",
    "\n",
    "RRF combina rankings sin necesidad de normalizar los scores de cada retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear BM25 retriever (busqueda por keywords)\n",
    "bm25_retriever = BM25Retriever.from_documents(documents, k=3)\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(rankings: list[list[Document]], k: int = 60) -> list[Document]:\n",
    "    \"\"\"Combina multiples rankings usando Reciprocal Rank Fusion.\n",
    "\n",
    "    RRF score para cada documento = sum(1 / (k + rank_i)) para cada ranking.\n",
    "    k=60 es el valor estandar de la literatura (Cormack et al., 2009).\n",
    "    \"\"\"\n",
    "    doc_scores = {}  # page_content -> (score, Document)\n",
    "    for ranking in rankings:\n",
    "        for rank, doc in enumerate(ranking):\n",
    "            content = doc.page_content\n",
    "            score = 1.0 / (k + rank + 1)\n",
    "            if content in doc_scores:\n",
    "                doc_scores[content] = (doc_scores[content][0] + score, doc)\n",
    "            else:\n",
    "                doc_scores[content] = (score, doc)\n",
    "\n",
    "    # Ordenar por score descendente\n",
    "    sorted_docs = sorted(doc_scores.values(), key=lambda x: x[0], reverse=True)\n",
    "    return [doc for _, doc in sorted_docs]\n",
    "\n",
    "\n",
    "def ensemble_retrieve(question: str, top_k: int = 5) -> list[Document]:\n",
    "    \"\"\"Ensemble: semantic + BM25 con Reciprocal Rank Fusion.\"\"\"\n",
    "    semantic_docs = base_retriever.invoke(question)\n",
    "    bm25_docs = bm25_retriever.invoke(question)\n",
    "    fused = reciprocal_rank_fusion([semantic_docs, bm25_docs])\n",
    "    return fused[:top_k]\n",
    "\n",
    "\n",
    "print(\"Ensemble Retriever creado (semantic + BM25 con RRF)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar donde el ensemble gana: busqueda con terminos especificos\n",
    "queries_especificas = [\n",
    "    \"que es VacaTrack v3.2\",               # nombre de sistema interno\n",
    "    \"cual es el MTTR objetivo para SEV1\",   # acronimos tecnicos\n",
    "    \"cuanto es el copago en MediPlus\",      # nombre de plan + numero\n",
    "]\n",
    "\n",
    "print(\"Comparacion: Semantica pura vs BM25 vs Ensemble\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in queries_especificas:\n",
    "    docs_sem = base_retriever.invoke(q)\n",
    "    docs_bm25 = bm25_retriever.invoke(q)\n",
    "    docs_ens = ensemble_retrieve(q)\n",
    "\n",
    "    src_sem = set(d.metadata.get('source', '')[:30] for d in docs_sem)\n",
    "    src_bm25 = set(d.metadata.get('source', '')[:30] for d in docs_bm25)\n",
    "    src_ens = set(d.metadata.get('source', '')[:30] for d in docs_ens)\n",
    "\n",
    "    print(f\"\\nQuery: '{q}'\")\n",
    "    print(f\"  Semantica ({len(docs_sem)} docs): {src_sem}\")\n",
    "    print(f\"  BM25 ({len(docs_bm25)} docs):      {src_bm25}\")\n",
    "    print(f\"  Ensemble ({len(docs_ens)} docs):  {src_ens}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble chain\n",
    "def ensemble_retriever_fn(question: str) -> str:\n",
    "    docs = ensemble_retrieve(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "\n",
    "ensemble_chain = (\n",
    "    {\"context\": RunnableLambda(ensemble_retriever_fn), \"question\": RunnablePassthrough()}\n",
    "    | prompt_base\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "resp_ens = ensemble_chain.invoke(\"cual es el MTTR objetivo para incidentes SEV1\")\n",
    "print(\"Ensemble RAG:\")\n",
    "print(resp_ens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Seccion 3: Contextual Compression\n",
    "\n",
    "### El problema\n",
    "\n",
    "Los chunks recuperados contienen mucho texto irrelevante para la pregunta. Si la pregunta es *\"cuantos dias de vacaciones\"*, el chunk de 500 caracteres incluye datos sobre VacaTrack, compensacion, periodo colectivo, etc.\n",
    "\n",
    "Esto desperdicia tokens del contexto y puede confundir al LLM.\n",
    "\n",
    "### La solucion\n",
    "\n",
    "**Contextual Compression** usa un LLM para extraer solo las partes relevantes:\n",
    "\n",
    "```\n",
    "Chunk completo (500 chars)\n",
    "      |\n",
    "      v\n",
    "  LLM: \"extrae solo lo relevante para la query\"\n",
    "      |\n",
    "      v\n",
    "  Chunk comprimido (50-100 chars)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacion de Contextual Compression desde cero\n",
    "compress_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Dado el siguiente documento y la pregunta, extrae SOLO las partes del documento \n",
    "que son directamente relevantes para responder la pregunta. \n",
    "Si ninguna parte es relevante, responde exactamente: IRRELEVANTE\n",
    "\n",
    "Documento:\n",
    "{document}\n",
    "\n",
    "Pregunta: {question}\n",
    "\n",
    "Contenido relevante extraido:\"\"\"\n",
    ")\n",
    "\n",
    "compress_chain = compress_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def compression_retrieve(question: str) -> list[Document]:\n",
    "    \"\"\"Recupera documentos y comprime cada uno al contenido relevante.\"\"\"\n",
    "    raw_docs = base_retriever.invoke(question)\n",
    "    compressed_docs = []\n",
    "\n",
    "    for doc in raw_docs:\n",
    "        compressed_text = compress_chain.invoke({\n",
    "            \"document\": doc.page_content,\n",
    "            \"question\": question,\n",
    "        })\n",
    "        if compressed_text.strip().upper() != \"IRRELEVANTE\":\n",
    "            compressed_docs.append(Document(\n",
    "                page_content=compressed_text,\n",
    "                metadata=doc.metadata,\n",
    "            ))\n",
    "\n",
    "    return compressed_docs\n",
    "\n",
    "\n",
    "print(\"Contextual Compression implementado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar la compresion\n",
    "query_comp = \"cuantos dias de vacaciones tienen los empleados\"\n",
    "\n",
    "docs_sin_comp = base_retriever.invoke(query_comp)\n",
    "docs_con_comp = compression_retrieve(query_comp)\n",
    "\n",
    "print(f\"Query: '{query_comp}'\")\n",
    "print()\n",
    "print(\"SIN compresion (chunks completos):\")\n",
    "print(\"=\" * 60)\n",
    "total_chars_sin = 0\n",
    "for i, doc in enumerate(docs_sin_comp, 1):\n",
    "    total_chars_sin += len(doc.page_content)\n",
    "    print(f\"  Doc {i} ({len(doc.page_content)} chars): {doc.page_content[:120]}...\")\n",
    "\n",
    "print(f\"\\n  Total: {total_chars_sin} caracteres\")\n",
    "\n",
    "print()\n",
    "print(\"CON compresion (solo lo relevante):\")\n",
    "print(\"=\" * 60)\n",
    "total_chars_con = 0\n",
    "for i, doc in enumerate(docs_con_comp, 1):\n",
    "    total_chars_con += len(doc.page_content)\n",
    "    print(f\"  Doc {i} ({len(doc.page_content)} chars): {doc.page_content}\")\n",
    "\n",
    "print(f\"\\n  Total: {total_chars_con} caracteres\")\n",
    "if total_chars_sin > 0:\n",
    "    print(f\"  Reduccion: {(1 - total_chars_con/total_chars_sin)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compression chain\n",
    "def compression_retriever_fn(question: str) -> str:\n",
    "    docs = compression_retrieve(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "\n",
    "compression_chain = (\n",
    "    {\"context\": RunnableLambda(compression_retriever_fn), \"question\": RunnablePassthrough()}\n",
    "    | prompt_base\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "resp_comp = compression_chain.invoke(query_comp)\n",
    "print(\"Contextual Compression RAG:\")\n",
    "print(resp_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Seccion 4: RAG Conversacional con Memoria\n",
    "\n",
    "### El problema\n",
    "\n",
    "Un RAG basico no recuerda preguntas anteriores. Si el usuario pregunta:\n",
    "1. \"Cuantos dias de vacaciones hay?\" -> \"22 dias\"\n",
    "2. \"Y como se solicitan?\" -> El RAG no sabe que \"se\" refiere a \"vacaciones\"\n",
    "\n",
    "### La solucion\n",
    "\n",
    "**RAG Conversacional** reformula la pregunta del usuario usando el historial del chat antes de hacer retrieval:\n",
    "\n",
    "```\n",
    "Historial + Pregunta nueva\n",
    "           |\n",
    "           v\n",
    "   LLM: \"reformula la pregunta de forma autonoma\"\n",
    "           |\n",
    "           v\n",
    "   Pregunta reformulada ---> Retrieval ---> Generacion\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt para reformular la pregunta con historial\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Dado el historial de la conversacion y la ultima pregunta del usuario \n",
    "(que podria hacer referencia al historial), reformula la pregunta para que sea \n",
    "autonoma y se entienda sin el historial. NO respondas la pregunta, solo reformulala. \n",
    "Si ya es autonoma, devuelvela tal cual.\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Chain para reformular\n",
    "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Test de reformulacion\n",
    "test_history = [\n",
    "    HumanMessage(content=\"Cuantos dias de vacaciones tienen los empleados?\"),\n",
    "    AIMessage(content=\"Los empleados tienen 22 dias habiles de vacaciones al anio.\"),\n",
    "]\n",
    "\n",
    "reformulada = contextualize_chain.invoke({\n",
    "    \"chat_history\": test_history,\n",
    "    \"input\": \"Y como se solicitan?\",\n",
    "})\n",
    "\n",
    "print(\"Pregunta original: 'Y como se solicitan?'\")\n",
    "print(f\"Pregunta reformulada: '{reformulada}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG conversacional completo\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Eres un asistente de NovaTech Solutions. Responde basandote UNICAMENTE \n",
    "en el contexto proporcionado. Si no tienes la informacion, dilo.\n",
    "\n",
    "Contexto:\n",
    "{context}\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])\n",
    "\n",
    "\n",
    "def contextualize_and_retrieve(input_dict):\n",
    "    \"\"\"Reformula la pregunta si hay historial y luego recupera documentos.\"\"\"\n",
    "    chat_history = input_dict.get(\"chat_history\", [])\n",
    "    question = input_dict[\"input\"]\n",
    "\n",
    "    if chat_history:\n",
    "        reformulated = contextualize_chain.invoke({\n",
    "            \"chat_history\": chat_history,\n",
    "            \"input\": question,\n",
    "        })\n",
    "    else:\n",
    "        reformulated = question\n",
    "\n",
    "    docs = base_retriever.invoke(reformulated)\n",
    "    return format_docs(docs)\n",
    "\n",
    "\n",
    "conversational_chain = (\n",
    "    RunnablePassthrough.assign(context=RunnableLambda(contextualize_and_retrieve))\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Almacen de sesiones\n",
    "session_store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in session_store:\n",
    "        session_store[session_id] = InMemoryChatMessageHistory()\n",
    "    return session_store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag = RunnableWithMessageHistory(\n",
    "    conversational_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print(\"RAG Conversacional creado con soporte de sesiones.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular una conversacion\n",
    "session_id = \"demo_session_1\"\n",
    "config = {\"configurable\": {\"session_id\": session_id}}\n",
    "\n",
    "conversacion = [\n",
    "    \"Cuantos dias de vacaciones tienen los empleados?\",\n",
    "    \"Y como se solicitan?\",\n",
    "    \"Que pasa si no los uso?\",\n",
    "    \"Cambiando de tema, que es DataPulse?\",\n",
    "    \"Cual es su SLA?\",\n",
    "]\n",
    "\n",
    "print(\"CONVERSACION CON RAG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for pregunta in conversacion:\n",
    "    respuesta = conversational_rag.invoke(\n",
    "        {\"input\": pregunta},\n",
    "        config=config,\n",
    "    )\n",
    "    print(f\"\\nUsuario: {pregunta}\")\n",
    "    print(f\"Asistente: {respuesta}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Observa:** El chatbot entiende que \"se solicitan\" se refiere a las vacaciones, que \"no los uso\" se refiere a los dias, y que \"su SLA\" se refiere a DataPulse. Todo gracias a la reformulacion con historial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Seccion 5: Evaluacion Cuantitativa\n",
    "\n",
    "Comparemos todos los patrones con un conjunto de preguntas de evaluacion. Para cada pregunta medimos:\n",
    "\n",
    "- **Keyword recall**: proporcion de keywords esperadas que aparecen en la respuesta\n",
    "- **Latencia**: tiempo total de ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = [\n",
    "    {\n",
    "        \"query\": \"Cuantos dias de vacaciones tienen los empleados y como se solicitan?\",\n",
    "        \"keywords\": [\"22\", \"VacaTrack\", \"15 dias\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Que tecnologias usa NovaTech en el backend y frontend?\",\n",
    "        \"keywords\": [\"FastAPI\", \"PostgreSQL\", \"React\", \"TypeScript\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Que es DataPulse, cual es su SLA y cuantos clientes tiene?\",\n",
    "        \"keywords\": [\"99.95\", \"847\", \"analytics\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Como funciona el manejo de incidentes SEV1 y cual es el MTTR?\",\n",
    "        \"keywords\": [\"5 minutos\", \"30 minutos\", \"MTTR\", \"PagerDuty\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Que beneficios economicos reciben los empleados ademas del salario?\",\n",
    "        \"keywords\": [\"3,500\", \"stock options\", \"15,000\", \"fondo de ahorro\"],\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Como es el proceso de onboarding y que equipo recibe el nuevo empleado?\",\n",
    "        \"keywords\": [\"4 semanas\", \"buddy\", \"Dell XPS\", \"5,000\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def keyword_recall(answer, keywords):\n",
    "    answer_lower = answer.lower()\n",
    "    hits = sum(1 for kw in keywords if kw.lower() in answer_lower)\n",
    "    return hits / len(keywords)\n",
    "\n",
    "\n",
    "print(f\"Evaluacion con {len(eval_set)} preguntas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los patrones a evaluar (sin prints internos para multi-query)\n",
    "def mq_retriever_fn_quiet(question: str) -> str:\n",
    "    \"\"\"Multi-query sin prints.\"\"\"\n",
    "    alt_queries_text = generate_queries_chain.invoke({\"question\": question})\n",
    "    alt_queries = [q.strip() for q in alt_queries_text.strip().split(\"\\n\") if q.strip()]\n",
    "    all_queries = [question] + alt_queries\n",
    "    seen_contents = set()\n",
    "    unique_docs = []\n",
    "    for q in all_queries:\n",
    "        for doc in base_retriever.invoke(q):\n",
    "            if doc.page_content not in seen_contents:\n",
    "                seen_contents.add(doc.page_content)\n",
    "                unique_docs.append(doc)\n",
    "    return format_docs(unique_docs)\n",
    "\n",
    "\n",
    "mq_chain_quiet = (\n",
    "    {\"context\": RunnableLambda(mq_retriever_fn_quiet), \"question\": RunnablePassthrough()}\n",
    "    | prompt_base\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "patterns = {\n",
    "    \"Baseline\": baseline_chain,\n",
    "    \"Multi-Query\": mq_chain_quiet,\n",
    "    \"Ensemble\": ensemble_chain,\n",
    "    \"Compression\": compression_chain,\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for pattern_name, chain in patterns.items():\n",
    "    print(f\"Evaluando: {pattern_name}...\")\n",
    "    for eval_item in eval_set:\n",
    "        q = eval_item[\"query\"]\n",
    "        kws = eval_item[\"keywords\"]\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        answer = chain.invoke(q)\n",
    "        latency = time.perf_counter() - start\n",
    "\n",
    "        recall = keyword_recall(answer, kws)\n",
    "\n",
    "        results.append({\n",
    "            \"pattern\": pattern_name,\n",
    "            \"query\": q[:40] + \"...\",\n",
    "            \"recall\": recall,\n",
    "            \"latency\": latency,\n",
    "            \"answer_len\": len(answer),\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nEvaluacion completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen\n",
    "summary = df_results.groupby(\"pattern\").agg(\n",
    "    recall_promedio=(\"recall\", \"mean\"),\n",
    "    latencia_promedio=(\"latency\", \"mean\"),\n",
    "    respuesta_promedio=(\"answer_len\", \"mean\"),\n",
    ").round(3)\n",
    "\n",
    "order = [\"Baseline\", \"Multi-Query\", \"Ensemble\", \"Compression\"]\n",
    "summary = summary.reindex(order)\n",
    "\n",
    "print(\"Resumen por patron:\")\n",
    "print(\"=\" * 60)\n",
    "print(summary.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizacion: 3 paneles\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors_patterns = ['#95a5a6', '#3498db', '#2ecc71', '#f39c12']\n",
    "\n",
    "# Recall\n",
    "axes[0].bar(order, summary.loc[order, 'recall_promedio'],\n",
    "            color=colors_patterns, edgecolor='black', linewidth=0.5)\n",
    "axes[0].set_title('Keyword Recall Promedio')\n",
    "axes[0].set_ylabel('Recall (0-1)')\n",
    "axes[0].set_ylim(0, 1.1)\n",
    "for i, v in enumerate(summary.loc[order, 'recall_promedio']):\n",
    "    axes[0].text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# Latencia\n",
    "axes[1].bar(order, summary.loc[order, 'latencia_promedio'],\n",
    "            color=colors_patterns, edgecolor='black', linewidth=0.5)\n",
    "axes[1].set_title('Latencia Promedio (s)')\n",
    "axes[1].set_ylabel('Segundos')\n",
    "for i, v in enumerate(summary.loc[order, 'latencia_promedio']):\n",
    "    axes[1].text(i, v + 0.1, f\"{v:.1f}\", ha='center', fontweight='bold')\n",
    "\n",
    "# Recall por query (heatmap)\n",
    "pivot = df_results.pivot(index='query', columns='pattern', values='recall')\n",
    "pivot = pivot[order]\n",
    "sns.heatmap(pivot, annot=True, fmt='.2f', cmap='RdYlGn', vmin=0, vmax=1,\n",
    "            ax=axes[2], linewidths=0.5)\n",
    "axes[2].set_title('Recall por Query x Patron')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Seccion 6: Cuando Usar Cada Patron\n",
    "\n",
    "| Patron | Ventaja principal | Costo extra | Mejor para |\n",
    "|--------|------------------|-------------|------------|\n",
    "| **Baseline** | Simple, rapido, predecible | Ninguno | Prototipos, preguntas directas |\n",
    "| **Multi-Query** | Mayor recall, cubre mas angulos | 1 LLM call extra + N retrievals | Preguntas complejas que tocan multiples temas |\n",
    "| **Ensemble** | Combina semantica + keywords | BM25 index en memoria | Datos con nombres propios, acronimos, numeros |\n",
    "| **Compression** | Contexto preciso, menos tokens | 1 LLM call por chunk recuperado | Chunks grandes, contexto limitado |\n",
    "| **Conversacional** | Soporta dialogo multi-turno | 1 LLM call para reformular | Chatbots, asistentes interactivos |\n",
    "\n",
    "### Combinaciones comunes en produccion\n",
    "\n",
    "1. **Chatbot empresarial**: Conversacional + Ensemble\n",
    "2. **Q&A sobre documentos**: Multi-Query + Compression\n",
    "3. **Busqueda interna**: Ensemble (para nombres y codigos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumen\n",
    "\n",
    "En esta notebook implementamos 5 patrones avanzados de RAG **desde los primitivos**:\n",
    "\n",
    "| Patron | Que hace | Problema que resuelve |\n",
    "|--------|----------|-----------------------|\n",
    "| Multi-Query | Genera multiples queries con LLM | Bajo recall con una sola query |\n",
    "| Ensemble + RRF | Combina semantic + BM25 retrieval | Falla con nombres propios y numeros |\n",
    "| Compression | Extrae solo lo relevante de cada chunk | Contexto ruidoso, desperdicio de tokens |\n",
    "| Conversacional | Reformula con historial de chat | Preguntas dependientes del contexto |\n",
    "| Evaluacion | Compara patrones con metricas | No saber cual patron usar |\n",
    "\n",
    "### Checklist de consolidacion\n",
    "\n",
    "- [ ] Puedo explicar cuando Multi-Query mejora sobre el baseline\n",
    "- [ ] Entiendo la diferencia entre busqueda semantica y BM25\n",
    "- [ ] Se como funciona Reciprocal Rank Fusion para combinar rankings\n",
    "- [ ] Comprendo como funciona la compresion contextual\n",
    "- [ ] Puedo construir un RAG conversacional con memoria de sesion\n",
    "- [ ] Se evaluar y comparar patrones con metricas cuantitativas\n",
    "- [ ] Puedo elegir el patron adecuado segun el caso de uso"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}