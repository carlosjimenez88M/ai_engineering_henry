{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258c1269",
   "metadata": {},
   "source": [
    "![Henry Logo](https://www.soyhenry.com/_next/static/media/HenryLogo.bb57fd6f.svg)\n",
    "\n",
    "# Ejercicio Sencillo Agent2Agent con Batman + RAG\n",
    "\n",
    "## Objetivo\n",
    "Implementar un flujo **agent2agent** minimalista y util en AI Engineering aplicado:\n",
    "- **Agent 1 (Detective Retriever)**: recupera evidencia desde la base vectorial.\n",
    "- **Agent 2 (Profesor Synthesizer)**: construye la respuesta final sustentada en contexto.\n",
    "\n",
    "Si la respuesta no esta suficientemente fundamentada, el orquestador solicita un segundo ciclo de retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72558fc",
   "metadata": {},
   "source": [
    "## Diagrama del ejercicio\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "  A[\"Pregunta del usuario\"] --> B[\"Detective Retriever Agent\"]\n",
    "  B --> C[\"Contexto recuperado\"]\n",
    "  C --> D[\"Profesor Synthesizer Agent\"]\n",
    "  D --> E[\"Evaluacion de grounding\"]\n",
    "  E -->|\"bajo\"| B\n",
    "  E -->|\"suficiente\"| F[\"Respuesta final\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jkk0z1p54m",
   "metadata": {},
   "source": [
    "## Marco teorico: El patron Agent2Agent\n",
    "\n",
    "### Definicion formal\n",
    "\n",
    "El patron **Agent2Agent** (A2A) organiza un sistema en torno a dos o mas agentes con responsabilidades claramente separadas que colaboran a traves de un orquestador. En este ejercicio:\n",
    "\n",
    "- **Agent 1 — Detective Retriever**: responsable exclusivamente de la recuperacion de evidencia desde la base vectorial. No genera texto natural; su output es un conjunto de documentos con metadata.\n",
    "- **Agent 2 — Profesor Synthesizer**: responsable de la generacion de respuesta. Recibe contexto (los documentos del Agent 1) y produce una respuesta citada y coherente. No interactua con la base vectorial.\n",
    "- **Orquestador**: media la comunicacion entre agentes y aplica un control de calidad (grounding check) antes de entregar la respuesta final. Si la calidad es insuficiente, solicita un segundo ciclo.\n",
    "\n",
    "### Ventajas sobre un pipeline monolitico\n",
    "\n",
    "| Aspecto | Monolitico (todo en un paso) | Agent2Agent |\n",
    "|---|---|---|\n",
    "| **Trazabilidad** | Dificil saber que parte del pipeline fallo | Se puede auditar el output de cada agente independientemente |\n",
    "| **Modularidad** | Cambiar el retriever implica cambiar todo | Se puede reemplazar un agente sin afectar al otro |\n",
    "| **Control de calidad** | El grounding check se aplica al final, sin opcion de remediar | El orquestador puede solicitar un segundo ciclo con parametros ajustados |\n",
    "| **Depuracion** | Los logs mezclan retrieval y generacion | El dialogo entre agentes genera una traza natural y legible |\n",
    "\n",
    "### Relacion con \"tool use\" en agentes LLM\n",
    "\n",
    "Este patron es conceptualmente analogo al **tool use** de agentes LLM (como el paradigma ReAct), pero con una diferencia clave:\n",
    "- En tool use, un agente LLM invoca herramientas genericas (busqueda, calculadora, API).\n",
    "- En Agent2Agent, cada \"herramienta\" es un agente especializado con su propio system prompt, logica interna y criterios de calidad.\n",
    "\n",
    "La ventaja del Agent2Agent es que cada agente puede tener un prompt optimizado para su tarea especifica, mientras que en tool use el agente principal debe manejar toda la complejidad en un solo prompt.\n",
    "\n",
    "### El grounding check como gate de calidad\n",
    "\n",
    "El segundo ciclo (cuando `groundedness < 0.18`) no es simplemente \"reintentar\": el orquestador modifica la query para enfocarse en evidencia concreta y cambia el modo del synthesizer a \"estricto\". Esto es un patron de **retroalimentacion correctiva**, donde el sistema intenta mejorar su output antes de entregarlo al usuario. En produccion, el numero de reintentos deberia estar acotado (tipicamente 1-2) para evitar loops infinitos y latencia excesiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15b90fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:18:05.184091Z",
     "iopub.status.busy": "2026-02-19T22:18:05.183843Z",
     "iopub.status.idle": "2026-02-19T22:18:09.217562Z",
     "shell.execute_reply": "2026-02-19T22:18:09.217103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosdaniel/Documents/Projects/labor_projects/Henry/2026/01-introduction_ai_engineering/ai_engineering_henry/02-vector_data_bases/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from scripts.common import generate_answer\n",
    "from scripts.evaluation import groundedness_score\n",
    "from scripts.vector_store_lab import build_index_from_json\n",
    "\n",
    "OUTPUTS_DIR = ROOT / 'outputs'\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PATH = ROOT / 'data' / 'batman_comics.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d55f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:18:09.218742Z",
     "iopub.status.busy": "2026-02-19T22:18:09.218588Z",
     "iopub.status.idle": "2026-02-19T22:18:09.291916Z",
     "shell.execute_reply": "2026-02-19T22:18:09.291565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index stats: {'collection': 'agent2agent_batman', 'indexed_chunks': 38, 'embedding_provider': 'local:hash-embedding'}\n",
      "Chunk stats: {'chunk_count': 38, 'unique_sources': 12, 'avg_chars_per_chunk': 682.5, 'max_chars_per_chunk': 792, 'themes_distribution': {'origen': 3, 'villanos': 3, 'legado': 3, 'conspiracion': 3, 'derrota': 3, 'misterio': 3, 'relaciones': 3, 'equipo': 3, 'escenario': 3, 'mentoria': 4, 'recursos': 4, 'filosofia': 3}, 'hero_distribution': {'batman': 38}}\n"
     ]
    }
   ],
   "source": [
    "db, chunks, index_stats, chunk_stats = build_index_from_json(\n",
    "    json_path=DATA_PATH,\n",
    "    persist_dir=OUTPUTS_DIR / 'chroma_agent2agent_batman',\n",
    "    collection_name='agent2agent_batman',\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=120,\n",
    "    embedding_model='text-embedding-3-small',\n",
    ")\n",
    "\n",
    "print('Index stats:', index_stats)\n",
    "print('Chunk stats:', chunk_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97f4740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:18:09.292846Z",
     "iopub.status.busy": "2026-02-19T22:18:09.292791Z",
     "iopub.status.idle": "2026-02-19T22:18:09.296764Z",
     "shell.execute_reply": "2026-02-19T22:18:09.296423Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DetectiveRetrieverAgent:\n",
    "    vector_db: object\n",
    "    embedding_model: str = 'text-embedding-3-small'\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 5) -> tuple[list[dict], str]:\n",
    "        docs, provider = self.vector_db.query(\n",
    "            query_text=query,\n",
    "            n_results=k,\n",
    "            embedding_model=self.embedding_model,\n",
    "        )\n",
    "        return docs, provider\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProfesorSynthesizerAgent:\n",
    "    model: str = 'gpt-5-mini'\n",
    "\n",
    "    def answer(self, question: str, docs: list[dict], mode: str = 'normal') -> tuple[str, str]:\n",
    "        contexts = [str(doc.get('text', '')) for doc in docs]\n",
    "        if mode == 'estricto':\n",
    "            system_prompt = (\n",
    "                'Modo estricto de verificabilidad: responde solo con evidencia del contexto, '\n",
    "                'no inventes datos, y cita [D#].'\n",
    "            )\n",
    "        else:\n",
    "            system_prompt = (\n",
    "                'Eres profesor de AI Engineering aplicado. Explica con claridad y rigor tecnico, '\n",
    "                'limitandote al contexto recuperado y citando [D#].'\n",
    "            )\n",
    "\n",
    "        return generate_answer(\n",
    "            query=question,\n",
    "            contexts=contexts,\n",
    "            model=self.model,\n",
    "            system_prompt=system_prompt,\n",
    "        )\n",
    "\n",
    "\n",
    "class Agent2AgentOrchestrator:\n",
    "    def __init__(self, retriever: DetectiveRetrieverAgent, synthesizer: ProfesorSynthesizerAgent) -> None:\n",
    "        self.retriever = retriever\n",
    "        self.synthesizer = synthesizer\n",
    "\n",
    "    def run(self, question: str, k: int = 5, grounding_threshold: float = 0.18) -> dict:\n",
    "        t0 = time.perf_counter()\n",
    "        dialogue = []\n",
    "\n",
    "        docs, retrieval_provider = self.retriever.retrieve(question, k=k)\n",
    "        dialogue.append({\n",
    "            'agent': 'DetectiveRetrieverAgent',\n",
    "            'message': f'Recuperados {len(docs)} documentos con provider={retrieval_provider}',\n",
    "        })\n",
    "\n",
    "        answer, llm_provider = self.synthesizer.answer(question=question, docs=docs, mode='normal')\n",
    "        score = groundedness_score(answer=answer, contexts=[doc.get('text', '') for doc in docs])\n",
    "        dialogue.append({\n",
    "            'agent': 'ProfesorSynthesizerAgent',\n",
    "            'message': f'Borrador generado con provider={llm_provider} y groundedness={score}',\n",
    "        })\n",
    "\n",
    "        if score < grounding_threshold:\n",
    "            refined_query = question + ' Enfocate en evidencia canonica concreta y eventos verificables.'\n",
    "            docs_refined, retrieval_provider_2 = self.retriever.retrieve(refined_query, k=k + 2)\n",
    "            dialogue.append({\n",
    "                'agent': 'DetectiveRetrieverAgent',\n",
    "                'message': (\n",
    "                    f'Retrieval de refuerzo: {len(docs_refined)} docs con provider={retrieval_provider_2}'\n",
    "                ),\n",
    "            })\n",
    "            docs = docs_refined\n",
    "            answer, llm_provider = self.synthesizer.answer(question=question, docs=docs, mode='estricto')\n",
    "            score = groundedness_score(answer=answer, contexts=[doc.get('text', '') for doc in docs])\n",
    "            dialogue.append({\n",
    "                'agent': 'ProfesorSynthesizerAgent',\n",
    "                'message': f'Respuesta final regenerada con groundedness={score}',\n",
    "            })\n",
    "\n",
    "        latency = round(time.perf_counter() - t0, 4)\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'groundedness': score,\n",
    "            'retrieved_docs': len(docs),\n",
    "            'latency_seconds': latency,\n",
    "            'llm_provider': llm_provider,\n",
    "            'retrieval_provider': retrieval_provider,\n",
    "            'dialogue': dialogue,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7faed9b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:18:09.297589Z",
     "iopub.status.busy": "2026-02-19T22:18:09.297540Z",
     "iopub.status.idle": "2026-02-19T22:18:09.299154Z",
     "shell.execute_reply": "2026-02-19T22:18:09.298813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent2Agent pipeline listo.\n"
     ]
    }
   ],
   "source": [
    "retriever_agent = DetectiveRetrieverAgent(vector_db=db, embedding_model='text-embedding-3-small')\n",
    "synthesizer_agent = ProfesorSynthesizerAgent(model='gpt-5-mini')\n",
    "orchestrator = Agent2AgentOrchestrator(retriever=retriever_agent, synthesizer=synthesizer_agent)\n",
    "\n",
    "print('Agent2Agent pipeline listo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad8cf622",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:18:09.300027Z",
     "iopub.status.busy": "2026-02-19T22:18:09.299969Z",
     "iopub.status.idle": "2026-02-19T22:18:09.306502Z",
     "shell.execute_reply": "2026-02-19T22:18:09.306161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DetectiveRetrieverAgent</td>\n",
       "      <td>Recuperados 5 documentos con provider=local:ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProfesorSynthesizerAgent</td>\n",
       "      <td>Borrador generado con provider=local:fallback-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      agent                                            message\n",
       "0   DetectiveRetrieverAgent  Recuperados 5 documentos con provider=local:ha...\n",
       "1  ProfesorSynthesizerAgent  Borrador generado con provider=local:fallback-..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Explica como Batman combina investigacion y estrategia para enfrentar amenazas como Bane y el Joker.'\n",
    "result = orchestrator.run(question=query, k=5, grounding_threshold=0.18)\n",
    "\n",
    "pd.DataFrame(result['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e7c2a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:18:09.307328Z",
     "iopub.status.busy": "2026-02-19T22:18:09.307283Z",
     "iopub.status.idle": "2026-02-19T22:18:09.308889Z",
     "shell.execute_reply": "2026-02-19T22:18:09.308639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta:\n",
      "Explica como Batman combina investigacion y estrategia para enfrentar amenazas como Bane y el Joker.\n",
      "\n",
      "Respuesta:\n",
      "Respuesta local fallback (sin llamada a OpenAI):\n",
      "- The Killing Joke de Alan Moore es considerada la historia definitiva sobre la relacion entre Batman y el Joker [D4]\n",
      "- La premisa central del Joker es simple y aterradora: cualquier persona esta a un mal dia de volverse como el [D4]\n",
      "- Para probarlo, el Joker dispara a Barbara Gordon (Batgirl) en la columna vertebral, la fotografa mientras sufre, y secuestra al comisionado Gordon para someterlo a una noche de tortura psicologica en un parque de diversiones abandonado [D4]\n",
      "\n",
      "Metricas:\n",
      "{'groundedness': 0.9032, 'retrieved_docs': 5, 'latency_seconds': 0.0019, 'llm_provider': 'local:fallback-summary', 'retrieval_provider': 'local:hash-embedding'}\n"
     ]
    }
   ],
   "source": [
    "print('Pregunta:')\n",
    "print(result['question'])\n",
    "print('\\nRespuesta:')\n",
    "print(result['answer'])\n",
    "print('\\nMetricas:')\n",
    "print({\n",
    "    'groundedness': result['groundedness'],\n",
    "    'retrieved_docs': result['retrieved_docs'],\n",
    "    'latency_seconds': result['latency_seconds'],\n",
    "    'llm_provider': result['llm_provider'],\n",
    "    'retrieval_provider': result['retrieval_provider'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206c121",
   "metadata": {},
   "source": [
    "## Mini-practica guiada\n",
    "\n",
    "Ejecuta estas consultas y compara el comportamiento del dialogo entre agentes:\n",
    "1. `Que leccion deja Knightfall sobre delegacion y resiliencia?`\n",
    "2. `Como se relaciona la filosofia de no matar con decisiones tacticas en Gotham?`\n",
    "3. `Por que Batman es efectivo en la Liga de la Justicia sin superpoderes?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14384e20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:18:09.309730Z",
     "iopub.status.busy": "2026-02-19T22:18:09.309690Z",
     "iopub.status.idle": "2026-02-19T22:18:09.315048Z",
     "shell.execute_reply": "2026-02-19T22:18:09.314642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>groundedness</th>\n",
       "      <th>retrieved_docs</th>\n",
       "      <th>latency_seconds</th>\n",
       "      <th>llm_provider</th>\n",
       "      <th>retrieval_provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Que leccion deja Knightfall sobre delegacion y...</td>\n",
       "      <td>0.9118</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>local:fallback-summary</td>\n",
       "      <td>local:hash-embedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Como se relaciona la filosofia de no matar con...</td>\n",
       "      <td>0.8987</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>local:fallback-summary</td>\n",
       "      <td>local:hash-embedding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Por que Batman es efectivo en la Liga de la Ju...</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>local:fallback-summary</td>\n",
       "      <td>local:hash-embedding</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  groundedness  \\\n",
       "0  Que leccion deja Knightfall sobre delegacion y...        0.9118   \n",
       "1  Como se relaciona la filosofia de no matar con...        0.8987   \n",
       "2  Por que Batman es efectivo en la Liga de la Ju...        0.8929   \n",
       "\n",
       "   retrieved_docs  latency_seconds            llm_provider  \\\n",
       "0               5           0.0007  local:fallback-summary   \n",
       "1               5           0.0006  local:fallback-summary   \n",
       "2               5           0.0006  local:fallback-summary   \n",
       "\n",
       "     retrieval_provider  \n",
       "0  local:hash-embedding  \n",
       "1  local:hash-embedding  \n",
       "2  local:hash-embedding  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_queries = [\n",
    "    'Que leccion deja Knightfall sobre delegacion y resiliencia?',\n",
    "    'Como se relaciona la filosofia de no matar con decisiones tacticas en Gotham?',\n",
    "    'Por que Batman es efectivo en la Liga de la Justicia sin superpoderes?',\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for q in exercise_queries:\n",
    "    run = orchestrator.run(question=q, k=5, grounding_threshold=0.18)\n",
    "    rows.append({\n",
    "        'question': q,\n",
    "        'groundedness': run['groundedness'],\n",
    "        'retrieved_docs': run['retrieved_docs'],\n",
    "        'latency_seconds': run['latency_seconds'],\n",
    "        'llm_provider': run['llm_provider'],\n",
    "        'retrieval_provider': run['retrieval_provider'],\n",
    "    })\n",
    "\n",
    "exercise_df = pd.DataFrame(rows)\n",
    "exercise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181bc9c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-19T22:18:09.315883Z",
     "iopub.status.busy": "2026-02-19T22:18:09.315830Z",
     "iopub.status.idle": "2026-02-19T22:18:09.318501Z",
     "shell.execute_reply": "2026-02-19T22:18:09.318232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/carlosdaniel/Documents/Projects/labor_projects/Henry/2026/01-introduction_ai_engineering/ai_engineering_henry/02-vector_data_bases/batman_vector_db_orchestration/outputs/agent2agent_exercise_results.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = OUTPUTS_DIR / 'agent2agent_exercise_results.csv'\n",
    "exercise_df.to_csv(csv_path, index=False)\n",
    "print(f'Saved: {csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb204df",
   "metadata": {},
   "source": [
    "## Cierre didactico\n",
    "\n",
    "### Conceptos clave de esta notebook\n",
    "\n",
    "- **Separacion de responsabilidades mejora auditabilidad**: cuando retrieval y generacion son agentes separados, puedes evaluar cada uno independientemente. Si la respuesta es mala, sabes si el problema fue el retrieval (documentos irrelevantes) o la generacion (mala sintesis del contexto).\n",
    "- **El dialogo entre agentes es una traza natural**: la lista de `dialogue` no es solo decorativa — en produccion, ese log es tu herramienta principal de debugging. Incluyelo siempre.\n",
    "- **El segundo ciclo correctivo tiene costo**: cada ciclo adicional multiplica la latencia y el costo de API. El threshold de grounding (0.18) y el numero maximo de ciclos deben calibrarse para el trade-off calidad/costo de tu caso de uso.\n",
    "- **Agent2Agent simple > multi-agente complejo** (en la mayoria de casos): antes de disenar un sistema con 5+ agentes, verifica que un flujo de 2 agentes con un orquestador no resuelve tu problema igualmente bien.\n",
    "\n",
    "### Conexion con la siguiente notebook\n",
    "\n",
    "En la siguiente notebook extendemos este patron con **especializacion por roles**: multiples agentes especialistas (timeline, villains, strategy) son coordinados por un router heuristico. Esto introduce la pregunta: *cuando vale la pena especializar agentes vs tener uno generalista?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
