{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258c1269",
   "metadata": {},
   "source": [
    "# Ejercicio Sencillo Agent2Agent con Batman + RAG\n",
    "\n",
    "## Objetivo\n",
    "Implementar un flujo **agent2agent** minimalista y util en AI Engineering aplicado:\n",
    "- **Agent 1 (Detective Retriever)**: recupera evidencia desde la base vectorial.\n",
    "- **Agent 2 (Profesor Synthesizer)**: construye la respuesta final sustentada en contexto.\n",
    "\n",
    "Si la respuesta no esta suficientemente fundamentada, el orquestador solicita un segundo ciclo de retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72558fc",
   "metadata": {},
   "source": [
    "## Diagrama del ejercicio\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "  A[\"Pregunta del usuario\"] --> B[\"Detective Retriever Agent\"]\n",
    "  B --> C[\"Contexto recuperado\"]\n",
    "  C --> D[\"Profesor Synthesizer Agent\"]\n",
    "  D --> E[\"Evaluacion de grounding\"]\n",
    "  E -->|\"bajo\"| B\n",
    "  E -->|\"suficiente\"| F[\"Respuesta final\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT))\n",
    "\n",
    "from scripts.common import generate_answer\n",
    "from scripts.evaluation import groundedness_score\n",
    "from scripts.vector_store_lab import build_index_from_json\n",
    "\n",
    "OUTPUTS_DIR = ROOT / 'outputs'\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PATH = ROOT / 'data' / 'batman_comics.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d55f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "db, chunks, index_stats, chunk_stats = build_index_from_json(\n",
    "    json_path=DATA_PATH,\n",
    "    persist_dir=OUTPUTS_DIR / 'chroma_agent2agent_batman',\n",
    "    collection_name='agent2agent_batman',\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=120,\n",
    "    embedding_model='text-embedding-3-small',\n",
    ")\n",
    "\n",
    "print('Index stats:', index_stats)\n",
    "print('Chunk stats:', chunk_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f4740",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DetectiveRetrieverAgent:\n",
    "    vector_db: object\n",
    "    embedding_model: str = 'text-embedding-3-small'\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 5) -> tuple[list[dict], str]:\n",
    "        docs, provider = self.vector_db.query(\n",
    "            query_text=query,\n",
    "            n_results=k,\n",
    "            embedding_model=self.embedding_model,\n",
    "        )\n",
    "        return docs, provider\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProfesorSynthesizerAgent:\n",
    "    model: str = 'gpt-5-mini'\n",
    "\n",
    "    def answer(self, question: str, docs: list[dict], mode: str = 'normal') -> tuple[str, str]:\n",
    "        contexts = [str(doc.get('text', '')) for doc in docs]\n",
    "        if mode == 'estricto':\n",
    "            system_prompt = (\n",
    "                'Modo estricto de verificabilidad: responde solo con evidencia del contexto, '\n",
    "                'no inventes datos, y cita [D#].'\n",
    "            )\n",
    "        else:\n",
    "            system_prompt = (\n",
    "                'Eres profesor de AI Engineering aplicado. Explica con claridad y rigor tecnico, '\n",
    "                'limitandote al contexto recuperado y citando [D#].'\n",
    "            )\n",
    "\n",
    "        return generate_answer(\n",
    "            query=question,\n",
    "            contexts=contexts,\n",
    "            model=self.model,\n",
    "            system_prompt=system_prompt,\n",
    "        )\n",
    "\n",
    "\n",
    "class Agent2AgentOrchestrator:\n",
    "    def __init__(self, retriever: DetectiveRetrieverAgent, synthesizer: ProfesorSynthesizerAgent) -> None:\n",
    "        self.retriever = retriever\n",
    "        self.synthesizer = synthesizer\n",
    "\n",
    "    def run(self, question: str, k: int = 5, grounding_threshold: float = 0.18) -> dict:\n",
    "        t0 = time.perf_counter()\n",
    "        dialogue = []\n",
    "\n",
    "        docs, retrieval_provider = self.retriever.retrieve(question, k=k)\n",
    "        dialogue.append({\n",
    "            'agent': 'DetectiveRetrieverAgent',\n",
    "            'message': f'Recuperados {len(docs)} documentos con provider={retrieval_provider}',\n",
    "        })\n",
    "\n",
    "        answer, llm_provider = self.synthesizer.answer(question=question, docs=docs, mode='normal')\n",
    "        score = groundedness_score(answer=answer, contexts=[doc.get('text', '') for doc in docs])\n",
    "        dialogue.append({\n",
    "            'agent': 'ProfesorSynthesizerAgent',\n",
    "            'message': f'Borrador generado con provider={llm_provider} y groundedness={score}',\n",
    "        })\n",
    "\n",
    "        if score < grounding_threshold:\n",
    "            refined_query = question + ' Enfocate en evidencia canonica concreta y eventos verificables.'\n",
    "            docs_refined, retrieval_provider_2 = self.retriever.retrieve(refined_query, k=k + 2)\n",
    "            dialogue.append({\n",
    "                'agent': 'DetectiveRetrieverAgent',\n",
    "                'message': (\n",
    "                    f'Retrieval de refuerzo: {len(docs_refined)} docs con provider={retrieval_provider_2}'\n",
    "                ),\n",
    "            })\n",
    "            docs = docs_refined\n",
    "            answer, llm_provider = self.synthesizer.answer(question=question, docs=docs, mode='estricto')\n",
    "            score = groundedness_score(answer=answer, contexts=[doc.get('text', '') for doc in docs])\n",
    "            dialogue.append({\n",
    "                'agent': 'ProfesorSynthesizerAgent',\n",
    "                'message': f'Respuesta final regenerada con groundedness={score}',\n",
    "            })\n",
    "\n",
    "        latency = round(time.perf_counter() - t0, 4)\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'groundedness': score,\n",
    "            'retrieved_docs': len(docs),\n",
    "            'latency_seconds': latency,\n",
    "            'llm_provider': llm_provider,\n",
    "            'retrieval_provider': retrieval_provider,\n",
    "            'dialogue': dialogue,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faed9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_agent = DetectiveRetrieverAgent(vector_db=db, embedding_model='text-embedding-3-small')\n",
    "synthesizer_agent = ProfesorSynthesizerAgent(model='gpt-5-mini')\n",
    "orchestrator = Agent2AgentOrchestrator(retriever=retriever_agent, synthesizer=synthesizer_agent)\n",
    "\n",
    "print('Agent2Agent pipeline listo.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cf622",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Explica como Batman combina investigacion y estrategia para enfrentar amenazas como Bane y el Joker.'\n",
    "result = orchestrator.run(question=query, k=5, grounding_threshold=0.18)\n",
    "\n",
    "pd.DataFrame(result['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pregunta:')\n",
    "print(result['question'])\n",
    "print('\\nRespuesta:')\n",
    "print(result['answer'])\n",
    "print('\\nMetricas:')\n",
    "print({\n",
    "    'groundedness': result['groundedness'],\n",
    "    'retrieved_docs': result['retrieved_docs'],\n",
    "    'latency_seconds': result['latency_seconds'],\n",
    "    'llm_provider': result['llm_provider'],\n",
    "    'retrieval_provider': result['retrieval_provider'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206c121",
   "metadata": {},
   "source": [
    "## Mini-practica guiada\n",
    "\n",
    "Ejecuta estas consultas y compara el comportamiento del dialogo entre agentes:\n",
    "1. `Que leccion deja Knightfall sobre delegacion y resiliencia?`\n",
    "2. `Como se relaciona la filosofia de no matar con decisiones tacticas en Gotham?`\n",
    "3. `Por que Batman es efectivo en la Liga de la Justicia sin superpoderes?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14384e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_queries = [\n",
    "    'Que leccion deja Knightfall sobre delegacion y resiliencia?',\n",
    "    'Como se relaciona la filosofia de no matar con decisiones tacticas en Gotham?',\n",
    "    'Por que Batman es efectivo en la Liga de la Justicia sin superpoderes?',\n",
    "]\n",
    "\n",
    "rows = []\n",
    "for q in exercise_queries:\n",
    "    run = orchestrator.run(question=q, k=5, grounding_threshold=0.18)\n",
    "    rows.append({\n",
    "        'question': q,\n",
    "        'groundedness': run['groundedness'],\n",
    "        'retrieved_docs': run['retrieved_docs'],\n",
    "        'latency_seconds': run['latency_seconds'],\n",
    "        'llm_provider': run['llm_provider'],\n",
    "        'retrieval_provider': run['retrieval_provider'],\n",
    "    })\n",
    "\n",
    "exercise_df = pd.DataFrame(rows)\n",
    "exercise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181bc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = OUTPUTS_DIR / 'agent2agent_exercise_results.csv'\n",
    "exercise_df.to_csv(csv_path, index=False)\n",
    "print(f'Saved: {csv_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb204df",
   "metadata": {},
   "source": [
    "## Cierre\n",
    "\n",
    "Este ejercicio ilustra una idea importante: en muchos casos reales, un pipeline **agent2agent simple y auditable** aporta mas valor que una arquitectura multi-agente compleja."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
