{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Robustez y Validacion para Pipelines de AI/ML\n",
    "\n",
    "**Modulo:** 01 - Programacion Python  \n",
    "**Nivel:** Avanzado (el notebook mas completo del modulo 01)  \n",
    "**Tiempo estimado:** 90 - 120 minutos  \n",
    "**Temas cubiertos:** `05_errores_y_debug` | `08_excepciones_avanzadas` | `09_generadores` | `10_comprehensions` | `11_logging` | `12_pydantic`\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook seras capaz de:\n",
    "\n",
    "1. Manejar errores de forma estructurada con `try / except / else / finally`.\n",
    "2. Crear **excepciones custom** con jerarquia para pipelines de ML.\n",
    "3. Usar **generadores** para procesamiento lazy de grandes volumenes de datos.\n",
    "4. Aplicar **comprehensions** (list, dict, set) de forma correcta y eficiente.\n",
    "5. Configurar **logging profesional** con niveles, handlers y formatters.\n",
    "6. Validar datos de entrada y salida con **Pydantic** antes de alimentar modelos.\n",
    "\n",
    "---\n",
    "\n",
    "### Por que importa en AI Engineering\n",
    "\n",
    "Un modelo de ML puede ser excelente, pero si el pipeline que lo alimenta:\n",
    "- no valida datos de entrada,\n",
    "- no maneja errores de forma explicita,\n",
    "- no registra que paso en cada etapa,\n",
    "\n",
    "...entonces el sistema completo es **fragil**. Este notebook te da las herramientas para construir pipelines **robustos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup y verificacion del entorno\n",
    "# ============================================================\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import collections\n",
    "import functools\n",
    "from typing import List, Dict, Optional, Generator, Any, Literal\n",
    "from io import StringIO\n",
    "\n",
    "# Pydantic (ya incluido en el proyecto)\n",
    "try:\n",
    "    import pydantic\n",
    "    _pydantic_ok = True\n",
    "except ImportError:\n",
    "    _pydantic_ok = False\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"  Verificacion del entorno\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Python  : {sys.version.split()[0]}\")\n",
    "print(f\"  Pydantic: {pydantic.__version__ if _pydantic_ok else 'NO INSTALADO'}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not _pydantic_ok:\n",
    "    print(\"\\n[!] Instala pydantic: pip install pydantic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 1: Manejo de Errores con `try / except / else / finally`\n",
    "\n",
    "### Flujo de ejecucion\n",
    "\n",
    "```\n",
    "        try:\n",
    "          codigo_riesgoso()\n",
    "              |\n",
    "     +--------+--------+\n",
    "     |                  |\n",
    "  EXCEPCION?         SIN ERROR\n",
    "     |                  |\n",
    "  except:            else:\n",
    "  manejar error      codigo si todo ok\n",
    "     |                  |\n",
    "     +--------+--------+\n",
    "              |\n",
    "          finally:\n",
    "          SIEMPRE se ejecuta\n",
    "          (cleanup, cerrar archivos, etc.)\n",
    "```\n",
    "\n",
    "### Excepciones comunes en Python\n",
    "\n",
    "| Excepcion            | Causa tipica                                 | Ejemplo en AI/ML                        |\n",
    "|----------------------|----------------------------------------------|-----------------------------------------|\n",
    "| `TypeError`          | Tipo incorrecto en operacion                 | Pasar string donde se espera int        |\n",
    "| `ValueError`         | Valor fuera de rango o formato invalido      | `temperature = -1.0`                    |\n",
    "| `KeyError`           | Clave no existe en diccionario               | Acceder a feature que no existe          |\n",
    "| `FileNotFoundError`  | Archivo/directorio no encontrado             | Ruta de modelo inexistente              |\n",
    "| `IndexError`         | Indice fuera de rango                        | Acceder a batch vacio                   |\n",
    "| `AttributeError`     | Objeto no tiene el atributo                  | Modelo sin metodo `.predict()`          |\n",
    "| `RuntimeError`       | Error generico en tiempo de ejecucion        | Fallo en GPU / framework                |\n",
    "| `StopIteration`      | Iterador agotado                             | Generador sin mas datos                 |\n",
    "\n",
    "**Regla de oro:** Captura excepciones **especificas**, nunca uses `except:` sin tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ejemplo completo: try / except / else / finally\n",
    "# Caso practico: cargar configuracion de un modelo\n",
    "# ============================================================\n",
    "import json\n",
    "\n",
    "def cargar_config_modelo(ruta: str) -> dict:\n",
    "    \"\"\"Carga configuracion JSON de un modelo de forma segura.\"\"\"\n",
    "    archivo = None\n",
    "    try:\n",
    "        archivo = open(ruta, \"r\", encoding=\"utf-8\")\n",
    "        config = json.load(archivo)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"  [ERROR] Archivo no encontrado: {ruta}\")\n",
    "        print(\"  -> Usando configuracion por defecto\")\n",
    "        config = {\"model_name\": \"default\", \"max_tokens\": 256}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"  [ERROR] JSON invalido en {ruta}: {e}\")\n",
    "        config = {\"model_name\": \"default\", \"max_tokens\": 256}\n",
    "    else:\n",
    "        # Solo se ejecuta si NO hubo excepcion\n",
    "        print(f\"  [OK] Configuracion cargada desde {ruta}\")\n",
    "        print(f\"  -> Claves encontradas: {list(config.keys())}\")\n",
    "    finally:\n",
    "        # SIEMPRE se ejecuta: limpieza\n",
    "        if archivo is not None:\n",
    "            archivo.close()\n",
    "            print(\"  [CLEANUP] Archivo cerrado correctamente\")\n",
    "    return config\n",
    "\n",
    "\n",
    "# --- Demo 1: archivo que no existe ---\n",
    "print(\"Caso 1: archivo inexistente\")\n",
    "print(\"-\" * 40)\n",
    "config1 = cargar_config_modelo(\"/ruta/que/no/existe.json\")\n",
    "print(f\"  Resultado: {config1}\\n\")\n",
    "\n",
    "# --- Demo 2: archivo valido (creamos uno temporal) ---\n",
    "print(\"Caso 2: archivo JSON valido\")\n",
    "print(\"-\" * 40)\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n",
    "    json.dump({\"model_name\": \"gpt-mini\", \"max_tokens\": 512, \"temperature\": 0.7}, f)\n",
    "    ruta_temp = f.name\n",
    "\n",
    "config2 = cargar_config_modelo(ruta_temp)\n",
    "print(f\"  Resultado: {config2}\\n\")\n",
    "os.unlink(ruta_temp)  # limpiar archivo temporal\n",
    "\n",
    "# --- Demo 3: JSON corrupto ---\n",
    "print(\"Caso 3: JSON corrupto\")\n",
    "print(\"-\" * 40)\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n",
    "    f.write(\"{modelo: sin comillas}\")\n",
    "    ruta_corrupta = f.name\n",
    "\n",
    "config3 = cargar_config_modelo(ruta_corrupta)\n",
    "print(f\"  Resultado: {config3}\")\n",
    "os.unlink(ruta_corrupta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Exceptions para AI/ML\n",
    "\n",
    "**Cuando crear excepciones propias:**\n",
    "- Cuando los errores built-in no describen bien la falla de tu dominio.\n",
    "- Cuando necesitas que distintos componentes del pipeline capturen errores de forma selectiva.\n",
    "- Cuando quieres incluir contexto adicional (nombre del modelo, ID del registro, etc.).\n",
    "\n",
    "**Diseno de jerarquia:**\n",
    "\n",
    "```\n",
    "  Exception\n",
    "      |\n",
    "  PipelineError          <- base de todos nuestros errores\n",
    "      |\n",
    "      +-- DataValidationError   <- datos de entrada invalidos\n",
    "      +-- ModelLoadError        <- fallo al cargar modelo\n",
    "      +-- InferenceError        <- fallo durante prediccion\n",
    "```\n",
    "\n",
    "Con esta jerarquia puedes capturar `PipelineError` para atrapar **todos** los errores del pipeline, o capturar subclases para manejar cada caso por separado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Jerarquia de excepciones custom para un pipeline de ML\n",
    "# ============================================================\n",
    "\n",
    "class PipelineError(Exception):\n",
    "    \"\"\"Error base para todas las fallas del pipeline.\"\"\"\n",
    "    def __init__(self, message: str, component: str = \"unknown\"):\n",
    "        self.component = component\n",
    "        super().__init__(f\"[{component}] {message}\")\n",
    "\n",
    "\n",
    "class DataValidationError(PipelineError):\n",
    "    \"\"\"Error cuando los datos de entrada no pasan validacion.\"\"\"\n",
    "    def __init__(self, message: str, field: str = \"\", value: Any = None):\n",
    "        self.field = field\n",
    "        self.value = value\n",
    "        detail = f\"{message} (field={field!r}, value={value!r})\"\n",
    "        super().__init__(detail, component=\"DataValidation\")\n",
    "\n",
    "\n",
    "class ModelLoadError(PipelineError):\n",
    "    \"\"\"Error al cargar un modelo desde disco o registro.\"\"\"\n",
    "    def __init__(self, model_name: str, reason: str):\n",
    "        self.model_name = model_name\n",
    "        super().__init__(f\"No se pudo cargar '{model_name}': {reason}\", component=\"ModelLoader\")\n",
    "\n",
    "\n",
    "class InferenceError(PipelineError):\n",
    "    \"\"\"Error durante la prediccion / inferencia.\"\"\"\n",
    "    def __init__(self, message: str, request_id: str = \"\"):\n",
    "        self.request_id = request_id\n",
    "        super().__init__(f\"{message} (request_id={request_id!r})\", component=\"Inference\")\n",
    "\n",
    "\n",
    "# --- Demo: lanzar y capturar cada tipo ---\n",
    "def demo_excepciones():\n",
    "    errores = [\n",
    "        DataValidationError(\"Valor negativo\", field=\"age\", value=-5),\n",
    "        ModelLoadError(\"sentiment-v2\", reason=\"archivo corrupto\"),\n",
    "        InferenceError(\"Timeout en GPU\", request_id=\"req-42\"),\n",
    "    ]\n",
    "\n",
    "    for error in errores:\n",
    "        try:\n",
    "            raise error\n",
    "        except DataValidationError as e:\n",
    "            print(f\"  DATOS INVALIDOS -> {e}  | field={e.field}\")\n",
    "        except ModelLoadError as e:\n",
    "            print(f\"  MODELO FALLO    -> {e}  | model={e.model_name}\")\n",
    "        except InferenceError as e:\n",
    "            print(f\"  INFERENCIA FALLO-> {e}  | req_id={e.request_id}\")\n",
    "\n",
    "    # --- Exception chaining con 'raise ... from ...' ---\n",
    "    print(\"\\n--- Exception chaining ---\")\n",
    "    try:\n",
    "        try:\n",
    "            data = {\"age\": \"no es numero\"}\n",
    "            int(data[\"age\"])\n",
    "        except ValueError as original:\n",
    "            raise DataValidationError(\n",
    "                \"No se pudo convertir a entero\",\n",
    "                field=\"age\",\n",
    "                value=data[\"age\"]\n",
    "            ) from original\n",
    "    except PipelineError as e:\n",
    "        print(f\"  Capturado: {e}\")\n",
    "        print(f\"  Causa original: {e.__cause__}\")\n",
    "        print(f\"  Tipo: {type(e).__name__}\")\n",
    "\n",
    "demo_excepciones()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 2: Generadores - Procesamiento Lazy\n",
    "\n",
    "### Lista vs Generador en memoria\n",
    "\n",
    "```\n",
    "  LISTA (eager)                         GENERADOR (lazy)\n",
    "  ============                          ================\n",
    "\n",
    "  [0, 1, 2, ..., 999_999]              (x for x in range(1_000_000))\n",
    "  ^^^^^^^^^^^^^^^^^^^^^^^^              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "  Todos los elementos viven             Solo el SIGUIENTE elemento\n",
    "  en memoria al mismo tiempo            se computa cuando se pide\n",
    "\n",
    "  RAM: ~8 MB (1M ints)                  RAM: ~120 bytes (constante)\n",
    "\n",
    "  Pro: acceso aleatorio (lista[500])    Pro: memoria constante O(1)\n",
    "  Con: memoria crece con N             Con: solo iteracion hacia adelante\n",
    "```\n",
    "\n",
    "### `yield` vs `return`\n",
    "\n",
    "| Aspecto          | `return`                      | `yield`                           |\n",
    "|------------------|-------------------------------|-----------------------------------|\n",
    "| Funcion termina? | Si, inmediatamente            | No, se **pausa** y se puede resumir|\n",
    "| Devuelve         | Un valor unico                | Un valor cada vez que se llama    |\n",
    "| Tipo resultado   | El tipo del valor             | Un objeto `generator`             |\n",
    "| Memoria          | Proporcional al resultado     | Constante O(1)                    |\n",
    "| Uso tipico       | Calcular y devolver           | Iterar sobre secuencias grandes   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generadores: basicos y batch_iterator\n",
    "# ============================================================\n",
    "\n",
    "# --- Generador basico con yield ---\n",
    "def contar_hasta(n: int) -> Generator[int, None, None]:\n",
    "    \"\"\"Genera numeros de 1 a n.\"\"\"\n",
    "    i = 1\n",
    "    while i <= n:\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "gen = contar_hasta(4)\n",
    "\n",
    "print(\"Tipo del generador:\", type(gen))\n",
    "print(\"next():\", next(gen))  # 1\n",
    "print(\"next():\", next(gen))  # 2\n",
    "print(\"Resto con for:\")\n",
    "for valor in gen:  # continua donde se quedo: 3, 4\n",
    "    print(f\"  {valor}\")\n",
    "\n",
    "# StopIteration al agotar\n",
    "try:\n",
    "    next(gen)\n",
    "except StopIteration:\n",
    "    print(\"\\nStopIteration: generador agotado\")\n",
    "\n",
    "\n",
    "# --- Generador practico: batch_iterator ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"batch_iterator: partir datos en lotes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def batch_iterator(data: list, batch_size: int) -> Generator[list, None, None]:\n",
    "    \"\"\"Genera lotes (sublistas) de tamanio batch_size.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i : i + batch_size]\n",
    "\n",
    "\n",
    "# Simular un dataset de 12 registros\n",
    "dataset = [f\"registro_{i:03d}\" for i in range(12)]\n",
    "print(f\"Dataset total: {len(dataset)} registros\\n\")\n",
    "\n",
    "for batch_num, batch in enumerate(batch_iterator(dataset, batch_size=4), start=1):\n",
    "    print(f\"  Batch {batch_num}: {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comparacion de memoria: lista vs generador\n",
    "# ============================================================\n",
    "\n",
    "N = 1_000_000\n",
    "\n",
    "# Enfoque 1: lista completa en memoria\n",
    "lista_completa = [x ** 2 for x in range(N)]\n",
    "\n",
    "# Enfoque 2: generador (lazy)\n",
    "generador = (x ** 2 for x in range(N))\n",
    "\n",
    "mem_lista = sys.getsizeof(lista_completa)\n",
    "mem_gen = sys.getsizeof(generador)\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(f\"  Comparacion de memoria: {N:,} elementos\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"  {'Estructura':<20} {'Memoria':>15} {'Ratio':>10}\")\n",
    "print(f\"  {'-'*20} {'-'*15} {'-'*10}\")\n",
    "print(f\"  {'Lista':<20} {mem_lista:>12,} B {'1.0x':>10}\")\n",
    "print(f\"  {'Generador':<20} {mem_gen:>12,} B {f'{mem_lista/mem_gen:.0f}x menos':>10}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\n  La lista usa ~{mem_lista / mem_gen:.0f}x mas memoria que el generador.\")\n",
    "print(\"  El generador NO almacena los resultados; los computa bajo demanda.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Pipelines (cadenas de generadores)\n",
    "\n",
    "Los generadores se pueden **encadenar** para crear pipelines de procesamiento donde los datos fluyen de uno a otro **sin materializar resultados intermedios**.\n",
    "\n",
    "```\n",
    "  Datos crudos\n",
    "      |\n",
    "      v\n",
    "  [read_lines]    -- yield linea por linea\n",
    "      |\n",
    "      v\n",
    "  [filter_valid]  -- yield solo lineas validas\n",
    "      |\n",
    "      v\n",
    "  [parse_records] -- yield dicts parseados\n",
    "      |\n",
    "      v\n",
    "  [aggregate]     -- consume y acumula resultado final\n",
    "\n",
    "  Memoria usada: O(1) en cada etapa (solo 1 elemento a la vez)\n",
    "```\n",
    "\n",
    "Esto es especialmente util cuando procesas archivos grandes de features, logs de inferencia, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Generator pipeline: procesamiento lazy en cadena\n",
    "# ============================================================\n",
    "\n",
    "# Datos crudos simulados (como si vinieran de un archivo CSV)\n",
    "datos_crudos = [\n",
    "    \"user_id,edad,score\",    # header\n",
    "    \"u001,25,0.85\",\n",
    "    \"u002,,0.72\",            # edad vacia -> invalido\n",
    "    \"u003,30,0.91\",\n",
    "    \"CORRUPTO\",              # linea corrupta\n",
    "    \"u004,22,0.65\",\n",
    "    \"u005,45,0.88\",\n",
    "    \"u006,-3,0.50\",          # edad negativa -> invalido\n",
    "    \"u007,35,0.77\",\n",
    "]\n",
    "\n",
    "\n",
    "def read_lines(data: list) -> Generator[str, None, None]:\n",
    "    \"\"\"Etapa 1: emite lineas saltando el header.\"\"\"\n",
    "    for linea in data[1:]:\n",
    "        yield linea\n",
    "\n",
    "\n",
    "def filter_valid(lines: Generator) -> Generator[str, None, None]:\n",
    "    \"\"\"Etapa 2: filtra lineas que tengan exactamente 3 campos.\"\"\"\n",
    "    for linea in lines:\n",
    "        campos = linea.split(\",\")\n",
    "        if len(campos) == 3 and all(c.strip() for c in campos):\n",
    "            yield linea\n",
    "\n",
    "\n",
    "def parse_records(lines: Generator) -> Generator[dict, None, None]:\n",
    "    \"\"\"Etapa 3: convierte cada linea en un diccionario tipado.\"\"\"\n",
    "    for linea in lines:\n",
    "        uid, edad_str, score_str = linea.split(\",\")\n",
    "        edad = int(edad_str)\n",
    "        if edad < 0:\n",
    "            continue  # filtrar edades invalidas\n",
    "        yield {\"user_id\": uid, \"edad\": edad, \"score\": float(score_str)}\n",
    "\n",
    "\n",
    "def aggregate(records: Generator) -> dict:\n",
    "    \"\"\"Etapa 4 (terminal): calcula estadisticas agregadas.\"\"\"\n",
    "    total = 0\n",
    "    suma_score = 0.0\n",
    "    suma_edad = 0\n",
    "    for record in records:\n",
    "        total += 1\n",
    "        suma_score += record[\"score\"]\n",
    "        suma_edad += record[\"edad\"]\n",
    "    if total == 0:\n",
    "        return {\"total\": 0, \"avg_score\": 0, \"avg_edad\": 0}\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"avg_score\": round(suma_score / total, 3),\n",
    "        \"avg_edad\": round(suma_edad / total, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Encadenar el pipeline ---\n",
    "pipeline = aggregate(\n",
    "    parse_records(\n",
    "        filter_valid(\n",
    "            read_lines(datos_crudos)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Resultado del pipeline:\")\n",
    "print(f\"  Registros validos : {pipeline['total']}\")\n",
    "print(f\"  Score promedio    : {pipeline['avg_score']}\")\n",
    "print(f\"  Edad promedio     : {pipeline['avg_edad']}\")\n",
    "print(f\"\\n  (De {len(datos_crudos) - 1} lineas de datos, {pipeline['total']} pasaron todas las etapas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 3: Comprehensions vs Loops\n",
    "\n",
    "### Cuando usar cada uno\n",
    "\n",
    "| Situacion                              | Recomendacion       | Razon                                |\n",
    "|----------------------------------------|---------------------|--------------------------------------|\n",
    "| Transformar todos los elementos         | **Comprehension**   | Mas conciso, mas rapido              |\n",
    "| Filtrar + transformar (1 condicion)     | **Comprehension**   | Legible en una linea                 |\n",
    "| Logica compleja (try/except, multiples ifs) | **Loop**       | Comprehension se vuelve ilegible     |\n",
    "| Efectos secundarios (print, log, append) | **Loop**          | Comprehension no debe tener side effects |\n",
    "| Crear dict/set desde otra estructura    | **Comprehension**   | Dict/set comprehension es muy claro  |\n",
    "| Iterar sin guardar resultado            | **Loop**            | No crees una lista solo para descartarla |\n",
    "\n",
    "**Anti-patron:**\n",
    "```python\n",
    "# MAL: comprehension solo por side effects\n",
    "[print(x) for x in data]   # crea lista de None, desperdicia memoria\n",
    "\n",
    "# BIEN: loop para side effects\n",
    "for x in data:\n",
    "    print(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comprehensions: list, dict, set + comparacion de rendimiento\n",
    "# ============================================================\n",
    "\n",
    "# --- Ejemplos basicos ---\n",
    "features = [\"age\", \"income\", \"tenure\", \"score\", \"age\", \"income\"]\n",
    "valores = [25, 50000, 12, 0.85, 30, 60000]\n",
    "\n",
    "# List comprehension: elevar al cuadrado\n",
    "cuadrados = [v ** 2 for v in valores if isinstance(v, int)]\n",
    "print(f\"List comprehension (cuadrados de ints): {cuadrados}\")\n",
    "\n",
    "# Dict comprehension: nombre_feature -> longitud\n",
    "feature_lengths = {feat: len(feat) for feat in set(features)}\n",
    "print(f\"Dict comprehension (feature -> len): {feature_lengths}\")\n",
    "\n",
    "# Set comprehension: features unicos\n",
    "features_unicos = {feat.upper() for feat in features}\n",
    "print(f\"Set comprehension (unicos upper): {features_unicos}\")\n",
    "\n",
    "\n",
    "# --- Comparacion de rendimiento: loop vs comprehension ---\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"  Rendimiento: loop vs comprehension (100,000 elementos)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "N = 100_000\n",
    "datos = list(range(N))\n",
    "\n",
    "# Enfoque 1: loop clasico\n",
    "t0 = time.perf_counter()\n",
    "resultado_loop = []\n",
    "for x in datos:\n",
    "    if x % 3 == 0:\n",
    "        resultado_loop.append(x * x + 2 * x + 1)\n",
    "t_loop = time.perf_counter() - t0\n",
    "\n",
    "# Enfoque 2: list comprehension\n",
    "t0 = time.perf_counter()\n",
    "resultado_comp = [x * x + 2 * x + 1 for x in datos if x % 3 == 0]\n",
    "t_comp = time.perf_counter() - t0\n",
    "\n",
    "assert resultado_loop == resultado_comp, \"Los resultados deben ser iguales\"\n",
    "\n",
    "print(f\"  {'Enfoque':<25} {'Tiempo (ms)':>12} {'Speedup':>10}\")\n",
    "print(f\"  {'-'*25} {'-'*12} {'-'*10}\")\n",
    "print(f\"  {'Loop + append':<25} {t_loop*1000:>12.2f} {'1.00x':>10}\")\n",
    "print(f\"  {'List comprehension':<25} {t_comp*1000:>12.2f} {f'{t_loop/t_comp:.2f}x':>10}\")\n",
    "print(f\"\\n  Elementos resultantes: {len(resultado_comp):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 4: Logging Profesional\n",
    "\n",
    "### Los 5 niveles de logging\n",
    "\n",
    "| Nivel      | Valor | Cuando usarlo                                         | Ejemplo en AI/ML                       |\n",
    "|------------|-------|-------------------------------------------------------|----------------------------------------|\n",
    "| `DEBUG`    | 10    | Detalle fino para diagnostico                         | Forma del tensor en cada capa          |\n",
    "| `INFO`     | 20    | Confirmacion de que todo funciona                     | \"Modelo cargado en 2.3s\"              |\n",
    "| `WARNING`  | 30    | Algo inesperado, pero el sistema sigue               | \"GPU no disponible, usando CPU\"        |\n",
    "| `ERROR`    | 40    | Fallo que impide una operacion                        | \"No se pudo procesar el batch 42\"      |\n",
    "| `CRITICAL` | 50    | Fallo total del sistema                               | \"Base de datos de features inalcanzable\" |\n",
    "\n",
    "### Arquitectura del logging\n",
    "\n",
    "```\n",
    "  Tu codigo\n",
    "      |\n",
    "      v\n",
    "  Logger (nombre unico, ej: 'ml_pipeline')\n",
    "      |\n",
    "      +---> Handler 1: StreamHandler  --> consola\n",
    "      |         |\n",
    "      |     Formatter: '%(asctime)s - %(levelname)s - %(message)s'\n",
    "      |\n",
    "      +---> Handler 2: FileHandler    --> archivo.log\n",
    "                |\n",
    "            Formatter: (puede ser diferente)\n",
    "```\n",
    "\n",
    "**Regla importante:** Nunca uses `print()` para diagnostico en produccion. Usa `logging`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Logging profesional: configuracion con handlers\n",
    "# ============================================================\n",
    "\n",
    "# Crear un logger dedicado (NO usar el root logger)\n",
    "logger = logging.getLogger(\"ml_pipeline_demo\")\n",
    "logger.setLevel(logging.DEBUG)  # nivel mas bajo; los handlers filtran\n",
    "\n",
    "# Limpiar handlers previos (por si re-ejecutas la celda)\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Handler 1: consola (solo INFO y superiores)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "console_fmt = logging.Formatter(\"%(asctime)s | %(levelname)-8s | %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "console_handler.setFormatter(console_fmt)\n",
    "\n",
    "# Handler 2: StringIO (captura todo, incluyendo DEBUG)\n",
    "log_capture = StringIO()\n",
    "string_handler = logging.StreamHandler(log_capture)\n",
    "string_handler.setLevel(logging.DEBUG)\n",
    "string_fmt = logging.Formatter(\"%(levelname)-8s | %(name)s | %(message)s\")\n",
    "string_handler.setFormatter(string_fmt)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(string_handler)\n",
    "\n",
    "# --- Demo de los 5 niveles ---\n",
    "print(\"--- Mensajes visibles en consola (>= INFO) ---\\n\")\n",
    "\n",
    "logger.debug(\"Forma del input: (32, 768)\")\n",
    "logger.info(\"Modelo cargado exitosamente en %.2f segundos\", 2.34)\n",
    "logger.warning(\"GPU no detectada, fallback a CPU\")\n",
    "logger.error(\"Batch %d fallo: datos incompletos\", 42)\n",
    "logger.critical(\"Conexion a base de features PERDIDA\")\n",
    "\n",
    "# Mostrar lo que capturo el string handler (incluye DEBUG)\n",
    "print(\"\\n--- Captura completa (incluye DEBUG) ---\")\n",
    "print(log_capture.getvalue())\n",
    "\n",
    "# --- Anti-patron vs patron correcto ---\n",
    "print(\"--- Anti-patron vs patron correcto ---\\n\")\n",
    "\n",
    "modelo = \"gpt-mini\"\n",
    "latencia = 0.342\n",
    "\n",
    "# MAL: f-string se evalua SIEMPRE, incluso si el nivel esta desactivado\n",
    "# logger.debug(f\"Modelo {modelo} respondio en {latencia:.3f}s\")  # <-- NO\n",
    "print(\"  MAL:  logger.debug(f'Modelo {modelo} respondio en {latencia:.3f}s')\")\n",
    "print(\"        -> El f-string se evalua SIEMPRE aunque DEBUG este desactivado\\n\")\n",
    "\n",
    "# BIEN: lazy formatting con % -- solo se evalua si el mensaje se emite\n",
    "# logger.debug(\"Modelo %s respondio en %.3fs\", modelo, latencia)  # <-- SI\n",
    "print(\"  BIEN: logger.debug('Modelo %s respondio en %.3fs', modelo, latencia)\")\n",
    "print(\"        -> Se evalua SOLO si el nivel DEBUG esta activo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Seccion 5: Pydantic - Validacion de Datos\n",
    "\n",
    "### Por que Pydantic en AI/ML\n",
    "\n",
    "En pipelines de ML, la mayoria de los bugs **no vienen del modelo**, sino de:\n",
    "- Datos de entrada mal formateados (tipos incorrectos, campos faltantes).\n",
    "- Valores fuera de rango que el modelo no sabe manejar.\n",
    "- Contratos rotos entre componentes del sistema.\n",
    "\n",
    "**Pydantic** resuelve esto con:\n",
    "\n",
    "| Concepto         | Que hace                                             |\n",
    "|------------------|------------------------------------------------------|\n",
    "| `BaseModel`      | Clase base que valida datos al instanciar            |\n",
    "| `Field(...)`     | Define restricciones: min, max, default, descripcion |\n",
    "| `field_validator` | Validacion custom con logica de negocio             |\n",
    "| `ValidationError`| Excepcion estructurada con detalle de cada error     |\n",
    "\n",
    "### Flujo\n",
    "\n",
    "```\n",
    "  Datos crudos (dict, JSON, API request)\n",
    "      |\n",
    "      v\n",
    "  PydanticModel(**datos)\n",
    "      |\n",
    "      +-- Valido?   SI --> objeto tipado con autocompletado\n",
    "      |               \n",
    "      +-- Invalido? NO --> ValidationError con lista de errores\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pydantic: modelos de validacion para AI/ML\n",
    "# ============================================================\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "\n",
    "\n",
    "class InferenceRequest(BaseModel):\n",
    "    \"\"\"Modelo de request para un endpoint de inferencia.\"\"\"\n",
    "    prompt: str = Field(min_length=5, max_length=2000, description=\"Texto de entrada\")\n",
    "    max_tokens: int = Field(default=256, ge=1, le=4096, description=\"Tokens maximos\")\n",
    "    temperature: float = Field(default=0.2, ge=0.0, le=2.0, description=\"Creatividad\")\n",
    "    model: str = Field(default=\"gpt-mini\", description=\"Modelo a usar\")\n",
    "\n",
    "\n",
    "class FeatureRow(BaseModel):\n",
    "    \"\"\"Modelo para una fila de features de un dataset de ML.\"\"\"\n",
    "    user_id: str = Field(min_length=1, description=\"ID unico del usuario\")\n",
    "    age: int = Field(ge=0, le=120, description=\"Edad en anios\")\n",
    "    monthly_income_usd: float = Field(ge=0, description=\"Ingreso mensual en USD\")\n",
    "    tenure_months: int = Field(ge=0, description=\"Meses como cliente\")\n",
    "\n",
    "    @field_validator(\"monthly_income_usd\")\n",
    "    @classmethod\n",
    "    def reject_unrealistic_income(cls, value: float) -> float:\n",
    "        if value > 1_000_000:\n",
    "            raise ValueError(\"monthly_income_usd parece fuera de rango realista (> 1M)\")\n",
    "        return value\n",
    "\n",
    "\n",
    "# --- Demo 1: datos validos ---\n",
    "print(\"=\" * 55)\n",
    "print(\"  Datos VALIDOS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "req = InferenceRequest(prompt=\"Explica que es machine learning\", temperature=0.7)\n",
    "print(f\"  InferenceRequest: {req.model_dump()}\")\n",
    "\n",
    "fila = FeatureRow(user_id=\"u001\", age=28, monthly_income_usd=4500.0, tenure_months=18)\n",
    "print(f\"  FeatureRow: {fila.model_dump()}\")\n",
    "\n",
    "\n",
    "# --- Demo 2: datos invalidos ---\n",
    "print(f\"\\n{'=' * 55}\")\n",
    "print(\"  Datos INVALIDOS (Pydantic captura cada error)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Request invalido\n",
    "try:\n",
    "    InferenceRequest(\n",
    "        prompt=\"Hi\",         # muy corto (min_length=5)\n",
    "        max_tokens=-10,       # negativo (ge=1)\n",
    "        temperature=5.0,      # fuera de rango (le=2.0)\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"\\n  InferenceRequest - {e.error_count()} errores:\")\n",
    "    for err in e.errors():\n",
    "        print(f\"    - Campo: {err['loc']} | Tipo: {err['type']} | Msg: {err['msg']}\")\n",
    "\n",
    "# Feature row invalida\n",
    "try:\n",
    "    FeatureRow(\n",
    "        user_id=\"\",               # vacio (min_length=1)\n",
    "        age=200,                   # fuera de rango (le=120)\n",
    "        monthly_income_usd=5_000_000,  # validador custom\n",
    "        tenure_months=-5,          # negativo (ge=0)\n",
    "    )\n",
    "except ValidationError as e:\n",
    "    print(f\"\\n  FeatureRow - {e.error_count()} errores:\")\n",
    "    for err in e.errors():\n",
    "        print(f\"    - Campo: {err['loc']} | Tipo: {err['type']} | Msg: {err['msg']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comparacion: validacion con dict vs Pydantic\n",
    "# ============================================================\n",
    "\n",
    "def fake_model_predict(age: int, income: float, tenure: int) -> float:\n",
    "    \"\"\"Simula un modelo que espera tipos y rangos correctos.\"\"\"\n",
    "    if not isinstance(age, int):\n",
    "        raise TypeError(f\"age debe ser int, recibido {type(age).__name__}\")\n",
    "    # Calcular un \"score\" ficticio\n",
    "    return round(min(1.0, (age * 0.01 + income * 0.00001 + tenure * 0.005)), 3)\n",
    "\n",
    "\n",
    "# --- Enfoque 1: dict sin validacion ---\n",
    "print(\"=\" * 55)\n",
    "print(\"  Enfoque 1: Dict sin validacion\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "datos_malos = {\n",
    "    \"user_id\": \"u099\",\n",
    "    \"age\": \"veintiocho\",          # string en vez de int!\n",
    "    \"monthly_income_usd\": 4500,\n",
    "    \"tenure_months\": 12,\n",
    "}\n",
    "\n",
    "# El dict NO valida nada. El error aparece DESPUES, lejos del origen.\n",
    "try:\n",
    "    score = fake_model_predict(\n",
    "        age=datos_malos[\"age\"],\n",
    "        income=datos_malos[\"monthly_income_usd\"],\n",
    "        tenure=datos_malos[\"tenure_months\"],\n",
    "    )\n",
    "except TypeError as e:\n",
    "    print(f\"  Error TARDIO en el modelo: {e}\")\n",
    "    print(\"  -> El error se detecto LEJOS de donde entraron los datos\")\n",
    "\n",
    "\n",
    "# --- Enfoque 2: Pydantic ---\n",
    "print(f\"\\n{'=' * 55}\")\n",
    "print(\"  Enfoque 2: Pydantic (falla TEMPRANO)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "try:\n",
    "    fila_validada = FeatureRow(**datos_malos)\n",
    "except ValidationError as e:\n",
    "    print(f\"  Error TEMPRANO en la validacion: {e.error_count()} error(es)\")\n",
    "    for err in e.errors():\n",
    "        print(f\"    - Campo: {err['loc']} | Msg: {err['msg']}\")\n",
    "    print(\"  -> El error se detecto ANTES de llegar al modelo\")\n",
    "\n",
    "# Con datos correctos, Pydantic garantiza tipos\n",
    "print(f\"\\n{'=' * 55}\")\n",
    "print(\"  Enfoque 2: Pydantic con datos correctos\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "datos_buenos = {\n",
    "    \"user_id\": \"u100\",\n",
    "    \"age\": 28,\n",
    "    \"monthly_income_usd\": 4500.0,\n",
    "    \"tenure_months\": 12,\n",
    "}\n",
    "\n",
    "fila_ok = FeatureRow(**datos_buenos)\n",
    "score = fake_model_predict(\n",
    "    age=fila_ok.age,\n",
    "    income=fila_ok.monthly_income_usd,\n",
    "    tenure=fila_ok.tenure_months,\n",
    ")\n",
    "print(f\"  Datos validados: {fila_ok.model_dump()}\")\n",
    "print(f\"  Score predicho: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicios\n",
    "\n",
    "Los siguientes ejercicios combinan todos los temas vistos en este notebook. Completa el cuerpo de cada funcion/clase reemplazando `pass` con tu implementacion.\n",
    "\n",
    "**Tip:** Ejecuta cada celda para verificar que funciona. Los `assert` al final te confirman si la solucion es correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ejercicio 1: Data Loader Resiliente\n",
    "# ============================================================\n",
    "#\n",
    "# Construi un generador `cargar_registros(datos_crudos)` que:\n",
    "#\n",
    "# 1. Recibe una lista de diccionarios (simulando registros JSON).\n",
    "# 2. Para cada registro:\n",
    "#    a. Si le falta el campo \"user_id\" -> loguear WARNING y saltar.\n",
    "#    b. Si el campo \"score\" no es numerico -> loguear WARNING y saltar.\n",
    "#    c. Si el campo \"score\" es negativo -> levantar DataValidationError.\n",
    "#    d. Si es valido -> yield el registro.\n",
    "# 3. Al terminar, loguear INFO con la cantidad de registros procesados.\n",
    "#\n",
    "# Usa: generadores, logging, excepciones custom (PipelineError/DataValidationError\n",
    "# definidos arriba), try/except.\n",
    "#\n",
    "# Ejemplo de datos de entrada:\n",
    "# datos = [\n",
    "#     {\"user_id\": \"u1\", \"score\": 0.9},   # valido\n",
    "#     {\"score\": 0.5},                      # sin user_id -> warning\n",
    "#     {\"user_id\": \"u3\", \"score\": \"abc\"},   # score no numerico -> warning\n",
    "#     {\"user_id\": \"u4\", \"score\": -0.1},    # score negativo -> DataValidationError\n",
    "#     {\"user_id\": \"u5\", \"score\": 0.7},     # valido\n",
    "# ]\n",
    "\n",
    "ej1_logger = logging.getLogger(\"ejercicio_1\")\n",
    "ej1_logger.setLevel(logging.DEBUG)\n",
    "ej1_logger.handlers.clear()\n",
    "ej1_handler = logging.StreamHandler(sys.stdout)\n",
    "ej1_handler.setFormatter(logging.Formatter(\"%(levelname)-8s | %(message)s\"))\n",
    "ej1_logger.addHandler(ej1_handler)\n",
    "\n",
    "\n",
    "def cargar_registros(datos_crudos: list) -> Generator[dict, None, None]:\n",
    "    \"\"\"Generador que carga y valida registros de forma resiliente.\"\"\"\n",
    "    pass  # <-- Tu implementacion aqui\n",
    "\n",
    "\n",
    "# --- Prueba ---\n",
    "# datos_prueba = [\n",
    "#     {\"user_id\": \"u1\", \"score\": 0.9},\n",
    "#     {\"score\": 0.5},\n",
    "#     {\"user_id\": \"u3\", \"score\": \"abc\"},\n",
    "#     {\"user_id\": \"u5\", \"score\": 0.7},\n",
    "# ]\n",
    "# resultados = list(cargar_registros(datos_prueba))\n",
    "# assert len(resultados) == 2, f\"Se esperaban 2 validos, se obtuvieron {len(resultados)}\"\n",
    "# print(f\"\\nEjercicio 1 OK: {len(resultados)} registros validos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ejercicio 2: Pydantic - TrainingConfig\n",
    "# ============================================================\n",
    "#\n",
    "# Crea un modelo Pydantic `TrainingConfig` con las siguientes restricciones:\n",
    "#\n",
    "# - experiment_name: str, minimo 3 caracteres, maximo 50\n",
    "# - learning_rate: float, default 0.001, rango (0.0, 1.0) exclusivo\n",
    "#                  (gt=0, lt=1.0)\n",
    "# - epochs: int, default 10, rango [1, 1000]\n",
    "# - batch_size: int, default 32, debe ser potencia de 2\n",
    "#              (usa field_validator para verificar: n & (n - 1) == 0 y n > 0)\n",
    "# - optimizer: Literal[\"adam\", \"sgd\", \"adamw\"], default \"adam\"\n",
    "# - dropout: float, default 0.1, rango [0.0, 0.9]\n",
    "#\n",
    "# Despues de definir el modelo:\n",
    "# 1. Crea una instancia valida y muestrala.\n",
    "# 2. Intenta crear una invalida (batch_size=48, learning_rate=5.0)\n",
    "#    y captura el ValidationError.\n",
    "\n",
    "\n",
    "class TrainingConfig(BaseModel):\n",
    "    \"\"\"Configuracion validada para un entrenamiento de modelo ML.\"\"\"\n",
    "    pass  # <-- Tu implementacion aqui\n",
    "\n",
    "\n",
    "# --- Prueba ---\n",
    "# config_ok = TrainingConfig(experiment_name=\"churn_v3\")\n",
    "# print(f\"Config valida: {config_ok.model_dump()}\")\n",
    "#\n",
    "# try:\n",
    "#     TrainingConfig(experiment_name=\"x\", batch_size=48, learning_rate=5.0)\n",
    "# except ValidationError as e:\n",
    "#     print(f\"\\nErrores capturados: {e.error_count()}\")\n",
    "#     for err in e.errors():\n",
    "#         print(f\"  - {err['loc']}: {err['msg']}\")\n",
    "#     print(\"\\nEjercicio 2 OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Ejercicio 3: Pipeline completo con generadores + logging +\n",
    "#              excepciones + Pydantic\n",
    "# ============================================================\n",
    "#\n",
    "# Construi un pipeline que procese un stream de registros de prediccion:\n",
    "#\n",
    "# 1. Defini un modelo Pydantic `PredictionRecord`:\n",
    "#    - request_id: str (min 1 char)\n",
    "#    - input_text: str (min 10 chars)\n",
    "#    - confidence: float (0.0 a 1.0)\n",
    "#\n",
    "# 2. Defini un generador `validar_stream(records)` que:\n",
    "#    - Recibe una lista de dicts\n",
    "#    - Intenta crear un PredictionRecord con cada dict\n",
    "#    - Si es valido, yield el modelo validado\n",
    "#    - Si es invalido, loguear WARNING con los errores y continuar\n",
    "#\n",
    "# 3. Defini un generador `filtrar_alta_confianza(stream, umbral=0.8)` que:\n",
    "#    - Recibe el generador anterior\n",
    "#    - Solo yield registros con confidence >= umbral\n",
    "#\n",
    "# 4. Defini una funcion `procesar_pipeline(records, umbral)` que:\n",
    "#    - Encadena validar_stream â†’ filtrar_alta_confianza\n",
    "#    - Retorna lista de resultados\n",
    "#    - Loguea INFO con el total de resultados\n",
    "\n",
    "\n",
    "# Tu implementacion aqui:\n",
    "pass\n",
    "\n",
    "\n",
    "# --- Datos de prueba ---\n",
    "# registros_prueba = [\n",
    "#     {\"request_id\": \"r1\", \"input_text\": \"analizar este texto de ejemplo\", \"confidence\": 0.95},\n",
    "#     {\"request_id\": \"r2\", \"input_text\": \"otro texto largo suficiente\", \"confidence\": 0.60},\n",
    "#     {\"request_id\": \"\", \"input_text\": \"corto\", \"confidence\": 1.5},  # invalido\n",
    "#     {\"request_id\": \"r4\", \"input_text\": \"texto de prueba completo\", \"confidence\": 0.88},\n",
    "# ]\n",
    "#\n",
    "# resultados = procesar_pipeline(registros_prueba, umbral=0.8)\n",
    "# assert len(resultados) == 2, f\"Se esperaban 2, se obtuvieron {len(resultados)}\"\n",
    "# print(f\"\\nEjercicio 3 OK: {len(resultados)} registros de alta confianza\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checklist de consolidacion\n",
    "\n",
    "Antes de pasar al siguiente notebook, verifica que puedes:\n",
    "\n",
    "- [ ] Escribir un bloque `try / except / else / finally` completo y explicar cuando se ejecuta cada parte.\n",
    "- [ ] Crear una jerarquia de excepciones custom con atributos adicionales y usar `raise ... from ...` para encadenar errores.\n",
    "- [ ] Explicar la diferencia entre `return` y `yield`, y cuando un generador es preferible a una lista.\n",
    "- [ ] Construir un pipeline de generadores encadenados que procese datos de forma lazy.\n",
    "- [ ] Usar `sys.getsizeof()` para comparar el consumo de memoria entre una lista y un generador.\n",
    "- [ ] Elegir correctamente entre comprehension y loop segun el caso (transformacion vs side effects).\n",
    "- [ ] Configurar un logger con multiples handlers, niveles y formatters (sin usar `print` para diagnostico).\n",
    "- [ ] Usar lazy formatting (`%s`, `%d`) en lugar de f-strings dentro de llamadas a logging.\n",
    "- [ ] Definir modelos Pydantic con `Field`, `field_validator` y capturar `ValidationError` de forma estructurada.\n",
    "- [ ] Integrar excepciones + generadores + logging + Pydantic en un pipeline resiliente.\n",
    "\n",
    "---\n",
    "\n",
    "### Proximo paso\n",
    "\n",
    "**Notebook 04: Programacion Orientada a Objetos (OOP)** - Clases, herencia, protocolos y patrones de diseno para sistemas de ML.\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook 03 - Modulo 01 - Python Extra Class - AI Engineering Henry*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}