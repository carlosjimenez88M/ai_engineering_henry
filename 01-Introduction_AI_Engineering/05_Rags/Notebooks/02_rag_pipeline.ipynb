{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Pipeline RAG Completo - De Documentos a Respuestas Fundamentadas\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook, seras capaz de:\n",
    "\n",
    "1. **Comprender** la arquitectura end-to-end de un pipeline RAG (Retrieval-Augmented Generation)\n",
    "2. **Implementar** estrategias de chunking y evaluar sus trade-offs\n",
    "3. **Crear embeddings** con OpenAI y almacenarlos en un vector store (ChromaDB)\n",
    "4. **Construir** un sistema de retrieval con filtros por metadata\n",
    "5. **Integrar** retrieval + generacion en un pipeline completo y funcional\n",
    "6. **Evaluar** la calidad de las respuestas (faithfulness, relevance, groundedness)\n",
    "7. **Comparar** respuestas de un LLM \"naive\" vs un LLM aumentado con RAG\n",
    "\n",
    "### Prerequisitos\n",
    "- Notebook 01 completado (conceptos basicos de embeddings y similitud)\n",
    "- API key de OpenAI configurada en `.env`\n",
    "- Familiaridad basica con Python y f-strings\n",
    "\n",
    "### Tiempo estimado: 90-120 minutos\n",
    "\n",
    "> **Nota importante**: Este notebook sigue el principio de *\"show, don't tell\"*. Cada concepto se introduce con una breve explicacion teorica seguida inmediatamente de codigo ejecutable. La mejor forma de aprender RAG es construyendo uno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Arquitectura RAG: Vista de Pajaro\n",
    "\n",
    "Antes de escribir una sola linea de codigo, entendamos el flujo completo.\n",
    "\n",
    "### Pipeline de Indexacion (offline, se ejecuta una vez)\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌──────────────┐     ┌──────────────┐     ┌──────────────────┐\n",
    "│  DOCUMENTOS  │────>│   CHUNKING   │────>│  EMBEDDING   │────>│   VECTOR STORE   │\n",
    "│  (.md files) │     │ (split text) │     │  (OpenAI)    │     │   (ChromaDB)     │\n",
    "└─────────────┘     └──────────────┘     └──────────────┘     └──────────────────┘\n",
    "                                                                       │\n",
    "                                                                       │ persistido\n",
    "                                                                       v\n",
    "                                                               ┌──────────────────┐\n",
    "                                                               │   Indices listos  │\n",
    "                                                               │   para consulta   │\n",
    "                                                               └──────────────────┘\n",
    "```\n",
    "\n",
    "### Pipeline de Consulta (online, se ejecuta por cada query del usuario)\n",
    "\n",
    "```\n",
    "┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐     ┌────────────┐\n",
    "│  QUERY   │────>│  EMBEDDING   │────>│   RETRIEVAL   │────>│   CONTEXT    │────>│    LLM     │\n",
    "│ (usuario)│     │  del query   │     │  (top-k mas   │     │  assembly    │     │  (OpenAI)  │\n",
    "│          │     │              │     │   similares)  │     │              │     │            │\n",
    "└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘     └────────────┘\n",
    "                                                                                       │\n",
    "                                                                                       v\n",
    "                                                                               ┌────────────┐\n",
    "                                                                               │  RESPUESTA │\n",
    "                                                                               │ fundamentada│\n",
    "                                                                               └────────────┘\n",
    "```\n",
    "\n",
    "### Por que esta arquitectura?\n",
    "\n",
    "| Componente | Funcion | Analogia |\n",
    "|:-----------|:--------|:---------|\n",
    "| Chunking | Dividir documentos en fragmentos manejables | Crear fichas de estudio a partir de un libro |\n",
    "| Embedding | Convertir texto a vectores numericos | Traducir palabras a coordenadas en un mapa semantico |\n",
    "| Vector Store | Almacenar y buscar vectores eficientemente | Una biblioteca con indice por temas |\n",
    "| Retrieval | Encontrar los fragmentos mas relevantes | Un bibliotecario que sabe donde buscar |\n",
    "| LLM + Context | Generar respuesta basada en evidencia | Un experto que responde citando fuentes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup e Importaciones\n",
    "\n",
    "Instalamos y cargamos todas las dependencias necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Instalacion de dependencias (ejecutar solo la primera vez) ===\n",
    "# %pip install openai chromadb langchain langchain-text-splitters python-dotenv matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Importaciones ===\n",
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# OpenAI para embeddings y generacion\n",
    "from openai import OpenAI\n",
    "\n",
    "# ChromaDB como vector store\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# LangChain para estrategias de chunking\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Visualizacion\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Variables de entorno\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Cargar variables de entorno ---\n",
    "load_dotenv()\n",
    "\n",
    "# --- Inicializar cliente de OpenAI ---\n",
    "client = OpenAI()  # Usa OPENAI_API_KEY del entorno automaticamente\n",
    "\n",
    "# --- Verificacion ---\n",
    "print(\"=\" * 60)\n",
    "print(\"SETUP COMPLETO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"OpenAI API key cargada: {'Si' if os.getenv('OPENAI_API_KEY') else 'No'}\")\n",
    "print(f\"ChromaDB version: {chromadb.__version__}\")\n",
    "print(f\"Directorio de trabajo: {os.getcwd()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 1: Carga de Documentos\n",
    "\n",
    "Todo pipeline RAG comienza con **documentos fuente**. La calidad de tu RAG depende directamente de la calidad de estos documentos.\n",
    "\n",
    "### Que hace un buen documento fuente?\n",
    "\n",
    "- **Estructura clara**: Headers, secciones, listas. Facilita el chunking.\n",
    "- **Informacion precisa y actualizada**: Garbage in, garbage out.\n",
    "- **Metadata rica**: Fecha, autor, categoria. Permite filtros en retrieval.\n",
    "- **Sin ambiguedades**: Informacion contradictoria confunde al retriever y al LLM.\n",
    "\n",
    "Vamos a cargar dos bases de conocimiento de NovaTech Solutions:\n",
    "1. `base_conocimiento_productos.md` - Informacion comercial (planes, precios, features)\n",
    "2. `base_conocimiento_tecnica.md` - Informacion tecnica (arquitectura, deploys, troubleshooting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 1: Cargar documentos fuente ===\n",
    "\n",
    "# Ruta al directorio de datos (relativa al notebook)\n",
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "# Definimos los archivos a cargar con su metadata\n",
    "archivos_fuente: dict[str, dict] = {\n",
    "    \"base_conocimiento_productos.md\": {\n",
    "        \"domain\": \"productos\",\n",
    "        \"description\": \"Informacion comercial de productos NovaTech\"\n",
    "    },\n",
    "    \"base_conocimiento_tecnica.md\": {\n",
    "        \"domain\": \"tecnica\",\n",
    "        \"description\": \"Guia tecnica y operaciones NovaTech\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Cargar contenido de cada archivo\n",
    "documentos: dict[str, str] = {}\n",
    "\n",
    "for nombre_archivo, metadata in archivos_fuente.items():\n",
    "    ruta = DATA_DIR / nombre_archivo\n",
    "    \n",
    "    if not ruta.exists():\n",
    "        print(f\"ADVERTENCIA: No se encontro {ruta}\")\n",
    "        continue\n",
    "    \n",
    "    with open(ruta, \"r\", encoding=\"utf-8\") as f:\n",
    "        contenido = f.read()\n",
    "    \n",
    "    documentos[nombre_archivo] = contenido\n",
    "    \n",
    "    # Vista previa\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Archivo: {nombre_archivo}\")\n",
    "    print(f\"Dominio: {metadata['domain']}\")\n",
    "    print(f\"Longitud: {len(contenido):,} caracteres | {len(contenido.split()):,} palabras\")\n",
    "    print(f\"Lineas: {len(contenido.splitlines()):,}\")\n",
    "    print(f\"{'-' * 60}\")\n",
    "    print(f\"Preview (primeros 300 caracteres):\")\n",
    "    print(contenido[:300])\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "print(f\"\\nTotal documentos cargados: {len(documentos)}\")\n",
    "print(f\"Total caracteres: {sum(len(c) for c in documentos.values()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 2: Estrategias de Chunking\n",
    "\n",
    "El **chunking** es una de las decisiones mas criticas en un pipeline RAG. Determina:\n",
    "\n",
    "- **Granularidad del retrieval**: Chunks muy grandes traen ruido; muy pequenos pierden contexto.\n",
    "- **Calidad del embedding**: Un embedding de un chunk coherente es mas util que uno de texto cortado arbitrariamente.\n",
    "- **Costo**: Mas chunks = mas embeddings = mas tokens = mas dinero.\n",
    "\n",
    "### Tres estrategias principales\n",
    "\n",
    "| Estrategia | Descripcion | Pros | Contras |\n",
    "|:-----------|:------------|:-----|:--------|\n",
    "| **Fixed-Size** | Cortar cada N caracteres | Simple, predecible | Rompe oraciones y parrafos |\n",
    "| **Recursive Character** | Intentar cortar por `\\n\\n`, luego `\\n`, luego ` `, luego caracter | Respeta estructura | Chunks de tamano variable |\n",
    "| **Semantic** | Agrupar por similitud semantica | Chunks mas coherentes | Mas costoso y complejo |\n",
    "\n",
    "Implementaremos las dos primeras y las compararemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Fixed-Size Chunking (implementacion manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Estrategia 1: Fixed-Size Chunking ===\n",
    "# Implementacion manual para entender exactamente que sucede\n",
    "\n",
    "def fixed_chunks(text: str, size: int = 500, overlap: int = 50) -> list[str]:\n",
    "    \"\"\"\n",
    "    Divide texto en chunks de tamano fijo con overlap.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a dividir.\n",
    "        size: Tamano de cada chunk en caracteres.\n",
    "        overlap: Cantidad de caracteres que se solapan entre chunks consecutivos.\n",
    "                 El overlap previene que informacion importante se pierda\n",
    "                 en el corte entre dos chunks.\n",
    "    \n",
    "    Returns:\n",
    "        Lista de strings, cada uno de longitud <= size.\n",
    "    \"\"\"\n",
    "    if overlap >= size:\n",
    "        raise ValueError(f\"El overlap ({overlap}) debe ser menor que el size ({size})\")\n",
    "    \n",
    "    chunks: list[str] = []\n",
    "    inicio = 0\n",
    "    \n",
    "    while inicio < len(text):\n",
    "        fin = inicio + size\n",
    "        chunk = text[inicio:fin]\n",
    "        chunks.append(chunk)\n",
    "        inicio += size - overlap  # Avanzamos menos que el size para crear overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Aplicar fixed chunking a ambos documentos ---\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "fixed_chunks_por_doc: dict[str, list[str]] = {}\n",
    "\n",
    "for nombre, contenido in documentos.items():\n",
    "    chunks = fixed_chunks(contenido, size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\n",
    "    fixed_chunks_por_doc[nombre] = chunks\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Documento: {nombre}\")\n",
    "    print(f\"Chunks generados: {len(chunks)}\")\n",
    "    print(f\"Tamano promedio: {sum(len(c) for c in chunks) / len(chunks):.0f} caracteres\")\n",
    "    print(f\"Tamano min/max: {min(len(c) for c in chunks)} / {max(len(c) for c in chunks)}\")\n",
    "\n",
    "# --- Mostrar un chunk de ejemplo ---\n",
    "ejemplo_chunk = fixed_chunks_por_doc[\"base_conocimiento_productos.md\"][2]\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"EJEMPLO - Chunk #2 de base_conocimiento_productos.md:\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(ejemplo_chunk)\n",
    "print(f\"\\n[Longitud: {len(ejemplo_chunk)} caracteres]\")\n",
    "print(\"\\n>>> Observa: el corte puede ocurrir en medio de una oracion o seccion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Recursive Character Splitting (LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Estrategia 2: Recursive Character Splitting ===\n",
    "# LangChain intenta dividir respetando la estructura del texto\n",
    "\n",
    "# El splitter intenta dividir en este orden de prioridad:\n",
    "# 1. \"\\n\\n\" (parrafos)\n",
    "# 2. \"\\n\" (lineas)\n",
    "# 3. \" \" (palabras)\n",
    "# 4. \"\" (caracteres individuales, ultimo recurso)\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Orden de prioridad\n",
    ")\n",
    "\n",
    "# --- Aplicar recursive splitting a ambos documentos ---\n",
    "recursive_chunks_por_doc: dict[str, list[str]] = {}\n",
    "\n",
    "for nombre, contenido in documentos.items():\n",
    "    chunks = recursive_splitter.split_text(contenido)\n",
    "    recursive_chunks_por_doc[nombre] = chunks\n",
    "    \n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Documento: {nombre}\")\n",
    "    print(f\"Chunks generados: {len(chunks)}\")\n",
    "    print(f\"Tamano promedio: {sum(len(c) for c in chunks) / len(chunks):.0f} caracteres\")\n",
    "    print(f\"Tamano min/max: {min(len(c) for c in chunks)} / {max(len(c) for c in chunks)}\")\n",
    "\n",
    "# --- Mostrar un chunk de ejemplo ---\n",
    "ejemplo_recursive = recursive_chunks_por_doc[\"base_conocimiento_productos.md\"][2]\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"EJEMPLO - Chunk #2 de base_conocimiento_productos.md (Recursive):\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(ejemplo_recursive)\n",
    "print(f\"\\n[Longitud: {len(ejemplo_recursive)} caracteres]\")\n",
    "print(\"\\n>>> Observa: el recursive splitter intenta respetar limites de parrafo y linea.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Comparacion de Estrategias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comparacion lado a lado ===\n",
    "\n",
    "print(\"COMPARACION DE ESTRATEGIAS DE CHUNKING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metrica':<35} {'Fixed-Size':>15} {'Recursive':>15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Agregar todos los chunks de ambos documentos\n",
    "all_fixed = [c for chunks in fixed_chunks_por_doc.values() for c in chunks]\n",
    "all_recursive = [c for chunks in recursive_chunks_por_doc.values() for c in chunks]\n",
    "\n",
    "metricas = [\n",
    "    (\"Total chunks\", len(all_fixed), len(all_recursive)),\n",
    "    (\"Tamano promedio (chars)\", \n",
    "     f\"{sum(len(c) for c in all_fixed) / len(all_fixed):.0f}\",\n",
    "     f\"{sum(len(c) for c in all_recursive) / len(all_recursive):.0f}\"),\n",
    "    (\"Tamano min (chars)\",\n",
    "     min(len(c) for c in all_fixed),\n",
    "     min(len(c) for c in all_recursive)),\n",
    "    (\"Tamano max (chars)\",\n",
    "     max(len(c) for c in all_fixed),\n",
    "     max(len(c) for c in all_recursive)),\n",
    "    (\"Desviacion estandar (chars)\",\n",
    "     f\"{(sum((len(c) - sum(len(c) for c in all_fixed)/len(all_fixed))**2 for c in all_fixed) / len(all_fixed))**0.5:.0f}\",\n",
    "     f\"{(sum((len(c) - sum(len(c) for c in all_recursive)/len(all_recursive))**2 for c in all_recursive) / len(all_recursive))**0.5:.0f}\")\n",
    "]\n",
    "\n",
    "for nombre, fixed, recursive in metricas:\n",
    "    print(f\"{nombre:<35} {str(fixed):>15} {str(recursive):>15}\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualizacion: Distribucion de tamanos de chunks ===\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histograma de tamanos - Fixed\n",
    "axes[0].hist([len(c) for c in all_fixed], bins=20, color=\"#4A90D9\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_title(\"Fixed-Size Chunking\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Tamano del chunk (caracteres)\")\n",
    "axes[0].set_ylabel(\"Frecuencia\")\n",
    "axes[0].axvline(x=CHUNK_SIZE, color=\"red\", linestyle=\"--\", label=f\"chunk_size={CHUNK_SIZE}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Histograma de tamanos - Recursive\n",
    "axes[1].hist([len(c) for c in all_recursive], bins=20, color=\"#50C878\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].set_title(\"Recursive Character Splitting\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Tamano del chunk (caracteres)\")\n",
    "axes[1].set_ylabel(\"Frecuencia\")\n",
    "axes[1].axvline(x=CHUNK_SIZE, color=\"red\", linestyle=\"--\", label=f\"chunk_size={CHUNK_SIZE}\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle(\"Distribucion de tamanos de chunks por estrategia\", fontsize=16, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n>>> El fixed-size genera chunks muy uniformes (todos ~500 chars, excepto el ultimo).\")\n",
    "print(\">>> El recursive genera chunks mas variados porque respeta limites naturales del texto.\")\n",
    "print(\">>> Para RAG, generalmente el recursive es preferible: chunks mas coherentes = mejores embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision**: Para el resto del notebook usaremos **Recursive Character Splitting** porque produce chunks mas coherentes semanticamente, lo cual mejora la calidad de los embeddings y, en consecuencia, la calidad del retrieval.\n",
    "\n",
    "> **Trade-off clave**: El recursive splitting genera chunks de tamano variable. Esto significa que algunos chunks seran mas cortos (y potencialmente menos informativos) que otros. En produccion, es comun filtrar chunks por debajo de un tamano minimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 3: Crear Embeddings e Indexar en Vector Store\n",
    "\n",
    "Ahora convertimos cada chunk en un **vector de alta dimension** (embedding) y lo almacenamos en ChromaDB.\n",
    "\n",
    "### Por que ChromaDB?\n",
    "- Open-source y ligero (corre in-memory o persistido en disco)\n",
    "- API simple y Pythonica\n",
    "- Soporta metadata filtering (crucial para RAG en produccion)\n",
    "- Ideal para prototipos y proyectos educativos\n",
    "\n",
    "### El modelo de embedding\n",
    "Usaremos `text-embedding-3-small` de OpenAI:\n",
    "- **1536 dimensiones** de salida\n",
    "- **Costo**: ~$0.02 por 1M de tokens\n",
    "- **Rendimiento**: Excelente para la mayoria de casos de uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 3: Crear embeddings e indexar ===\n",
    "\n",
    "# Modelo de embedding a utilizar\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "def crear_embedding(texto: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Genera un embedding para un texto dado usando la API de OpenAI.\n",
    "    \n",
    "    Args:\n",
    "        texto: El texto a convertir en vector.\n",
    "    \n",
    "    Returns:\n",
    "        Lista de floats representando el embedding.\n",
    "    \"\"\"\n",
    "    respuesta = client.embeddings.create(\n",
    "        input=texto,\n",
    "        model=EMBEDDING_MODEL\n",
    "    )\n",
    "    return respuesta.data[0].embedding\n",
    "\n",
    "\n",
    "def crear_embeddings_batch(textos: list[str]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Genera embeddings para multiples textos en una sola llamada a la API.\n",
    "    Esto es mas eficiente que llamar uno por uno.\n",
    "    \n",
    "    Args:\n",
    "        textos: Lista de textos a convertir.\n",
    "    \n",
    "    Returns:\n",
    "        Lista de embeddings (cada uno es una lista de floats).\n",
    "    \"\"\"\n",
    "    respuesta = client.embeddings.create(\n",
    "        input=textos,\n",
    "        model=EMBEDDING_MODEL\n",
    "    )\n",
    "    # Ordenar por indice para mantener el orden original\n",
    "    return [item.embedding for item in sorted(respuesta.data, key=lambda x: x.index)]\n",
    "\n",
    "\n",
    "# --- Inicializar ChromaDB (in-memory para este notebook) ---\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Crear (o recrear) la coleccion\n",
    "COLLECTION_NAME = \"novatech_knowledge_base\"\n",
    "\n",
    "# Eliminar coleccion existente si la hay (para re-ejecuciones limpias)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=COLLECTION_NAME)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Distancia coseno para similitud\n",
    ")\n",
    "\n",
    "print(f\"Coleccion '{COLLECTION_NAME}' creada en ChromaDB.\")\n",
    "print(f\"Metrica de distancia: cosine\")\n",
    "\n",
    "# --- Indexar chunks con embeddings y metadata ---\n",
    "total_indexados = 0\n",
    "\n",
    "for nombre_archivo, chunks in recursive_chunks_por_doc.items():\n",
    "    domain = archivos_fuente[nombre_archivo][\"domain\"]\n",
    "    \n",
    "    print(f\"\\nIndexando {nombre_archivo} ({len(chunks)} chunks)...\")\n",
    "    \n",
    "    # Generar embeddings en batch (mas eficiente)\n",
    "    embeddings = crear_embeddings_batch(chunks)\n",
    "    \n",
    "    # Preparar datos para ChromaDB\n",
    "    ids = [f\"{domain}_chunk_{i}\" for i in range(len(chunks))]\n",
    "    metadatas = [\n",
    "        {\n",
    "            \"source\": nombre_archivo,\n",
    "            \"chunk_id\": i,\n",
    "            \"domain\": domain,\n",
    "            \"chunk_size\": len(chunk)\n",
    "        }\n",
    "        for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    \n",
    "    # Insertar en ChromaDB\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        embeddings=embeddings,\n",
    "        documents=chunks,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    total_indexados += len(chunks)\n",
    "    print(f\"  -> {len(chunks)} chunks indexados correctamente.\")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"INDEXACION COMPLETA\")\n",
    "print(f\"Total documentos en la coleccion: {collection.count()}\")\n",
    "print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inspeccionar una entrada de ejemplo ===\n",
    "\n",
    "# Obtener un documento de ejemplo con su metadata\n",
    "muestra = collection.get(\n",
    "    ids=[\"productos_chunk_0\"],\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"]\n",
    ")\n",
    "\n",
    "print(\"EJEMPLO DE ENTRADA EN EL VECTOR STORE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ID: {muestra['ids'][0]}\")\n",
    "print(f\"\\nMetadata:\")\n",
    "for key, value in muestra['metadatas'][0].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nDocumento (texto):\")\n",
    "print(f\"  {muestra['documents'][0][:200]}...\")\n",
    "print(f\"\\nEmbedding (primeros 10 valores de {len(muestra['embeddings'][0])} dimensiones):\")\n",
    "print(f\"  {muestra['embeddings'][0][:10]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 4: Retrieval - Encontrar Informacion Relevante\n",
    "\n",
    "El **retrieval** es el corazon del pipeline RAG. Dado un query del usuario, buscamos los chunks mas semanticamente similares en nuestro vector store.\n",
    "\n",
    "### Como funciona?\n",
    "1. El query del usuario se convierte a un embedding (usando el mismo modelo)\n",
    "2. ChromaDB calcula la distancia coseno entre el query embedding y todos los chunks almacenados\n",
    "3. Retorna los top-K chunks mas cercanos (menor distancia = mayor similitud)\n",
    "\n",
    "### Filtros por metadata\n",
    "Ademas de la similitud semantica, podemos filtrar por metadata. Por ejemplo:\n",
    "- Solo buscar en documentos del dominio \"tecnica\"\n",
    "- Solo buscar en chunks de un archivo especifico\n",
    "\n",
    "Esto es **extremadamente util** en produccion para mejorar precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 4: Funcion de Retrieval ===\n",
    "\n",
    "def retrieve(\n",
    "    query: str,\n",
    "    n_results: int = 3,\n",
    "    domain_filter: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Recupera los chunks mas relevantes para una consulta dada.\n",
    "    \n",
    "    Args:\n",
    "        query: La pregunta o consulta del usuario.\n",
    "        n_results: Numero de chunks a recuperar.\n",
    "        domain_filter: Filtrar por dominio ('productos' o 'tecnica').\n",
    "                       None = buscar en todos.\n",
    "    \n",
    "    Returns:\n",
    "        Dict con keys: 'documents', 'metadatas', 'distances', 'ids'\n",
    "    \"\"\"\n",
    "    # Construir filtro de metadata (si aplica)\n",
    "    where_filter = None\n",
    "    if domain_filter:\n",
    "        where_filter = {\"domain\": domain_filter}\n",
    "    \n",
    "    # Generar embedding del query\n",
    "    query_embedding = crear_embedding(query)\n",
    "    \n",
    "    # Buscar en ChromaDB\n",
    "    resultados = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        where=where_filter,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"documents\": resultados[\"documents\"][0],\n",
    "        \"metadatas\": resultados[\"metadatas\"][0],\n",
    "        \"distances\": resultados[\"distances\"][0],\n",
    "        \"ids\": resultados[\"ids\"][0]\n",
    "    }\n",
    "\n",
    "\n",
    "def mostrar_resultados_retrieval(query: str, resultados: dict) -> None:\n",
    "    \"\"\"Helper para imprimir resultados de retrieval de forma legible.\"\"\"\n",
    "    print(f\"\\nQUERY: \\\"{query}\\\"\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, (doc, meta, dist) in enumerate(zip(\n",
    "        resultados[\"documents\"],\n",
    "        resultados[\"metadatas\"],\n",
    "        resultados[\"distances\"]\n",
    "    )):\n",
    "        similitud = 1 - dist  # Convertir distancia coseno a similitud\n",
    "        print(f\"\\n--- Resultado #{i+1} (similitud: {similitud:.4f}) ---\")\n",
    "        print(f\"    Fuente: {meta['source']} | Dominio: {meta['domain']} | Chunk ID: {meta['chunk_id']}\")\n",
    "        print(f\"    Texto ({len(doc)} chars):\")\n",
    "        # Mostrar texto con wrapping para legibilidad\n",
    "        for linea in textwrap.wrap(doc, width=80):\n",
    "            print(f\"      {linea}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "print(\"Funcion de retrieval definida. Probemos con 3 consultas diversas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Probar retrieval con 3 consultas diversas ===\n",
    "\n",
    "consultas_test = [\n",
    "    (\"cuanto cuesta Analytics Pro?\", None),\n",
    "    (\"como hago rollback de un deploy?\", None),\n",
    "    (\"que planes tiene el AI Assistant?\", None),\n",
    "]\n",
    "\n",
    "for query, domain in consultas_test:\n",
    "    resultados = retrieve(query, n_results=3, domain_filter=domain)\n",
    "    mostrar_resultados_retrieval(query, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ejemplo de retrieval con filtro por dominio ===\n",
    "\n",
    "print(\"\\n\" + \"*\" * 70)\n",
    "print(\"EJEMPLO: Misma consulta, filtrada por dominio\")\n",
    "print(\"*\" * 70)\n",
    "\n",
    "query_filtro = \"como se manejan las actualizaciones?\"\n",
    "\n",
    "# Sin filtro\n",
    "print(\"\\n[SIN FILTRO]\")\n",
    "res_sin_filtro = retrieve(query_filtro, n_results=2)\n",
    "for meta in res_sin_filtro[\"metadatas\"]:\n",
    "    print(f\"  -> Dominio: {meta['domain']} | Fuente: {meta['source']}\")\n",
    "\n",
    "# Solo tecnica\n",
    "print(\"\\n[FILTRO: domain='tecnica']\")\n",
    "res_tecnica = retrieve(query_filtro, n_results=2, domain_filter=\"tecnica\")\n",
    "for meta in res_tecnica[\"metadatas\"]:\n",
    "    print(f\"  -> Dominio: {meta['domain']} | Fuente: {meta['source']}\")\n",
    "\n",
    "print(\"\\n>>> Los filtros por metadata son cruciales cuando tienes multiples dominios.\")\n",
    "print(\">>> Permiten dirigir la busqueda al contexto correcto y reducir ruido.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Paso 5: Generacion con Contexto (RAG Generation)\n",
    "\n",
    "Ahora conectamos el retrieval con un LLM. La idea es simple pero poderosa:\n",
    "\n",
    "1. Recuperamos los chunks relevantes (Paso 4)\n",
    "2. Los inyectamos como **contexto** en el prompt del LLM\n",
    "3. Le indicamos al LLM que responda **unicamente** basandose en ese contexto\n",
    "\n",
    "### El system prompt es critico\n",
    "\n",
    "Un buen system prompt para RAG debe:\n",
    "- Instruir al modelo a usar SOLO el contexto proporcionado\n",
    "- Pedir que cite las fuentes\n",
    "- Indicar que diga \"no tengo informacion\" si el contexto no es suficiente\n",
    "- Definir el tono y formato de la respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Paso 5: Generacion con contexto ===\n",
    "\n",
    "# Modelo de generacion\n",
    "GENERATION_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# System prompt para RAG - este es uno de los elementos mas importantes\n",
    "RAG_SYSTEM_PROMPT = \"\"\"Eres un asistente experto de NovaTech Solutions. Tu trabajo es responder \n",
    "preguntas de forma precisa y util, basandote UNICAMENTE en el contexto proporcionado.\n",
    "\n",
    "REGLAS ESTRICTAS:\n",
    "1. Responde SOLO con informacion presente en el contexto. No inventes ni asumas.\n",
    "2. Si el contexto no contiene informacion suficiente para responder, di: \n",
    "   \"No tengo informacion suficiente en mi base de conocimiento para responder esta pregunta.\"\n",
    "3. Cita la fuente del contexto cuando sea posible (nombre del archivo fuente).\n",
    "4. Se conciso pero completo. Usa listas cuando sea apropiado.\n",
    "5. Si la pregunta es ambigua, indica las posibles interpretaciones.\n",
    "\n",
    "FORMATO DE RESPUESTA:\n",
    "- Responde en espanol\n",
    "- Usa markdown para formato\n",
    "- Al final, indica las fuentes usadas\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_answer(query: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera una respuesta usando el LLM con el contexto proporcionado.\n",
    "    \n",
    "    Args:\n",
    "        query: La pregunta del usuario.\n",
    "        context: El contexto recuperado del vector store (chunks concatenados).\n",
    "    \n",
    "    Returns:\n",
    "        La respuesta generada por el LLM.\n",
    "    \"\"\"\n",
    "    # Construir el user prompt con el contexto\n",
    "    user_prompt = f\"\"\"CONTEXTO (informacion de la base de conocimiento):\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "PREGUNTA DEL USUARIO:\n",
    "{query}\n",
    "\n",
    "Responde basandote unicamente en el contexto proporcionado arriba.\"\"\"\n",
    "    \n",
    "    respuesta = client.chat.completions.create(\n",
    "        model=GENERATION_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.1,  # Baja temperatura para respuestas mas factuales\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    \n",
    "    return respuesta.choices[0].message.content\n",
    "\n",
    "\n",
    "print(\"Funcion de generacion definida.\")\n",
    "print(f\"Modelo: {GENERATION_MODEL}\")\n",
    "print(f\"Temperature: 0.1 (baja, para respuestas factuales)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Probar generacion con las mismas 3 consultas ===\n",
    "\n",
    "consultas_generacion = [\n",
    "    \"cuanto cuesta Analytics Pro?\",\n",
    "    \"como hago rollback de un deploy?\",\n",
    "    \"que planes tiene el AI Assistant?\"\n",
    "]\n",
    "\n",
    "for query in consultas_generacion:\n",
    "    print(f\"\\n{'#' * 70}\")\n",
    "    print(f\"QUERY: \\\"{query}\\\"\")\n",
    "    print(f\"{'#' * 70}\")\n",
    "    \n",
    "    # 1. Retrieval\n",
    "    resultados = retrieve(query, n_results=3)\n",
    "    \n",
    "    # 2. Construir contexto concatenando chunks recuperados\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[Fuente: {meta['source']}]\\n{doc}\" \n",
    "        for doc, meta in zip(resultados[\"documents\"], resultados[\"metadatas\"])\n",
    "    ])\n",
    "    \n",
    "    # 3. Mostrar el prompt completo (transparencia)\n",
    "    print(f\"\\n--- CONTEXTO ENVIADO AL LLM ({len(context)} chars) ---\")\n",
    "    print(context[:500] + \"...\" if len(context) > 500 else context)\n",
    "    \n",
    "    # 4. Generar respuesta\n",
    "    respuesta = generate_answer(query, context)\n",
    "    \n",
    "    print(f\"\\n--- RESPUESTA DEL LLM ---\")\n",
    "    print(respuesta)\n",
    "    print(f\"{'#' * 70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pipeline Completo: Encadenando Todo\n",
    "\n",
    "Ahora encapsulamos todo el flujo (retrieve + generate) en una sola funcion que representa nuestro **pipeline RAG completo**.\n",
    "\n",
    "Esta es la funcion que expondrias como API en produccion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pipeline RAG Completo ===\n",
    "\n",
    "def rag_pipeline(\n",
    "    query: str,\n",
    "    n_results: int = 3,\n",
    "    domain_filter: Optional[str] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Pipeline RAG completo: Retrieval + Generation.\n",
    "    \n",
    "    Flujo:\n",
    "        1. Recibe query del usuario\n",
    "        2. Recupera los chunks mas relevantes del vector store\n",
    "        3. Construye contexto con los chunks y sus fuentes\n",
    "        4. Genera respuesta con el LLM usando el contexto\n",
    "        5. Retorna respuesta estructurada con trazabilidad\n",
    "    \n",
    "    Args:\n",
    "        query: Pregunta del usuario.\n",
    "        n_results: Numero de chunks a recuperar.\n",
    "        domain_filter: Filtrar por dominio (opcional).\n",
    "    \n",
    "    Returns:\n",
    "        Diccionario con: query, retrieved_chunks, sources, answer, model.\n",
    "    \"\"\"\n",
    "    # --- Paso 1: Retrieval ---\n",
    "    resultados_retrieval = retrieve(\n",
    "        query=query,\n",
    "        n_results=n_results,\n",
    "        domain_filter=domain_filter\n",
    "    )\n",
    "    \n",
    "    # --- Paso 2: Construir contexto ---\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[Fuente: {meta['source']}]\\n{doc}\"\n",
    "        for doc, meta in zip(\n",
    "            resultados_retrieval[\"documents\"],\n",
    "            resultados_retrieval[\"metadatas\"]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # --- Paso 3: Generar respuesta ---\n",
    "    answer = generate_answer(query=query, context=context)\n",
    "    \n",
    "    # --- Paso 4: Construir respuesta estructurada ---\n",
    "    # Extraer fuentes unicas\n",
    "    sources = list(set(\n",
    "        meta[\"source\"] for meta in resultados_retrieval[\"metadatas\"]\n",
    "    ))\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_chunks\": [\n",
    "            {\n",
    "                \"text\": doc[:200] + \"...\" if len(doc) > 200 else doc,\n",
    "                \"source\": meta[\"source\"],\n",
    "                \"domain\": meta[\"domain\"],\n",
    "                \"similarity\": round(1 - dist, 4)\n",
    "            }\n",
    "            for doc, meta, dist in zip(\n",
    "                resultados_retrieval[\"documents\"],\n",
    "                resultados_retrieval[\"metadatas\"],\n",
    "                resultados_retrieval[\"distances\"]\n",
    "            )\n",
    "        ],\n",
    "        \"sources\": sources,\n",
    "        \"answer\": answer,\n",
    "        \"model\": GENERATION_MODEL\n",
    "    }\n",
    "\n",
    "\n",
    "def pretty_print_resultado(resultado: dict) -> None:\n",
    "    \"\"\"Imprime el resultado del pipeline de forma legible.\"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"PREGUNTA: {resultado['query']}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    \n",
    "    print(f\"\\nChunks recuperados ({len(resultado['retrieved_chunks'])}):\")\n",
    "    for i, chunk in enumerate(resultado['retrieved_chunks']):\n",
    "        print(f\"  [{i+1}] Similitud: {chunk['similarity']:.4f} | \"\n",
    "              f\"Dominio: {chunk['domain']} | Fuente: {chunk['source']}\")\n",
    "    \n",
    "    print(f\"\\nFuentes consultadas: {', '.join(resultado['sources'])}\")\n",
    "    print(f\"Modelo: {resultado['model']}\")\n",
    "    \n",
    "    print(f\"\\n--- RESPUESTA ---\")\n",
    "    print(resultado['answer'])\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "\n",
    "print(\"Pipeline RAG completo definido. Listo para pruebas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Probar el pipeline completo con 5 consultas diversas ===\n",
    "\n",
    "consultas_pipeline: list[str] = [\n",
    "    \"cuanto cuesta Analytics Pro?\",\n",
    "    \"como hago rollback de un deploy?\",\n",
    "    \"que planes tiene el AI Assistant?\",\n",
    "    \"cuales son los tiempos de respuesta de soporte?\",\n",
    "    \"que hago si me sale el error 'Pipeline timeout after 300s'?\"\n",
    "]\n",
    "\n",
    "# Almacenar resultados para uso posterior\n",
    "resultados_pipeline: list[dict] = []\n",
    "\n",
    "for query in consultas_pipeline:\n",
    "    resultado = rag_pipeline(query)\n",
    "    resultados_pipeline.append(resultado)\n",
    "    pretty_print_resultado(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluacion Basica de Calidad\n",
    "\n",
    "Un pipeline RAG no esta completo sin **evaluacion**. En produccion necesitas medir sistematicamente la calidad.\n",
    "\n",
    "### Dimensiones clave de evaluacion\n",
    "\n",
    "| Dimension | Pregunta | Que mide |\n",
    "|:----------|:---------|:---------|\n",
    "| **Faithfulness** | La respuesta es fiel al contexto recuperado? | Detecta alucinaciones |\n",
    "| **Relevance** | La respuesta es relevante a la pregunta? | Detecta divagaciones |\n",
    "| **Groundedness** | La respuesta cita sus fuentes? | Detecta afirmaciones sin respaldo |\n",
    "| **Coverage** | La respuesta cubre toda la informacion del contexto? | Detecta omisiones |\n",
    "\n",
    "### Evaluacion automatica vs manual\n",
    "- **Manual**: Humanos evaluan las respuestas (gold standard, pero costoso)\n",
    "- **Automatica**: Otro LLM evalua las respuestas (escalable, pero imperfecto)\n",
    "- **Hibrida**: LLM pre-filtra, humanos revisan casos dudosos\n",
    "\n",
    "Aqui implementaremos una evaluacion basica: verificamos si la respuesta cita fuentes y si responde correctamente ante preguntas fuera de alcance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluacion basica de calidad ===\n",
    "\n",
    "def evaluar_respuesta(resultado: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluacion basica de una respuesta del pipeline RAG.\n",
    "    \n",
    "    Verifica:\n",
    "    - Si la respuesta menciona fuentes\n",
    "    - Si la respuesta tiene contenido sustancial\n",
    "    - Si la respuesta admite limitaciones cuando corresponde\n",
    "    \n",
    "    Args:\n",
    "        resultado: Salida del rag_pipeline().\n",
    "    \n",
    "    Returns:\n",
    "        Dict con metricas de evaluacion.\n",
    "    \"\"\"\n",
    "    answer = resultado[\"answer\"].lower()\n",
    "    \n",
    "    # Verificar si cita fuentes\n",
    "    cita_fuentes = any(\n",
    "        indicador in answer\n",
    "        for indicador in [\"fuente\", \"base_conocimiento\", \"segun\", \"de acuerdo\", \".md\"]\n",
    "    )\n",
    "    \n",
    "    # Verificar si tiene contenido sustancial (no solo \"no se\")\n",
    "    es_sustancial = len(answer) > 50\n",
    "    \n",
    "    # Verificar si reconoce limitaciones cuando no tiene info\n",
    "    reconoce_limites = any(\n",
    "        frase in answer\n",
    "        for frase in [\"no tengo informacion\", \"no dispongo\", \"no encuentro\", \n",
    "                      \"no cuento con\", \"fuera de mi\", \"no puedo responder\"]\n",
    "    )\n",
    "    \n",
    "    # Calcular similitud promedio de los chunks recuperados\n",
    "    avg_similarity = sum(\n",
    "        c[\"similarity\"] for c in resultado[\"retrieved_chunks\"]\n",
    "    ) / len(resultado[\"retrieved_chunks\"])\n",
    "    \n",
    "    return {\n",
    "        \"query\": resultado[\"query\"],\n",
    "        \"cita_fuentes\": cita_fuentes,\n",
    "        \"es_sustancial\": es_sustancial,\n",
    "        \"reconoce_limites\": reconoce_limites,\n",
    "        \"avg_retrieval_similarity\": round(avg_similarity, 4),\n",
    "        \"answer_length\": len(resultado[\"answer\"])\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Test 1: Consulta donde el contexto ES relevante ---\n",
    "print(\"TEST 1: Consulta con contexto relevante\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resultado_relevante = rag_pipeline(\"cuales son los planes y precios de DataSync?\")\n",
    "eval_relevante = evaluar_respuesta(resultado_relevante)\n",
    "\n",
    "print(f\"Query: {eval_relevante['query']}\")\n",
    "print(f\"Cita fuentes: {eval_relevante['cita_fuentes']}\")\n",
    "print(f\"Es sustancial: {eval_relevante['es_sustancial']}\")\n",
    "print(f\"Similitud promedio retrieval: {eval_relevante['avg_retrieval_similarity']}\")\n",
    "print(f\"Longitud respuesta: {eval_relevante['answer_length']} chars\")\n",
    "print(f\"\\nRespuesta:\\n{resultado_relevante['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test 2: Consulta donde el contexto NO es relevante ---\n",
    "print(\"\\nTEST 2: Consulta FUERA del alcance de la base de conocimiento\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resultado_fuera = rag_pipeline(\"cual es la capital de Francia?\")\n",
    "eval_fuera = evaluar_respuesta(resultado_fuera)\n",
    "\n",
    "print(f\"Query: {eval_fuera['query']}\")\n",
    "print(f\"Cita fuentes: {eval_fuera['cita_fuentes']}\")\n",
    "print(f\"Reconoce limites: {eval_fuera['reconoce_limites']}\")\n",
    "print(f\"Similitud promedio retrieval: {eval_fuera['avg_retrieval_similarity']}\")\n",
    "print(f\"Longitud respuesta: {eval_fuera['answer_length']} chars\")\n",
    "print(f\"\\nRespuesta:\\n{resultado_fuera['answer']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALISIS:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"- Un buen sistema RAG debe RECHAZAR preguntas fuera de alcance.\")\n",
    "print(\"- Observa la similitud promedio del retrieval: si es baja, el contexto\")\n",
    "print(\"  probablemente no es relevante y el LLM deberia indicarlo.\")\n",
    "print(\"- En produccion, podemos agregar un THRESHOLD de similitud minima\")\n",
    "print(\"  para decidir si el contexto recuperado es suficientemente relevante.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## RAG Naive vs RAG con Contexto\n",
    "\n",
    "Esta es la comparacion mas importante del notebook. Vamos a enviar la **misma pregunta** de dos formas:\n",
    "\n",
    "1. **Directo al LLM** (sin contexto, sin RAG) -- el LLM solo puede usar su conocimiento parametrico\n",
    "2. **A traves del pipeline RAG** -- el LLM tiene acceso a nuestra base de conocimiento\n",
    "\n",
    "Esto demuestra el valor fundamental de RAG: **respuestas fundamentadas vs respuestas potencialmente alucinadas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Comparacion: LLM Naive vs RAG Pipeline ===\n",
    "\n",
    "def respuesta_naive(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Envia la pregunta directamente al LLM sin ningun contexto.\n",
    "    Simula lo que pasa cuando NO usas RAG.\n",
    "    \"\"\"\n",
    "    respuesta = client.chat.completions.create(\n",
    "        model=GENERATION_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Eres un asistente de NovaTech Solutions. Responde las preguntas del usuario.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return respuesta.choices[0].message.content\n",
    "\n",
    "\n",
    "# --- Comparacion lado a lado ---\n",
    "query_comparacion = \"cuales son los planes y precios de NovaTech AI Assistant y que integraciones soporta?\"\n",
    "\n",
    "print(f\"QUERY: \\\"{query_comparacion}\\\"\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Respuesta Naive (sin RAG)\n",
    "print(\"\\n[1] RESPUESTA NAIVE (LLM sin contexto)\")\n",
    "print(\"-\" * 50)\n",
    "resp_naive = respuesta_naive(query_comparacion)\n",
    "print(resp_naive)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Respuesta RAG\n",
    "print(\"\\n[2] RESPUESTA RAG (LLM + contexto recuperado)\")\n",
    "print(\"-\" * 50)\n",
    "resultado_rag = rag_pipeline(query_comparacion)\n",
    "print(resultado_rag[\"answer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nDIFERENCIAS CLAVE:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"[Naive] El LLM INVENTA datos que no conoce (hallucination).\")\n",
    "print(\"        Los precios, features y detalles pueden ser incorrectos.\")\n",
    "print(\"        No hay forma de verificar la informacion.\")\n",
    "print(\"\")\n",
    "print(\"[RAG]   El LLM responde con datos REALES de la base de conocimiento.\")\n",
    "print(\"        Los precios y features son verificables contra la fuente.\")\n",
    "print(\"        La respuesta esta fundamentada y cita sus fuentes.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualizacion: Flujo del Pipeline\n",
    "\n",
    "Creemos una representacion visual del pipeline para consolidar lo aprendido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Visualizacion del Pipeline RAG ===\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.set_xlim(0, 16)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Pipeline RAG Completo - NovaTech Knowledge Base\", \n",
    "             fontsize=18, fontweight=\"bold\", pad=20)\n",
    "\n",
    "# --- Fase de Indexacion (arriba) ---\n",
    "ax.text(8, 9.3, \"FASE DE INDEXACION (offline)\", ha=\"center\", fontsize=13, \n",
    "        fontweight=\"bold\", color=\"#1a5276\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"#d4e6f1\", edgecolor=\"#1a5276\"))\n",
    "\n",
    "# Cajas de la fase de indexacion\n",
    "index_steps = [\n",
    "    (1.5, 7.5, \"Documentos\\n(.md files)\", \"#f9e79f\"),\n",
    "    (5.0, 7.5, \"Chunking\\n(Recursive)\", \"#abebc6\"),\n",
    "    (8.5, 7.5, \"Embeddings\\n(OpenAI)\", \"#aed6f1\"),\n",
    "    (12.0, 7.5, \"Vector Store\\n(ChromaDB)\", \"#d7bde2\"),\n",
    "]\n",
    "\n",
    "for x, y, label, color in index_steps:\n",
    "    rect = mpatches.FancyBboxPatch((x - 1.2, y - 0.6), 2.4, 1.2,\n",
    "                                    boxstyle=\"round,pad=0.15\",\n",
    "                                    facecolor=color, edgecolor=\"black\", linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, label, ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Flechas de indexacion\n",
    "for i in range(len(index_steps) - 1):\n",
    "    x_start = index_steps[i][0] + 1.3\n",
    "    x_end = index_steps[i + 1][0] - 1.3\n",
    "    ax.annotate(\"\", xy=(x_end, 7.5), xytext=(x_start, 7.5),\n",
    "                arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"#2c3e50\"))\n",
    "\n",
    "# --- Fase de Consulta (abajo) ---\n",
    "ax.text(8, 5.3, \"FASE DE CONSULTA (online, por cada query)\", ha=\"center\", fontsize=13, \n",
    "        fontweight=\"bold\", color=\"#7b241c\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"#fadbd8\", edgecolor=\"#7b241c\"))\n",
    "\n",
    "# Cajas de la fase de consulta\n",
    "query_steps = [\n",
    "    (1.5, 3.5, \"Query\\n(usuario)\", \"#fad7a0\"),\n",
    "    (5.0, 3.5, \"Retrieval\\n(top-k)\", \"#a9dfbf\"),\n",
    "    (8.5, 3.5, \"Context\\nAssembly\", \"#a9cce3\"),\n",
    "    (12.0, 3.5, \"LLM\\n(GPT-4o-mini)\", \"#d2b4de\"),\n",
    "]\n",
    "\n",
    "for x, y, label, color in query_steps:\n",
    "    rect = mpatches.FancyBboxPatch((x - 1.2, y - 0.6), 2.4, 1.2,\n",
    "                                    boxstyle=\"round,pad=0.15\",\n",
    "                                    facecolor=color, edgecolor=\"black\", linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, label, ha=\"center\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "# Flechas de consulta\n",
    "for i in range(len(query_steps) - 1):\n",
    "    x_start = query_steps[i][0] + 1.3\n",
    "    x_end = query_steps[i + 1][0] - 1.3\n",
    "    ax.annotate(\"\", xy=(x_end, 3.5), xytext=(x_start, 3.5),\n",
    "                arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"#7b241c\"))\n",
    "\n",
    "# Flecha de Vector Store a Retrieval (conexion entre fases)\n",
    "ax.annotate(\"\", xy=(5.0, 4.2), xytext=(12.0, 6.8),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"#8e44ad\", linestyle=\"dashed\"))\n",
    "ax.text(8.8, 5.8, \"busqueda\\nsimilitud\", ha=\"center\", fontsize=9, \n",
    "        fontstyle=\"italic\", color=\"#8e44ad\")\n",
    "\n",
    "# Caja de respuesta final\n",
    "rect_resp = mpatches.FancyBboxPatch((10.8, 1.2), 2.4, 1.0,\n",
    "                                     boxstyle=\"round,pad=0.15\",\n",
    "                                     facecolor=\"#82e0aa\", edgecolor=\"#1e8449\", linewidth=2)\n",
    "ax.add_patch(rect_resp)\n",
    "ax.text(12.0, 1.7, \"Respuesta\\nFundamentada\", ha=\"center\", va=\"center\", \n",
    "        fontsize=10, fontweight=\"bold\", color=\"#1e8449\")\n",
    "\n",
    "# Flecha de LLM a respuesta\n",
    "ax.annotate(\"\", xy=(12.0, 2.2), xytext=(12.0, 2.8),\n",
    "            arrowprops=dict(arrowstyle=\"->\", lw=2, color=\"#1e8449\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEl diagrama muestra las dos fases del pipeline RAG:\")\n",
    "print(\"  1. INDEXACION (arriba): Se ejecuta una vez al cargar documentos.\")\n",
    "print(\"  2. CONSULTA (abajo): Se ejecuta por cada pregunta del usuario.\")\n",
    "print(\"  La linea punteada conecta el Vector Store con el Retrieval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Trade-offs y Consideraciones de Produccion\n",
    "\n",
    "### 1. Analisis de Costos\n",
    "\n",
    "| Operacion | Modelo | Costo aproximado | Cuando se ejecuta |\n",
    "|:----------|:-------|:-----------------|:-------------------|\n",
    "| Embedding de chunks | `text-embedding-3-small` | ~$0.02 / 1M tokens | Una vez (indexacion) |\n",
    "| Embedding del query | `text-embedding-3-small` | ~$0.02 / 1M tokens | Cada consulta |\n",
    "| Generacion (input) | `gpt-4o-mini` | ~$0.15 / 1M tokens | Cada consulta |\n",
    "| Generacion (output) | `gpt-4o-mini` | ~$0.60 / 1M tokens | Cada consulta |\n",
    "\n",
    "> **Conclusion**: Los embeddings son extremadamente baratos. El costo principal esta en la **generacion**, especialmente los tokens de output. Optimizar el tamano del contexto reduce costos significativamente.\n",
    "\n",
    "### 2. Latencia por Paso\n",
    "\n",
    "| Paso | Latencia tipica | Optimizacion |\n",
    "|:-----|:----------------|:-------------|\n",
    "| Embedding del query | 50-100ms | Batch si hay multiples queries |\n",
    "| Busqueda en vector store | 5-50ms | Indices HNSW, filtros pre-query |\n",
    "| Generacion LLM | 500ms-3s | Streaming, modelos mas pequenos |\n",
    "| **Total** | **~600ms - 3.5s** | |\n",
    "\n",
    "> La generacion domina la latencia. Usa **streaming** para mejorar la percepcion del usuario.\n",
    "\n",
    "### 3. Cuando cachear embeddings\n",
    "\n",
    "- **Siempre** para los documentos fuente (no cambian frecuentemente)\n",
    "- **Considerar** para queries frecuentes (cache LRU por query normalizado)\n",
    "- **No cachear** el output del LLM si la temperatura > 0 (respuestas no deterministas)\n",
    "\n",
    "### 4. Cuando la estrategia de chunking importa mas\n",
    "\n",
    "- **Documentos largos y heterogeneos**: El chunking es critico. Un mal chunk mezcla temas.\n",
    "- **Documentos cortos y homogeneos**: El chunking importa menos.\n",
    "- **Tablas y datos estructurados**: Requieren chunking especializado (no romper filas).\n",
    "- **Codigo fuente**: Usar tree-sitter o AST-aware chunking.\n",
    "- **Regla general**: Si tus resultados de retrieval mezclan temas no relacionados, tu chunking necesita mejoras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Errores Comunes en Pipelines RAG\n",
    "\n",
    "### 1. Chunks demasiado grandes o demasiado pequenos\n",
    "- **Demasiado grandes** (>1000 chars): Traen mucho ruido al contexto. El LLM se distrae con informacion irrelevante. Ademas, los embeddings de textos largos tienden a ser \"promedios\" poco informativos.\n",
    "- **Demasiado pequenos** (<100 chars): Pierden contexto necesario. Un chunk que dice \"$99/mes\" sin decir a que producto se refiere es inutil.\n",
    "- **Regla practica**: 200-600 caracteres es un buen rango inicial. Ajustar segun los resultados de evaluacion.\n",
    "\n",
    "### 2. No incluir metadata en los chunks\n",
    "- Sin metadata, no puedes filtrar por dominio, fecha, autor, etc.\n",
    "- En produccion, la metadata es lo que permite construir experiencias como \"buscar solo en documentacion tecnica\" o \"solo documentos del ultimo mes\".\n",
    "- **Siempre** almacena al menos: fuente, fecha de creacion, dominio/categoria.\n",
    "\n",
    "### 3. No evaluar la calidad del retrieval antes de optimizar la generacion\n",
    "- Si el retrieval trae chunks irrelevantes, no importa que tan bueno sea tu prompt o tu LLM.\n",
    "- **Primero** asegura que el retrieval funcione bien. **Despues** optimiza la generacion.\n",
    "- Usa metricas como Precision@K, Recall@K y MRR (Mean Reciprocal Rank).\n",
    "\n",
    "### 4. Ignorar el system prompt\n",
    "- Un system prompt debil permite que el LLM alucine incluso con buen contexto.\n",
    "- **Siempre** incluye instrucciones explicitas de: usar solo el contexto, citar fuentes, admitir cuando no sabe.\n",
    "- El system prompt es codigo: iteralo, versionalo y evalualo como cualquier otro componente.\n",
    "\n",
    "### 5. No manejar el caso \"fuera de alcance\"\n",
    "- Cuando un usuario pregunta algo que no esta en tu base de conocimiento, el sistema debe responder honestamente.\n",
    "- **Implementa un threshold de similitud**: Si el mejor chunk tiene similitud < 0.3, probablemente no hay informacion relevante.\n",
    "- **No fuerces respuestas**: Es mejor decir \"no se\" que inventar una respuesta incorrecta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checklist de Comprension\n",
    "\n",
    "Antes de continuar al siguiente notebook, asegurate de poder responder estas preguntas:\n",
    "\n",
    "- [ ] **1.** Por que el **Recursive Character Splitting** generalmente produce mejores resultados que el Fixed-Size Chunking para RAG? Que trade-off introduce?\n",
    "\n",
    "- [ ] **2.** Si aumentas el `chunk_size` de 500 a 2000 caracteres, que impacto esperarias en (a) la calidad del retrieval, (b) el costo de embeddings y (c) la calidad de la respuesta del LLM?\n",
    "\n",
    "- [ ] **3.** Explica por que el **system prompt** del LLM es tan importante en un pipeline RAG. Que pasa si no le dices \"responde solo con el contexto\"?\n",
    "\n",
    "- [ ] **4.** En la comparacion RAG vs Naive, por que el LLM \"naive\" puede dar respuestas que *parecen* correctas pero son alucinaciones? Como verificarias esto en produccion?\n",
    "\n",
    "- [ ] **5.** Disena una estrategia para manejar el caso donde el usuario pregunta algo que NO esta en la base de conocimiento. Que componentes del pipeline modificarias?\n",
    "\n",
    "---\n",
    "\n",
    "### Siguiente paso\n",
    "\n",
    "En el **Notebook 03** exploraremos tecnicas avanzadas de RAG:\n",
    "- Hybrid search (keyword + semantic)\n",
    "- Re-ranking de resultados\n",
    "- Multi-query retrieval\n",
    "- Evaluacion sistematica con RAGAS framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}