{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: RAG con Prompt Chaining — Agente que Busca, Evalua y Corrige\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook seras capaz de:\n",
    "\n",
    "1. **Entender por que RAG naive no es suficiente** y como prompt chaining agrega control de calidad en cada etapa del pipeline.\n",
    "2. **Implementar un sistema Self-RAG** con autocorreccion: el agente evalua sus propios resultados y decide si necesita re-intentar.\n",
    "3. **Usar LangGraph para orquestar la cadena** de pasos, incluyendo edges condicionales que crean loops de correccion.\n",
    "4. **Diagnosticar y trazar** el flujo completo: saber exactamente por que el agente tomo cada decision.\n",
    "\n",
    "### Prerequisitos\n",
    "\n",
    "Este notebook asume que completaste:\n",
    "- **Notebook 01**: Bases de datos vectoriales, embeddings, ChromaDB.\n",
    "- **Notebook 02**: Pipeline RAG completo (indexacion, retrieval, generacion).\n",
    "\n",
    "Aqui vamos un paso mas alla: no solo generamos respuestas, sino que las **evaluamos y corregimos** automaticamente.\n",
    "\n",
    "### Referencia\n",
    "\n",
    "- Asai, A. et al. (2023). *Self-RAG: Learning to Retrieve, Generate, and Critique*. arXiv:2310.11511.\n",
    "- Chip Huyen (2024). *AI Engineering*, Cap. 10: RAG patterns in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Por que Prompt Chaining para RAG?\n",
    "\n",
    "### El problema del RAG naive\n",
    "\n",
    "En el Notebook 02 construimos un pipeline RAG funcional:\n",
    "\n",
    "```\n",
    "Query → Retrieval → Generacion → Respuesta\n",
    "```\n",
    "\n",
    "Este pipeline funciona, pero tiene **cero control de calidad**:\n",
    "\n",
    "- Si el retriever trae documentos irrelevantes, el LLM genera basura.\n",
    "- Si el LLM alucina a pesar de tener buen contexto, nadie lo detecta.\n",
    "- Si el query es ambiguo, nadie lo refina antes de buscar.\n",
    "\n",
    "En produccion, esto es inaceptable. Un chatbot de soporte que da informacion incorrecta sobre precios o politicas puede costarle dinero real a la empresa.\n",
    "\n",
    "### La solucion: Prompt Chaining con validacion\n",
    "\n",
    "Prompt chaining descompone el pipeline en pasos discretos, donde **cada paso valida y potencialmente corrige** al anterior:\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌───────────┐     ┌──────────────┐     ┌───────────┐     ┌──────────────┐     ┌──────────┐\n",
    "│   Analisis   │────→│ Retrieval │────→│   Evaluar    │────→│  Generar  │────→│   Evaluar    │────→│  Output  │\n",
    "│  del Query   │     │           │     │  Contexto    │     │ Respuesta │     │  Respuesta   │     │  Final   │\n",
    "└──────────────┘     └───────────┘     └──────┬───────┘     └───────────┘     └──────┬───────┘     └──────────┘\n",
    "       ^                                      │                                      │\n",
    "       │                                      │                                      │\n",
    "       └──── Contexto irrelevante? ───────────┘                                      │\n",
    "                  (re-query)                                                         │\n",
    "                                                          Alucinacion? ──────────────┘\n",
    "                                                           (re-generate)\n",
    "```\n",
    "\n",
    "### Que hace cada paso\n",
    "\n",
    "| Paso | Funcion | Que valida |\n",
    "|------|---------|------------|\n",
    "| **Query Analysis** | Refina el query del usuario para mejorar retrieval | Claridad y especificidad del query |\n",
    "| **Retrieval** | Busca documentos relevantes en la base vectorial | - |\n",
    "| **Grade Context** | Evalua si los documentos recuperados son relevantes | Relevancia del contexto vs. query |\n",
    "| **Generate** | Genera respuesta basada SOLO en el contexto | - |\n",
    "| **Grade Answer** | Evalua si la respuesta es fiel al contexto (no alucina) | Faithfulness de la respuesta |\n",
    "\n",
    "### Cuando usar Prompt Chaining para RAG\n",
    "\n",
    "**SI usalo cuando:**\n",
    "- El costo de una respuesta incorrecta es alto (soporte al cliente, asesor legal, chatbot medico)\n",
    "- Necesitas trazabilidad completa (por que el agente respondio X)\n",
    "- La base de conocimiento es heterogenea y el retrieval puede fallar\n",
    "\n",
    "**NO lo uses cuando:**\n",
    "- La latencia debe ser sub-segundo (chaining agrega 3-8 segundos)\n",
    "- El presupuesto de tokens es extremadamente limitado (cada evaluacion consume tokens)\n",
    "- Las queries son simples y el retrieval es consistentemente bueno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Arquitectura del Agente\n",
    "\n",
    "Vamos a implementar esto como un **state machine** con LangGraph. Cada nodo es una funcion que transforma el estado, y los edges condicionales deciden el flujo.\n",
    "\n",
    "### State Machine completa\n",
    "\n",
    "```\n",
    "                    ┌───────────────────────────────────────────────────────────────┐\n",
    "                    │                   LangGraph State Machine                     │\n",
    "                    │                                                               │\n",
    "                    │   ┌──────────────┐                                            │\n",
    "                    │   │ analyze_query│◄───────────────────────────┐               │\n",
    "                    │   └──────┬───────┘                           │               │\n",
    "                    │          │                                    │               │\n",
    "                    │          ▼                                    │               │\n",
    "                    │   ┌──────────────┐                           │               │\n",
    "                    │   │   retrieve   │                           │               │\n",
    "                    │   └──────┬───────┘                           │               │\n",
    "                    │          │                                    │               │\n",
    "                    │          ▼                                    │               │\n",
    "                    │   ┌──────────────┐    irrelevant    ┌────────┴──────┐        │\n",
    "                    │   │ grade_context│──────────────────→│ should_regen  │        │\n",
    "                    │   └──────┬───────┘                  │ (routing fn)  │        │\n",
    "                    │          │ relevant                  └───────────────┘        │\n",
    "                    │          ▼                                                    │\n",
    "                    │   ┌──────────────┐                                            │\n",
    "                    │   │   generate   │                                            │\n",
    "                    │   └──────┬───────┘                                            │\n",
    "                    │          │                                                    │\n",
    "                    │          ▼                                                    │\n",
    "                    │   ┌──────────────┐   hallucination  ┌────────────────┐        │\n",
    "                    │   │ grade_answer │──────────────────→│ should_rewrite │        │\n",
    "                    │   └──────┬───────┘                  │ (routing fn)   │        │\n",
    "                    │          │ faithful                  └──────┬─────────┘        │\n",
    "                    │          ▼                                  │ (re-generate)    │\n",
    "                    │      ┌───────┐                             │                  │\n",
    "                    │      │  END  │◄────────────────────────────┘ (finish)         │\n",
    "                    │      └───────┘                                                │\n",
    "                    └───────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### El estado que fluye entre nodos\n",
    "\n",
    "Todos los nodos leen y escriben en un diccionario tipado (`RAGState`). Esto es clave: **cada nodo solo modifica su parte del estado** y el siguiente nodo puede leer todo lo que paso antes.\n",
    "\n",
    "Campos del estado:\n",
    "- `query`: el query original del usuario\n",
    "- `refined_query`: version mejorada del query (post-analisis)\n",
    "- `retrieved_docs`: documentos recuperados del vector store\n",
    "- `context_grade`: \"relevant\" o \"irrelevant\"\n",
    "- `generation`: la respuesta generada\n",
    "- `answer_grade`: \"faithful\" o \"hallucination\"\n",
    "- `sources`: fuentes citadas en la respuesta\n",
    "- `retry_count` / `max_retries`: control de loops infinitos\n",
    "- `steps_trace`: registro de cada paso ejecutado (para debugging)\n",
    "\n",
    "### Edges condicionales\n",
    "\n",
    "Los edges condicionales son **funciones de routing** que deciden el proximo nodo basandose en el estado:\n",
    "\n",
    "1. **Despues de `grade_context`**: Si el contexto es irrelevante y no hemos agotado reintentos → volver a `analyze_query`. Si es relevante → ir a `generate`.\n",
    "2. **Despues de `grade_answer`**: Si la respuesta alucina y no hemos agotado reintentos → volver a `generate`. Si es fiel → terminar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Imports necesarios\n",
    "# =============================================================================\n",
    "import os\n",
    "import json\n",
    "from typing import Literal\n",
    "from pathlib import Path\n",
    "\n",
    "# LangGraph - orquestacion del grafo\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# LangChain - LLM y text processing\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Pydantic - structured output para grading\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# TypedDict para el estado del grafo\n",
    "from typing import TypedDict\n",
    "\n",
    "# ChromaDB - base de datos vectorial\n",
    "import chromadb\n",
    "\n",
    "# OpenAI - para embeddings y llamadas directas\n",
    "from openai import OpenAI\n",
    "\n",
    "# Utilidades\n",
    "from IPython.display import display, Markdown, Image\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# =============================================================================\n",
    "# Cargar variables de entorno\n",
    "# =============================================================================\n",
    "load_dotenv()\n",
    "\n",
    "# Inicializar clientes\n",
    "openai_client = OpenAI()  # Usa OPENAI_API_KEY del .env\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "    temperature=0  # Determinismo para evaluaciones de calidad\n",
    ")\n",
    "\n",
    "# Verificar conexion\n",
    "print(\"=\" * 60)\n",
    "print(\"SETUP COMPLETADO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Modelo LLM: {llm.model_name}\")\n",
    "print(f\"Temperatura: {llm.temperature}\")\n",
    "print(f\"API Key configurada: {'SI' if os.getenv('OPENAI_API_KEY') else 'NO'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Preparar Base de Conocimiento\n",
    "\n",
    "Vamos a indexar los mismos documentos que usamos en los notebooks anteriores: la base de conocimiento de productos y la guia tecnica de NovaTech. Usamos `RecursiveCharacterTextSplitter` para hacer chunking respetando la estructura del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cargar documentos desde ../data/\n",
    "# =============================================================================\n",
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "# Leer ambos archivos .md\n",
    "archivos_md = list(DATA_DIR.glob(\"*.md\"))\n",
    "print(f\"Archivos encontrados: {len(archivos_md)}\")\n",
    "\n",
    "documentos_raw = []\n",
    "metadatos_raw = []\n",
    "\n",
    "for archivo in archivos_md:\n",
    "    contenido = archivo.read_text(encoding=\"utf-8\")\n",
    "    documentos_raw.append(contenido)\n",
    "    metadatos_raw.append({\"source\": archivo.name})\n",
    "    print(f\"  - {archivo.name}: {len(contenido):,} caracteres\")\n",
    "\n",
    "# =============================================================================\n",
    "# Chunking con RecursiveCharacterTextSplitter\n",
    "# =============================================================================\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,       # Tamano maximo por chunk\n",
    "    chunk_overlap=50,     # Overlap para no perder contexto entre chunks\n",
    "    separators=[\"\\n## \", \"\\n### \", \"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")\n",
    "\n",
    "# Procesar cada documento\n",
    "all_chunks = []       # Textos de los chunks\n",
    "all_metadatas = []    # Metadata por chunk (source + chunk_index)\n",
    "all_ids = []          # IDs unicos\n",
    "\n",
    "chunk_counter = 0\n",
    "for doc_text, doc_meta in zip(documentos_raw, metadatos_raw):\n",
    "    chunks = splitter.split_text(doc_text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append(chunk)\n",
    "        all_metadatas.append({\n",
    "            \"source\": doc_meta[\"source\"],\n",
    "            \"chunk_index\": i,\n",
    "        })\n",
    "        all_ids.append(f\"chunk_{chunk_counter}\")\n",
    "        chunk_counter += 1\n",
    "\n",
    "print(f\"\\nTotal de chunks generados: {len(all_chunks)}\")\n",
    "print(f\"Tamano promedio por chunk: {sum(len(c) for c in all_chunks) // len(all_chunks)} caracteres\")\n",
    "\n",
    "# =============================================================================\n",
    "# Indexar en ChromaDB\n",
    "# =============================================================================\n",
    "chroma_client = chromadb.Client()  # Cliente en memoria (para desarrollo)\n",
    "\n",
    "# Eliminar coleccion si existe (para re-ejecucion limpia)\n",
    "try:\n",
    "    chroma_client.delete_collection(name=\"novatech_kb\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = chroma_client.create_collection(\n",
    "    name=\"novatech_kb\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Similitud coseno\n",
    ")\n",
    "\n",
    "# Agregar todos los chunks a la coleccion\n",
    "collection.add(\n",
    "    documents=all_chunks,\n",
    "    metadatas=all_metadatas,\n",
    "    ids=all_ids,\n",
    ")\n",
    "\n",
    "print(f\"\\nColeccion '{collection.name}' creada con {collection.count()} documentos indexados.\")\n",
    "print(\"Base de conocimiento lista para retrieval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Definir Estado del Grafo\n",
    "\n",
    "En LangGraph, el estado es un `TypedDict` que actua como la \"memoria compartida\" entre todos los nodos. Cada nodo recibe el estado completo, lo modifica parcialmente, y lo devuelve.\n",
    "\n",
    "Piensa en el estado como una **ficha medica**: cada doctor (nodo) la lee, agrega sus notas, y la pasa al siguiente.\n",
    "\n",
    "```\n",
    "RAGState = {\n",
    "    query:          \"pregunta original del usuario\"\n",
    "    refined_query:  \"pregunta mejorada por el analizador\"\n",
    "    retrieved_docs: [\"doc1\", \"doc2\", ...]\n",
    "    context_grade:  \"relevant\" | \"irrelevant\"\n",
    "    generation:     \"respuesta generada por el LLM\"\n",
    "    answer_grade:   \"faithful\" | \"hallucination\"\n",
    "    sources:        [\"fuente1.md\", \"fuente2.md\"]\n",
    "    retry_count:    0, 1, 2, ...\n",
    "    max_retries:    3\n",
    "    steps_trace:    [\"analyze_query\", \"retrieve\", ...]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Definicion del estado del grafo (TypedDict)\n",
    "# =============================================================================\n",
    "\n",
    "class RAGState(TypedDict):\n",
    "    \"\"\"Estado compartido entre todos los nodos del grafo Self-RAG.\n",
    "    \n",
    "    Cada nodo lee el estado, lo modifica parcialmente, y lo retorna.\n",
    "    LangGraph se encarga de hacer merge automatico.\n",
    "    \"\"\"\n",
    "    query: str                    # Query original del usuario\n",
    "    refined_query: str            # Query mejorada (post-analysis)\n",
    "    retrieved_docs: list[str]     # Documentos recuperados del vector store\n",
    "    context_grade: str            # \"relevant\" | \"irrelevant\"\n",
    "    generation: str               # Respuesta generada por el LLM\n",
    "    answer_grade: str             # \"faithful\" | \"hallucination\"\n",
    "    sources: list[str]            # Fuentes citadas en la respuesta\n",
    "    retry_count: int              # Contador de reintentos realizados\n",
    "    max_retries: int              # Maximo de reintentos permitidos\n",
    "    steps_trace: list[str]        # Traza de pasos ejecutados (debugging)\n",
    "\n",
    "\n",
    "print(\"Estado RAGState definido con los siguientes campos:\")\n",
    "print(\"-\" * 50)\n",
    "for field_name, field_type in RAGState.__annotations__.items():\n",
    "    print(f\"  {field_name:20s} -> {field_type}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total de campos: {len(RAGState.__annotations__)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Nodo 1: Analisis del Query\n",
    "\n",
    "El primer nodo del pipeline analiza el query del usuario y lo refina para mejorar la calidad del retrieval. Esto es especialmente util cuando el usuario escribe queries vagos, ambiguos o con errores.\n",
    "\n",
    "**Ejemplos de refinamiento:**\n",
    "- `\"precios\"` → `\"planes y precios de los productos NovaTech Analytics Pro, DataSync y AI Assistant\"`\n",
    "- `\"no me funciona\"` → `\"problemas comunes y troubleshooting de los productos NovaTech\"`\n",
    "- `\"cuanto cuesta el plan pro?\"` → `\"precio del plan Professional de Analytics Pro\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Nodo 1: Analisis del Query\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_query(state: RAGState) -> RAGState:\n",
    "    \"\"\"Analiza el query del usuario y genera una version refinada\n",
    "    optimizada para retrieval en la base de conocimiento.\n",
    "    \n",
    "    - Si el query es claro y especifico, lo mantiene igual.\n",
    "    - Si es vago o ambiguo, lo expande con terminos relevantes.\n",
    "    - Registra el paso en la traza.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PASO: analyze_query\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    steps_trace = state.get(\"steps_trace\", []).copy()\n",
    "    \n",
    "    print(f\"Query original: '{query}'\")\n",
    "    print(f\"Intento numero: {retry_count + 1}\")\n",
    "    \n",
    "    # Prompt para el analizador de queries\n",
    "    system_prompt = \"\"\"Eres un analizador de queries para un sistema RAG.\n",
    "Tu base de conocimiento contiene informacion sobre productos de NovaTech Solutions:\n",
    "- Analytics Pro (plataforma de business intelligence)\n",
    "- DataSync (herramienta ETL)\n",
    "- AI Assistant (chatbot empresarial)\n",
    "- Guia tecnica (arquitectura, deploy, monitoreo, troubleshooting)\n",
    "\n",
    "Tu tarea: dado el query del usuario, genera una version REFINADA que sea mas especifica\n",
    "y optimizada para busqueda semantica en la base de conocimiento.\n",
    "\n",
    "Reglas:\n",
    "- Si el query ya es claro y especifico, devuelvelo TAL CUAL.\n",
    "- Si es vago, expandelo con terminos relevantes del dominio.\n",
    "- Si tiene errores de tipeo, corrigelos.\n",
    "- NO inventes informacion. Solo reformula.\n",
    "- Responde UNICAMENTE con el query refinado, sin explicaciones.\"\"\"\n",
    "    \n",
    "    # Si es un re-intento, agregar contexto adicional\n",
    "    user_msg = f\"Query del usuario: {query}\"\n",
    "    if retry_count > 0:\n",
    "        user_msg += f\"\\n\\nNOTA: Este es el intento #{retry_count + 1}. \"\n",
    "        user_msg += \"El retrieval anterior no encontro contexto relevante. \"\n",
    "        user_msg += \"Intenta una reformulacion MAS AMPLIA o con terminos alternativos.\"\n",
    "    \n",
    "    # Llamada al LLM\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_msg),\n",
    "    ])\n",
    "    \n",
    "    refined_query = response.content.strip()\n",
    "    steps_trace.append(f\"analyze_query (intento {retry_count + 1}): '{query}' -> '{refined_query}'\")\n",
    "    \n",
    "    print(f\"Query refinado: '{refined_query}'\")\n",
    "    print(f\"Cambio aplicado: {'SI' if refined_query != query else 'NO (query ya era claro)'}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"refined_query\": refined_query,\n",
    "        \"steps_trace\": steps_trace,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Funcion analyze_query definida.\")\n",
    "print(\"Este nodo recibe el query y genera una version refinada para mejorar el retrieval.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Nodo 2: Retrieval\n",
    "\n",
    "El nodo de retrieval busca en ChromaDB los documentos mas relevantes para el `refined_query`. Usa busqueda semantica (similitud coseno) y retorna los top-K resultados con sus metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Nodo 2: Retrieval\n",
    "# =============================================================================\n",
    "\n",
    "def retrieve(state: RAGState) -> RAGState:\n",
    "    \"\"\"Busca documentos relevantes en ChromaDB usando el query refinado.\n",
    "    \n",
    "    Retorna los top-5 documentos mas similares junto con sus fuentes.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PASO: retrieve\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    refined_query = state[\"refined_query\"]\n",
    "    steps_trace = state.get(\"steps_trace\", []).copy()\n",
    "    \n",
    "    print(f\"Buscando en ChromaDB con query: '{refined_query}'\")\n",
    "    \n",
    "    # Buscar los top-5 documentos mas relevantes\n",
    "    results = collection.query(\n",
    "        query_texts=[refined_query],\n",
    "        n_results=5,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "    \n",
    "    # Extraer documentos y fuentes\n",
    "    retrieved_docs = results[\"documents\"][0]   # Lista de textos\n",
    "    metadatas = results[\"metadatas\"][0]         # Lista de metadatos\n",
    "    distances = results[\"distances\"][0]         # Distancias coseno\n",
    "    \n",
    "    # Extraer fuentes unicas\n",
    "    sources = list(set(m[\"source\"] for m in metadatas))\n",
    "    \n",
    "    # Mostrar resultados del retrieval\n",
    "    print(f\"\\nDocumentos recuperados: {len(retrieved_docs)}\")\n",
    "    print(f\"Fuentes: {sources}\")\n",
    "    print(f\"\\nTop-3 resultados:\")\n",
    "    for i, (doc, meta, dist) in enumerate(zip(retrieved_docs[:3], metadatas[:3], distances[:3])):\n",
    "        print(f\"  [{i+1}] (distancia: {dist:.4f}) [{meta['source']}]\")\n",
    "        print(f\"      {doc[:120]}...\")\n",
    "    \n",
    "    steps_trace.append(f\"retrieve: {len(retrieved_docs)} docs encontrados de {sources}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"sources\": sources,\n",
    "        \"steps_trace\": steps_trace,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Funcion retrieve definida.\")\n",
    "print(\"Este nodo busca los top-5 documentos en ChromaDB por similitud semantica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Nodo 3: Evaluar Contexto (Context Grading)\n",
    "\n",
    "Este es el primer punto de control de calidad. El LLM evalua si los documentos recuperados son **realmente relevantes** para responder el query del usuario.\n",
    "\n",
    "Usamos **structured output** con Pydantic: el LLM debe devolver un objeto JSON con exactamente dos campos:\n",
    "- `grade`: `\"relevant\"` o `\"irrelevant\"`\n",
    "- `reasoning`: explicacion breve de por que\n",
    "\n",
    "Esto elimina la ambiguedad de parsear texto libre y garantiza que siempre obtenemos una clasificacion limpia.\n",
    "\n",
    "### Por que evaluar el contexto?\n",
    "\n",
    "Porque el retriever puede traer documentos que son **semanticamente similares** pero **no utiles** para la pregunta. Ejemplo:\n",
    "- Query: \"cual es la capital de Francia?\"\n",
    "- Retriever devuelve: chunk sobre \"regiones de AWS en eu-west-1 (Irlanda)\"\n",
    "- Similitud coseno alta (ambos hablan de Europa), pero totalmente irrelevante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Modelo Pydantic para structured output del Context Grading\n",
    "# =============================================================================\n",
    "\n",
    "class ContextGrade(BaseModel):\n",
    "    \"\"\"Evaluacion de relevancia del contexto recuperado.\"\"\"\n",
    "    grade: Literal[\"relevant\", \"irrelevant\"] = Field(\n",
    "        description=\"Si el contexto recuperado es relevante para responder el query.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Explicacion breve de por que el contexto es o no relevante.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Nodo 3: Evaluar Contexto\n",
    "# =============================================================================\n",
    "\n",
    "def grade_context(state: RAGState) -> RAGState:\n",
    "    \"\"\"Evalua si los documentos recuperados son relevantes para el query.\n",
    "    \n",
    "    Usa structured output (Pydantic) para obtener una clasificacion limpia:\n",
    "    - 'relevant': el contexto contiene informacion util para responder.\n",
    "    - 'irrelevant': el contexto no ayuda a responder la pregunta.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PASO: grade_context\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = state[\"retrieved_docs\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    steps_trace = state.get(\"steps_trace\", []).copy()\n",
    "    \n",
    "    # Preparar contexto para evaluacion\n",
    "    context_text = \"\\n---\\n\".join(retrieved_docs)\n",
    "    \n",
    "    # LLM con structured output\n",
    "    llm_with_structure = llm.with_structured_output(ContextGrade)\n",
    "    \n",
    "    system_prompt = \"\"\"Eres un evaluador de calidad de retrieval para un sistema RAG.\n",
    "\n",
    "Tu tarea: determinar si el CONTEXTO recuperado es RELEVANTE para responder el QUERY del usuario.\n",
    "\n",
    "Criterios para \"relevant\":\n",
    "- El contexto contiene informacion directamente util para responder la pregunta.\n",
    "- No tiene que ser una respuesta completa, pero debe aportar informacion relevante.\n",
    "\n",
    "Criterios para \"irrelevant\":\n",
    "- El contexto no tiene relacion con la pregunta.\n",
    "- La informacion recuperada es de un tema completamente diferente.\n",
    "- No se podria construir una respuesta util a partir de este contexto.\"\"\"\n",
    "    \n",
    "    user_msg = f\"\"\"QUERY del usuario: {query}\n",
    "\n",
    "CONTEXTO recuperado:\n",
    "{context_text}\n",
    "\n",
    "Evalua si este contexto es relevante para responder el query.\"\"\"\n",
    "    \n",
    "    # Invocar LLM con structured output\n",
    "    result: ContextGrade = llm_with_structure.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_msg),\n",
    "    ])\n",
    "    \n",
    "    print(f\"Query evaluado: '{query}'\")\n",
    "    print(f\"Contexto evaluado: {len(retrieved_docs)} documentos\")\n",
    "    print(f\"\\n>>> Resultado: {result.grade.upper()}\")\n",
    "    print(f\">>> Razonamiento: {result.reasoning}\")\n",
    "    \n",
    "    steps_trace.append(f\"grade_context: {result.grade} - {result.reasoning}\")\n",
    "    \n",
    "    # Incrementar retry_count si el contexto es irrelevante (evita loop infinito)\n",
    "    new_retry_count = retry_count + 1 if result.grade == \"irrelevant\" else retry_count\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"context_grade\": result.grade,\n",
    "        \"retry_count\": new_retry_count,\n",
    "        \"steps_trace\": steps_trace,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Funcion grade_context definida.\")\n",
    "print(\"Este nodo evalua si los documentos recuperados son relevantes para el query.\")\n",
    "print(\"Usa structured output (Pydantic) para garantizar clasificacion limpia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Nodo 4: Generar Respuesta\n",
    "\n",
    "Si el contexto es relevante, procedemos a generar la respuesta. El LLM recibe instrucciones estrictas:\n",
    "\n",
    "1. Responder **UNICAMENTE** con informacion del contexto proporcionado.\n",
    "2. **Citar fuentes** cuando sea posible.\n",
    "3. Si el contexto no contiene la respuesta, decirlo explicitamente en lugar de inventar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Nodo 4: Generar Respuesta\n",
    "# =============================================================================\n",
    "\n",
    "def generate(state: RAGState) -> RAGState:\n",
    "    \"\"\"Genera una respuesta basada UNICAMENTE en el contexto recuperado.\n",
    "    \n",
    "    El system prompt instruye al LLM a:\n",
    "    - No inventar informacion fuera del contexto.\n",
    "    - Citar las fuentes cuando sea posible.\n",
    "    - Admitir cuando no tiene informacion suficiente.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PASO: generate\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = state[\"retrieved_docs\"]\n",
    "    sources = state.get(\"sources\", [])\n",
    "    steps_trace = state.get(\"steps_trace\", []).copy()\n",
    "    \n",
    "    # Construir contexto\n",
    "    context_text = \"\\n---\\n\".join(retrieved_docs)\n",
    "    \n",
    "    system_prompt = \"\"\"Eres un asistente de soporte de NovaTech Solutions.\n",
    "Responde la pregunta del usuario basandote UNICAMENTE en el contexto proporcionado.\n",
    "\n",
    "Reglas estrictas:\n",
    "1. SOLO usa informacion que aparezca explicitamente en el contexto.\n",
    "2. Si el contexto no contiene la respuesta, di: \"No tengo informacion suficiente en la base de conocimiento para responder esta pregunta.\"\n",
    "3. NO inventes datos, precios, fechas o politicas que no esten en el contexto.\n",
    "4. Cita las fuentes cuando sea relevante.\n",
    "5. Se conciso pero completo.\n",
    "6. Responde en espanol.\"\"\"\n",
    "    \n",
    "    user_msg = f\"\"\"CONTEXTO (informacion verificada de la base de conocimiento):\n",
    "{context_text}\n",
    "\n",
    "FUENTES: {', '.join(sources)}\n",
    "\n",
    "PREGUNTA DEL USUARIO: {query}\"\"\"\n",
    "    \n",
    "    # Generar respuesta\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_msg),\n",
    "    ])\n",
    "    \n",
    "    generation = response.content.strip()\n",
    "    \n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Contexto usado: {len(retrieved_docs)} documentos\")\n",
    "    print(f\"\\n>>> Respuesta generada:\")\n",
    "    print(f\"{generation}\")\n",
    "    \n",
    "    steps_trace.append(f\"generate: respuesta de {len(generation)} caracteres\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"generation\": generation,\n",
    "        \"steps_trace\": steps_trace,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Funcion generate definida.\")\n",
    "print(\"Este nodo genera la respuesta usando SOLO el contexto recuperado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Nodo 5: Evaluar Respuesta (Answer Grading)\n",
    "\n",
    "Este es el segundo punto de control de calidad. Evaluamos si la respuesta generada es **fiel al contexto** (faithfulness) o si el LLM introdujo informacion que no estaba en los documentos recuperados (hallucination).\n",
    "\n",
    "### Por que evaluar la respuesta?\n",
    "\n",
    "Incluso con un buen contexto, el LLM puede:\n",
    "- Agregar datos de su entrenamiento que no estan en el contexto (\"segun mi conocimiento...\")\n",
    "- Exagerar o generalizar lo que dice el contexto\n",
    "- Confundir datos de diferentes chunks\n",
    "\n",
    "El grading de respuesta detecta estos casos y permite re-generar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Modelo Pydantic para structured output del Answer Grading\n",
    "# =============================================================================\n",
    "\n",
    "class AnswerGrade(BaseModel):\n",
    "    \"\"\"Evaluacion de faithfulness de la respuesta generada.\"\"\"\n",
    "    grade: Literal[\"faithful\", \"hallucination\"] = Field(\n",
    "        description=\"Si la respuesta es fiel al contexto o contiene alucinaciones.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Explicacion breve de por que la respuesta es fiel o contiene alucinaciones.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Nodo 5: Evaluar Respuesta\n",
    "# =============================================================================\n",
    "\n",
    "def grade_answer(state: RAGState) -> RAGState:\n",
    "    \"\"\"Evalua si la respuesta generada es fiel al contexto (no alucina).\n",
    "    \n",
    "    Compara la respuesta contra los documentos recuperados:\n",
    "    - 'faithful': toda la informacion en la respuesta proviene del contexto.\n",
    "    - 'hallucination': la respuesta contiene informacion no presente en el contexto.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PASO: grade_answer\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    generation = state[\"generation\"]\n",
    "    retrieved_docs = state[\"retrieved_docs\"]\n",
    "    steps_trace = state.get(\"steps_trace\", []).copy()\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    \n",
    "    # Preparar contexto para evaluacion\n",
    "    context_text = \"\\n---\\n\".join(retrieved_docs)\n",
    "    \n",
    "    # LLM con structured output\n",
    "    llm_with_structure = llm.with_structured_output(AnswerGrade)\n",
    "    \n",
    "    system_prompt = \"\"\"Eres un evaluador de calidad de respuestas para un sistema RAG.\n",
    "\n",
    "Tu tarea: determinar si la RESPUESTA generada es FIEL al CONTEXTO proporcionado.\n",
    "\n",
    "Criterios para \"faithful\":\n",
    "- TODA la informacion factual en la respuesta se puede verificar en el contexto.\n",
    "- La respuesta no agrega datos, numeros, o afirmaciones que no esten en el contexto.\n",
    "- Es aceptable que la respuesta resuma o parafrasee el contexto.\n",
    "- Si la respuesta dice \"no tengo informacion suficiente\", eso es faithful.\n",
    "\n",
    "Criterios para \"hallucination\":\n",
    "- La respuesta contiene informacion especifica (datos, numeros, politicas) que NO aparece en el contexto.\n",
    "- La respuesta inventa o exagera mas alla de lo que dice el contexto.\n",
    "- La respuesta contradice al contexto.\"\"\"\n",
    "    \n",
    "    user_msg = f\"\"\"CONTEXTO (fuente de verdad):\n",
    "{context_text}\n",
    "\n",
    "RESPUESTA generada:\n",
    "{generation}\n",
    "\n",
    "Evalua si la respuesta es fiel al contexto.\"\"\"\n",
    "    \n",
    "    # Invocar LLM con structured output\n",
    "    result: AnswerGrade = llm_with_structure.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_msg),\n",
    "    ])\n",
    "    \n",
    "    print(f\"Respuesta evaluada ({len(generation)} chars)\")\n",
    "    print(f\"Contexto de referencia: {len(retrieved_docs)} documentos\")\n",
    "    print(f\"\\n>>> Resultado: {result.grade.upper()}\")\n",
    "    print(f\">>> Razonamiento: {result.reasoning}\")\n",
    "    \n",
    "    # Incrementar retry_count si es hallucination\n",
    "    new_retry_count = retry_count + 1 if result.grade == \"hallucination\" else retry_count\n",
    "    \n",
    "    steps_trace.append(f\"grade_answer: {result.grade} - {result.reasoning}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"answer_grade\": result.grade,\n",
    "        \"retry_count\": new_retry_count,\n",
    "        \"steps_trace\": steps_trace,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Funcion grade_answer definida.\")\n",
    "print(\"Este nodo evalua si la respuesta es fiel al contexto (faithfulness check).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Funciones de Routing (Edges Condicionales)\n",
    "\n",
    "Las funciones de routing son el \"cerebro\" del flujo condicional. Son funciones puras que leen el estado y devuelven un string indicando el proximo nodo.\n",
    "\n",
    "Hay dos puntos de decision:\n",
    "\n",
    "1. **Despues de evaluar contexto**: si es irrelevante y quedan reintentos → re-query; si no → generar.\n",
    "2. **Despues de evaluar respuesta**: si es alucinacion y quedan reintentos → re-generar; si no → terminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Funciones de Routing (edges condicionales)\n",
    "# =============================================================================\n",
    "\n",
    "def should_regenerate(state: RAGState) -> str:\n",
    "    \"\"\"Decide si re-hacer el query (contexto irrelevante) o generar respuesta.\n",
    "    \n",
    "    Returns:\n",
    "        're_query':  si el contexto es irrelevante Y hay reintentos disponibles.\n",
    "        'generate':  si el contexto es relevante O se agotaron los reintentos.\n",
    "    \"\"\"\n",
    "    context_grade = state[\"context_grade\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    max_retries = state.get(\"max_retries\", 3)\n",
    "    \n",
    "    if context_grade == \"irrelevant\" and retry_count < max_retries:\n",
    "        print(f\"\\n>>> ROUTING: Contexto irrelevante. Re-query (intento {retry_count + 1}/{max_retries})\")\n",
    "        return \"re_query\"\n",
    "    else:\n",
    "        if context_grade == \"irrelevant\":\n",
    "            print(f\"\\n>>> ROUTING: Contexto irrelevante pero se agotaron reintentos ({retry_count}/{max_retries}). Generando con lo que hay.\")\n",
    "        else:\n",
    "            print(f\"\\n>>> ROUTING: Contexto relevante. Procediendo a generar respuesta.\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def should_rewrite(state: RAGState) -> str:\n",
    "    \"\"\"Decide si re-generar la respuesta (alucinacion) o terminar.\n",
    "    \n",
    "    Returns:\n",
    "        're_generate':  si la respuesta alucina Y hay reintentos disponibles.\n",
    "        'finish':       si la respuesta es fiel O se agotaron los reintentos.\n",
    "    \"\"\"\n",
    "    answer_grade = state[\"answer_grade\"]\n",
    "    retry_count = state.get(\"retry_count\", 0)\n",
    "    max_retries = state.get(\"max_retries\", 3)\n",
    "    \n",
    "    if answer_grade == \"hallucination\" and retry_count < max_retries:\n",
    "        print(f\"\\n>>> ROUTING: Alucinacion detectada. Re-generando (intento {retry_count}/{max_retries})\")\n",
    "        return \"re_generate\"\n",
    "    else:\n",
    "        if answer_grade == \"hallucination\":\n",
    "            print(f\"\\n>>> ROUTING: Alucinacion detectada pero se agotaron reintentos ({retry_count}/{max_retries}). Finalizando.\")\n",
    "        else:\n",
    "            print(f\"\\n>>> ROUTING: Respuesta fiel al contexto. Finalizando.\")\n",
    "        return \"finish\"\n",
    "\n",
    "\n",
    "print(\"Funciones de routing definidas:\")\n",
    "print(\"  - should_regenerate: decide entre re_query y generate\")\n",
    "print(\"  - should_rewrite: decide entre re_generate y finish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Construir el Grafo\n",
    "\n",
    "Ahora conectamos todos los nodos y edges en un `StateGraph` de LangGraph. Este es el momento donde la arquitectura cobra vida.\n",
    "\n",
    "```\n",
    "analyze_query ──→ retrieve ──→ grade_context ──┬──→ generate ──→ grade_answer ──┬──→ END\n",
    "      ^                                        │                                │\n",
    "      └──────── (re_query) ────────────────────┘        (re_generate) ──────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Construccion del grafo LangGraph\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Crear el grafo con el tipo de estado\n",
    "workflow = StateGraph(RAGState)\n",
    "\n",
    "# 2. Agregar nodos (cada nodo es una funcion que transforma el estado)\n",
    "workflow.add_node(\"analyze_query\", analyze_query)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_context\", grade_context)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"grade_answer\", grade_answer)\n",
    "\n",
    "# 3. Definir el punto de entrada\n",
    "workflow.set_entry_point(\"analyze_query\")\n",
    "\n",
    "# 4. Agregar edges fijos (transiciones siempre iguales)\n",
    "workflow.add_edge(\"analyze_query\", \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_context\")\n",
    "workflow.add_edge(\"generate\", \"grade_answer\")\n",
    "\n",
    "# 5. Agregar edges condicionales (routing basado en el estado)\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_context\",          # Nodo de origen\n",
    "    should_regenerate,         # Funcion de routing\n",
    "    {                          # Mapa: resultado_routing -> nodo_destino\n",
    "        \"re_query\": \"analyze_query\",   # Si contexto irrelevante → volver a analizar\n",
    "        \"generate\": \"generate\",         # Si contexto relevante → generar\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_answer\",            # Nodo de origen\n",
    "    should_rewrite,            # Funcion de routing\n",
    "    {                          # Mapa: resultado_routing -> nodo_destino\n",
    "        \"re_generate\": \"generate\",  # Si alucinacion → re-generar\n",
    "        \"finish\": END,              # Si fiel → terminar\n",
    "    }\n",
    ")\n",
    "\n",
    "# 6. Compilar el grafo\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Grafo compilado exitosamente.\")\n",
    "print(f\"\\nNodos: {list(app.get_graph().nodes.keys())}\")\n",
    "print(f\"Punto de entrada: analyze_query\")\n",
    "print(f\"Punto de salida: END (despues de grade_answer)\")\n",
    "print(f\"\\nEdges condicionales:\")\n",
    "print(f\"  grade_context → re_query | generate\")\n",
    "print(f\"  grade_answer  → re_generate | finish (END)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Visualizar el Grafo\n",
    "\n",
    "LangGraph puede generar una representacion visual del grafo como diagrama Mermaid. Esto es util para documentacion y para verificar que la arquitectura esta correcta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualizacion del grafo\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    # Intentar generar imagen PNG del grafo\n",
    "    graph_image = app.get_graph().draw_mermaid_png()\n",
    "    display(Image(graph_image))\n",
    "    print(\"Grafo renderizado como imagen PNG.\")\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo renderizar PNG (necesita graphviz): {e}\")\n",
    "    print(\"\\nMostrando representacion Mermaid en texto:\")\n",
    "    print(\"=\" * 60)\n",
    "    # Fallback: mostrar mermaid como texto\n",
    "    mermaid_repr = app.get_graph().draw_mermaid()\n",
    "    print(mermaid_repr)\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nPuedes copiar este texto y pegarlo en https://mermaid.live/ para ver el diagrama.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Funcion Helper: Ejecutar y Mostrar Resultados\n",
    "\n",
    "Antes de los ejemplos, creamos una funcion helper que ejecuta el grafo y muestra los resultados de forma estructurada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Funcion helper para ejecutar y mostrar resultados\n",
    "# =============================================================================\n",
    "\n",
    "def run_self_rag(query: str, max_retries: int = 3) -> dict:\n",
    "    \"\"\"Ejecuta el pipeline Self-RAG completo y muestra resultados formateados.\n",
    "    \n",
    "    Args:\n",
    "        query: Pregunta del usuario.\n",
    "        max_retries: Maximo de reintentos para loops de correccion.\n",
    "    \n",
    "    Returns:\n",
    "        Estado final del grafo.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\" * 70)\n",
    "    print(f\"# SELF-RAG PIPELINE\")\n",
    "    print(f\"# Query: '{query}'\")\n",
    "    print(f\"# Max retries: {max_retries}\")\n",
    "    print(\"#\" * 70)\n",
    "    \n",
    "    # Estado inicial\n",
    "    initial_state: RAGState = {\n",
    "        \"query\": query,\n",
    "        \"refined_query\": \"\",\n",
    "        \"retrieved_docs\": [],\n",
    "        \"context_grade\": \"\",\n",
    "        \"generation\": \"\",\n",
    "        \"answer_grade\": \"\",\n",
    "        \"sources\": [],\n",
    "        \"retry_count\": 0,\n",
    "        \"max_retries\": max_retries,\n",
    "        \"steps_trace\": [],\n",
    "    }\n",
    "    \n",
    "    # Ejecutar el grafo\n",
    "    final_state = app.invoke(initial_state)\n",
    "    \n",
    "    # Mostrar resumen\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"RESUMEN FINAL\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Query original:   {final_state['query']}\")\n",
    "    print(f\"Query refinado:   {final_state['refined_query']}\")\n",
    "    print(f\"Contexto grade:   {final_state['context_grade']}\")\n",
    "    print(f\"Answer grade:     {final_state['answer_grade']}\")\n",
    "    print(f\"Reintentos:       {final_state['retry_count']}/{final_state['max_retries']}\")\n",
    "    print(f\"Fuentes:          {final_state['sources']}\")\n",
    "    \n",
    "    print(f\"\\n--- TRAZA DE PASOS ---\")\n",
    "    for i, step in enumerate(final_state[\"steps_trace\"], 1):\n",
    "        print(f\"  [{i}] {step}\")\n",
    "    \n",
    "    print(f\"\\n--- RESPUESTA FINAL ---\")\n",
    "    print(final_state[\"generation\"])\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return final_state\n",
    "\n",
    "\n",
    "print(\"Funcion run_self_rag definida. Lista para ejecutar ejemplos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Ejemplo 1: Query Directo (Sin Loops)\n",
    "\n",
    "Empezamos con un caso simple: una pregunta clara y especifica sobre los precios de un producto. Esperamos que:\n",
    "\n",
    "1. El query NO necesite refinamiento significativo.\n",
    "2. El retrieval traiga documentos relevantes.\n",
    "3. El contexto sea evaluado como \"relevant\".\n",
    "4. La respuesta sea \"faithful\".\n",
    "5. **No haya loops** — el flujo va de principio a fin sin re-intentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ejemplo 1: Query directo — flujo lineal sin loops\n",
    "# =============================================================================\n",
    "\n",
    "resultado_1 = run_self_rag(\n",
    "    query=\"Cuanto cuesta el plan Professional de Analytics Pro?\",\n",
    "    max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analisis del Ejemplo 1\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ANALISIS DEL EJEMPLO 1\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_pasos = len(resultado_1[\"steps_trace\"])\n",
    "tuvo_loops = resultado_1[\"retry_count\"] > 0\n",
    "\n",
    "print(f\"Total de pasos ejecutados: {total_pasos}\")\n",
    "print(f\"Loops de correccion: {'SI' if tuvo_loops else 'NO'}\")\n",
    "print(f\"Reintentos usados: {resultado_1['retry_count']}\")\n",
    "print(f\"\\nObservacion:\")\n",
    "\n",
    "if not tuvo_loops:\n",
    "    print(\"  El query era claro y especifico. El retrieval trajo documentos relevantes\")\n",
    "    print(\"  y la respuesta fue fiel al contexto. Flujo lineal optimo.\")\n",
    "    print(\"  Esto representa el 'caso feliz' del pipeline.\")\n",
    "else:\n",
    "    print(\"  El pipeline necesito correccion, lo cual es inesperado para un query tan claro.\")\n",
    "    print(\"  Esto podria indicar un problema con el chunking o el threshold de relevancia.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16. Ejemplo 2: Query Vago que Requiere Refinamiento\n",
    "\n",
    "Ahora probamos con un query intencionalmente vago: `\"precios\"`. Este query es demasiado generico y podria traer documentos de cualquier producto.\n",
    "\n",
    "Esperamos ver:\n",
    "1. El analizador de query lo **refina** a algo mas especifico.\n",
    "2. Posiblemente el contexto sea evaluado como irrelevante en el primer intento.\n",
    "3. El sistema podria hacer un **loop de re-query** con una reformulacion mas amplia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ejemplo 2: Query vago — posible loop de re-query\n",
    "# =============================================================================\n",
    "\n",
    "resultado_2 = run_self_rag(\n",
    "    query=\"precios\",\n",
    "    max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analisis del Ejemplo 2\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ANALISIS DEL EJEMPLO 2\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_pasos = len(resultado_2[\"steps_trace\"])\n",
    "tuvo_loops = resultado_2[\"retry_count\"] > 0\n",
    "\n",
    "print(f\"Total de pasos ejecutados: {total_pasos}\")\n",
    "print(f\"Loops de correccion: {'SI' if tuvo_loops else 'NO'}\")\n",
    "print(f\"Reintentos usados: {resultado_2['retry_count']}\")\n",
    "print(f\"\\nQuery original:  '{resultado_2['query']}'\")\n",
    "print(f\"Query refinado:  '{resultado_2['refined_query']}'\")\n",
    "print(f\"\\nObservacion:\")\n",
    "print(f\"  Nota como el analizador de queries transformo 'precios' en un query\")\n",
    "print(f\"  mucho mas especifico y optimizado para retrieval semantico.\")\n",
    "print(f\"  Esto demuestra el valor del primer nodo del pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 17. Ejemplo 3: Query Fuera de Dominio\n",
    "\n",
    "Probamos con una pregunta que **no tiene respuesta** en la base de conocimiento: `\"cual es la capital de Francia?\"`.\n",
    "\n",
    "Esperamos ver:\n",
    "1. El retrieval trae documentos, pero son irrelevantes.\n",
    "2. El grading de contexto marca como \"irrelevant\".\n",
    "3. El sistema intenta re-query varias veces.\n",
    "4. Despues de agotar reintentos, genera una respuesta honesta: \"No tengo informacion suficiente.\"\n",
    "\n",
    "Este es un caso critico: un RAG naive responderia con informacion inventada. Nuestro Self-RAG detecta el problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ejemplo 3: Query fuera de dominio — deteccion de contexto irrelevante\n",
    "# =============================================================================\n",
    "\n",
    "resultado_3 = run_self_rag(\n",
    "    query=\"Cual es la capital de Francia?\",\n",
    "    max_retries=2  # Menos reintentos para no gastar tokens innecesariamente\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analisis del Ejemplo 3\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ANALISIS DEL EJEMPLO 3\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_pasos = len(resultado_3[\"steps_trace\"])\n",
    "tuvo_loops = resultado_3[\"retry_count\"] > 0\n",
    "\n",
    "print(f\"Total de pasos ejecutados: {total_pasos}\")\n",
    "print(f\"Loops de correccion: {'SI' if tuvo_loops else 'NO'}\")\n",
    "print(f\"Reintentos usados: {resultado_3['retry_count']}/{resultado_3['max_retries']}\")\n",
    "print(f\"Context grade:    {resultado_3['context_grade']}\")\n",
    "print(f\"Answer grade:     {resultado_3['answer_grade']}\")\n",
    "print(f\"\\nObservacion:\")\n",
    "print(f\"  La pregunta sobre la capital de Francia NO tiene respuesta en la base\")\n",
    "print(f\"  de conocimiento de NovaTech. El sistema detecto esto correctamente:\")\n",
    "print(f\"  - El context grading marco el contexto como irrelevante.\")\n",
    "print(f\"  - El sistema intento re-queries pero la base no tiene esta info.\")\n",
    "print(f\"  - La respuesta final es honesta sobre la falta de informacion.\")\n",
    "print(f\"\\n  CLAVE: Un RAG naive habria respondido 'Paris' usando conocimiento\")\n",
    "print(f\"  del modelo, no de la base de datos. Eso es una ALUCINACION en contexto RAG.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 18. Comparacion: RAG Naive vs Prompt Chaining\n",
    "\n",
    "Para entender el valor real del prompt chaining, ejecutamos el mismo query por ambos enfoques y comparamos:\n",
    "\n",
    "| Aspecto | RAG Naive | RAG con Prompt Chaining |\n",
    "|---------|-----------|------------------------|\n",
    "| Pasos | 3 (query → retrieve → generate) | 5+ (con validacion y loops) |\n",
    "| Validacion de contexto | Ninguna | LLM evalua relevancia |\n",
    "| Validacion de respuesta | Ninguna | LLM evalua faithfulness |\n",
    "| Autocorreccion | No | Si (re-query, re-generate) |\n",
    "| Trazabilidad | Minima | Completa (steps_trace) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Comparacion: RAG Naive vs Self-RAG\n",
    "# =============================================================================\n",
    "\n",
    "query_comparacion = \"Que integraciones tiene AI Assistant y cuanto cuesta?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARACION: RAG Naive vs Self-RAG con Prompt Chaining\")\n",
    "print(f\"Query: '{query_comparacion}'\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- RAG Naive (sin validacion) ---\n",
    "print(\"\\n\" + \"-\" * 35)\n",
    "print(\"ENFOQUE 1: RAG Naive\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Paso 1: Retrieval directo (sin refinamiento de query)\n",
    "naive_results = collection.query(\n",
    "    query_texts=[query_comparacion],\n",
    "    n_results=5,\n",
    ")\n",
    "naive_context = \"\\n---\\n\".join(naive_results[\"documents\"][0])\n",
    "naive_sources = list(set(m[\"source\"] for m in naive_results[\"metadatas\"][0]))\n",
    "\n",
    "# Paso 2: Generacion directa (sin validacion)\n",
    "naive_response = llm.invoke([\n",
    "    SystemMessage(content=f\"Responde basandote en este contexto:\\n{naive_context}\"),\n",
    "    HumanMessage(content=query_comparacion),\n",
    "])\n",
    "\n",
    "print(f\"Pasos ejecutados: 2 (retrieve + generate)\")\n",
    "print(f\"Validacion de contexto: NINGUNA\")\n",
    "print(f\"Validacion de respuesta: NINGUNA\")\n",
    "print(f\"Fuentes: {naive_sources}\")\n",
    "print(f\"\\nRespuesta:\")\n",
    "print(naive_response.content)\n",
    "\n",
    "# --- Self-RAG (con validacion) ---\n",
    "print(\"\\n\" + \"-\" * 35)\n",
    "print(\"ENFOQUE 2: Self-RAG (Prompt Chaining)\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "resultado_chaining = run_self_rag(query_comparacion, max_retries=2)\n",
    "\n",
    "# --- Resumen comparativo ---\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESUMEN COMPARATIVO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Metrica':<30} {'RAG Naive':<20} {'Self-RAG':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Pasos ejecutados':<30} {'2':<20} {len(resultado_chaining['steps_trace']):<20}\")\n",
    "print(f\"{'Validacion contexto':<30} {'No':<20} {resultado_chaining['context_grade']:<20}\")\n",
    "print(f\"{'Validacion respuesta':<30} {'No':<20} {resultado_chaining['answer_grade']:<20}\")\n",
    "print(f\"{'Loops de correccion':<30} {'0':<20} {resultado_chaining['retry_count']:<20}\")\n",
    "print(f\"{'Trazabilidad':<30} {'Ninguna':<20} {'Completa':<20}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 19. Analisis de Costos\n",
    "\n",
    "Prompt chaining mejora la calidad, pero a un costo. Veamos cuanto cuesta realmente cada enfoque.\n",
    "\n",
    "### Modelo de costos (gpt-4o-mini, precios aproximados)\n",
    "\n",
    "| Recurso | Costo |\n",
    "|---------|-------|\n",
    "| Input tokens | ~$0.15 / 1M tokens |\n",
    "| Output tokens | ~$0.60 / 1M tokens |\n",
    "| Embedding (text-embedding-3-small) | ~$0.02 / 1M tokens |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Analisis de costos: RAG Naive vs Self-RAG\n",
    "# =============================================================================\n",
    "\n",
    "# Estimaciones de tokens por paso (aproximadas para gpt-4o-mini)\n",
    "# Basadas en los prompts que usamos en este notebook\n",
    "\n",
    "costos_por_millon = {\n",
    "    \"input\": 0.15,   # USD por 1M tokens de input\n",
    "    \"output\": 0.60,  # USD por 1M tokens de output\n",
    "    \"embedding\": 0.02,  # USD por 1M tokens de embedding\n",
    "}\n",
    "\n",
    "# Tokens estimados por paso\n",
    "tokens_por_paso = {\n",
    "    \"analyze_query\": {\"input\": 350, \"output\": 50},\n",
    "    \"retrieve (embedding)\": {\"embedding\": 30},\n",
    "    \"grade_context\": {\"input\": 800, \"output\": 80},\n",
    "    \"generate\": {\"input\": 900, \"output\": 200},\n",
    "    \"grade_answer\": {\"input\": 1100, \"output\": 80},\n",
    "}\n",
    "\n",
    "print(\"ANALISIS DE COSTOS POR QUERY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Modelo: gpt-4o-mini\")\n",
    "print(f\"Precios: input=${costos_por_millon['input']}/1M, output=${costos_por_millon['output']}/1M\")\n",
    "print()\n",
    "\n",
    "# --- Costo RAG Naive ---\n",
    "naive_pasos = [\"retrieve (embedding)\", \"generate\"]\n",
    "costo_naive_total = 0.0\n",
    "\n",
    "print(\"RAG NAIVE (2 pasos):\")\n",
    "print(\"-\" * 50)\n",
    "for paso in naive_pasos:\n",
    "    tokens = tokens_por_paso[paso]\n",
    "    costo_paso = 0.0\n",
    "    for tipo, cantidad in tokens.items():\n",
    "        costo_paso += (cantidad / 1_000_000) * costos_por_millon[tipo]\n",
    "    costo_naive_total += costo_paso\n",
    "    print(f\"  {paso:<25} tokens: {tokens}  costo: ${costo_paso:.6f}\")\n",
    "print(f\"  {'TOTAL':<25} ${costo_naive_total:.6f} por query\")\n",
    "\n",
    "# --- Costo Self-RAG (sin loops) ---\n",
    "chaining_pasos = [\"analyze_query\", \"retrieve (embedding)\", \"grade_context\", \"generate\", \"grade_answer\"]\n",
    "costo_chaining_total = 0.0\n",
    "\n",
    "print(f\"\\nSELF-RAG SIN LOOPS (5 pasos):\")\n",
    "print(\"-\" * 50)\n",
    "for paso in chaining_pasos:\n",
    "    tokens = tokens_por_paso[paso]\n",
    "    costo_paso = 0.0\n",
    "    for tipo, cantidad in tokens.items():\n",
    "        costo_paso += (cantidad / 1_000_000) * costos_por_millon[tipo]\n",
    "    costo_chaining_total += costo_paso\n",
    "    print(f\"  {paso:<25} tokens: {tokens}  costo: ${costo_paso:.6f}\")\n",
    "print(f\"  {'TOTAL':<25} ${costo_chaining_total:.6f} por query\")\n",
    "\n",
    "# --- Costo Self-RAG (con 1 loop de re-query) ---\n",
    "loop_pasos = [\"analyze_query\", \"retrieve (embedding)\", \"grade_context\",\n",
    "              \"analyze_query\", \"retrieve (embedding)\", \"grade_context\",  # re-query\n",
    "              \"generate\", \"grade_answer\"]\n",
    "costo_loop_total = 0.0\n",
    "\n",
    "print(f\"\\nSELF-RAG CON 1 LOOP (8 pasos):\")\n",
    "print(\"-\" * 50)\n",
    "for paso in loop_pasos:\n",
    "    tokens = tokens_por_paso[paso]\n",
    "    costo_paso = 0.0\n",
    "    for tipo, cantidad in tokens.items():\n",
    "        costo_paso += (cantidad / 1_000_000) * costos_por_millon[tipo]\n",
    "    costo_loop_total += costo_paso\n",
    "print(f\"  {'TOTAL (8 pasos)':<25} ${costo_loop_total:.6f} por query\")\n",
    "\n",
    "# --- Resumen ---\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"RESUMEN DE COSTOS\")\n",
    "print(f\"{'=' * 70}\")\n",
    "print(f\"RAG Naive:              ${costo_naive_total:.6f} / query\")\n",
    "print(f\"Self-RAG (sin loops):   ${costo_chaining_total:.6f} / query  ({costo_chaining_total/costo_naive_total:.1f}x vs naive)\")\n",
    "print(f\"Self-RAG (1 loop):      ${costo_loop_total:.6f} / query  ({costo_loop_total/costo_naive_total:.1f}x vs naive)\")\n",
    "print(f\"\\nA 1,000 queries/dia:\")\n",
    "print(f\"  RAG Naive:            ${costo_naive_total * 1000 * 30:.2f} / mes\")\n",
    "print(f\"  Self-RAG (sin loops): ${costo_chaining_total * 1000 * 30:.2f} / mes\")\n",
    "print(f\"  Self-RAG (1 loop):    ${costo_loop_total * 1000 * 30:.2f} / mes\")\n",
    "print(f\"\\nConclusion:\")\n",
    "print(f\"  El costo adicional del Self-RAG es de centavos por query.\")\n",
    "print(f\"  Cuando el costo de una respuesta INCORRECTA es alto (soporte al cliente,\")\n",
    "print(f\"  chatbot medico, asesor legal), ese centavo extra esta ABSOLUTAMENTE justificado.\")\n",
    "print(f\"  Cuando el costo de error es bajo y el volumen es masivo, RAG naive puede ser suficiente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 20. Errores Comunes\n",
    "\n",
    "Basandome en experiencia real implementando Self-RAG en produccion, estos son los errores mas frecuentes:\n",
    "\n",
    "### Error 1: No poner limite de reintentos\n",
    "\n",
    "Sin `max_retries`, un query fuera de dominio genera un **loop infinito**: el contexto siempre es irrelevante, el sistema siempre re-intenta, y nunca termina. Siempre define un tope (2-3 reintentos es razonable).\n",
    "\n",
    "```python\n",
    "# MAL: sin limite\n",
    "if context_grade == \"irrelevant\":\n",
    "    return \"re_query\"  # Loop infinito si la info no existe en la KB\n",
    "\n",
    "# BIEN: con limite\n",
    "if context_grade == \"irrelevant\" and retry_count < max_retries:\n",
    "    return \"re_query\"\n",
    "```\n",
    "\n",
    "### Error 2: Evaluar contexto con el mismo modelo que genera\n",
    "\n",
    "Idealmente, el evaluador deberia ser un modelo diferente (o al menos con un prompt muy distinto) al generador. Si usas el mismo modelo y prompt, tiende a evaluar sus propias respuestas como \"correctas\" — es como pedirle a un estudiante que se califique a si mismo.\n",
    "\n",
    "En produccion, considera usar un modelo mas grande/critico para grading (ej. gpt-4o) y uno mas rapido/barato para generacion (ej. gpt-4o-mini).\n",
    "\n",
    "### Error 3: Ignorar la traza de pasos\n",
    "\n",
    "El `steps_trace` no es solo para debugging. En produccion, es tu **audit trail**. Cuando un usuario reporta una respuesta incorrecta, la traza te dice exactamente:\n",
    "- Que documentos se recuperaron\n",
    "- Como se evaluo el contexto\n",
    "- Cuantos reintentos hubo\n",
    "- Por que el sistema decidio que la respuesta era correcta\n",
    "\n",
    "Guardala en logs estructurados.\n",
    "\n",
    "### Error 4: Structured output sin Pydantic\n",
    "\n",
    "Parsear la respuesta del LLM como texto libre para extraer \"relevant\" o \"irrelevant\" es fragil. El modelo puede responder \"The context is somewhat relevant\" y tu regex falla. Pydantic con `with_structured_output()` **garantiza** el formato.\n",
    "\n",
    "```python\n",
    "# MAL: parseo de texto libre\n",
    "response = llm.invoke(\"Is this relevant? Answer 'relevant' or 'irrelevant'\")\n",
    "grade = response.content.strip().lower()  # Puede fallar con formatos inesperados\n",
    "\n",
    "# BIEN: structured output con Pydantic\n",
    "llm_structured = llm.with_structured_output(ContextGrade)\n",
    "result = llm_structured.invoke(messages)  # Siempre devuelve ContextGrade valido\n",
    "```\n",
    "\n",
    "### Error 5: Chunks demasiado grandes en el context grading\n",
    "\n",
    "Si pasas 5 chunks de 2000 tokens cada uno al evaluador de contexto, estas consumiendo muchos tokens en evaluacion. Considera evaluar cada chunk individualmente y filtrar solo los relevantes antes de pasarlos al generador. Esto es mas costoso en llamadas pero mas preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 21. Checklist de Comprension\n",
    "\n",
    "Antes de pasar al Notebook 04 (RAG con Routing), verifica que puedes responder:\n",
    "\n",
    "- [ ] **1. Cual es la diferencia fundamental entre RAG naive y Self-RAG?**\n",
    "  - Pista: piensa en que pasa cuando el retrieval falla o el LLM alucina.\n",
    "\n",
    "- [ ] **2. Por que usamos `TypedDict` para el estado y no un diccionario comun?**\n",
    "  - Pista: piensa en type safety, documentacion, y tooling del IDE.\n",
    "\n",
    "- [ ] **3. Que pasaria si eliminamos el `max_retries` del estado?**\n",
    "  - Pista: piensa en el query \"capital de Francia\" y que pasaria sin limite.\n",
    "\n",
    "- [ ] **4. Cuando NO vale la pena usar prompt chaining en un sistema RAG?**\n",
    "  - Pista: piensa en latencia, costo, y el costo real de una respuesta incorrecta.\n",
    "\n",
    "- [ ] **5. Como modificarias este grafo para agregar un nodo de \"reformulacion de respuesta\" que simplifique respuestas muy tecnicas para usuarios no-tecnicos?**\n",
    "  - Pista: seria un nodo nuevo entre `generate` y `grade_answer`, o despues de `grade_answer`?\n",
    "\n",
    "---\n",
    "\n",
    "### Siguiente paso\n",
    "\n",
    "En el **Notebook 04** implementaremos RAG con **Routing Multi-Dominio**: un clasificador que dirige cada query al retriever especializado (productos vs. tecnico vs. fallback). Esto complementa el prompt chaining: routing selecciona el dominio, chaining asegura la calidad dentro del dominio.\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook creado para el curso de AI Engineering. Basado en Asai et al. (2023) Self-RAG y Chip Huyen (2024) AI Engineering.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}