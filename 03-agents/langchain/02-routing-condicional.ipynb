{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# 02 \u2014 Routing Condicional\n",
    "\n",
    "**Objetivo**: Clasificar queries del usuario y dirigirlas al especialista correcto (Batman, Spider-Man, o Comparacion).\n",
    "\n",
    "## Contenido\n",
    "1. Router LLM con Pydantic\n",
    "2. Especialistas por personaje\n",
    "3. StateGraph con conditional edges\n",
    "4. Visualizacion del grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Literal, TypedDict, Annotated\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ROUTING CONDICIONAL\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000003",
   "metadata": {},
   "source": [
    "## 1. Router LLM con Pydantic\n",
    "\n",
    "El router clasifica la query del usuario en una de 3 rutas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODELO DE ROUTING\n",
    "# ============================================================\n",
    "\n",
    "class RouteDecision(BaseModel):\n",
    "    \"\"\"Decision de routing para una query sobre comics.\"\"\"\n",
    "    ruta: Literal[\"batman\", \"spiderman\", \"comparacion\"] = Field(\n",
    "        description=\"Ruta seleccionada segun el contenido de la query\"\n",
    "    )\n",
    "    confianza: float = Field(\n",
    "        description=\"Nivel de confianza en la decision (0.0 a 1.0)\",\n",
    "        ge=0.0, le=1.0\n",
    "    )\n",
    "    razonamiento: str = Field(\n",
    "        description=\"Breve explicacion de por que se eligio esta ruta\"\n",
    "    )\n",
    "\n",
    "\n",
    "router_llm = llm.with_structured_output(RouteDecision)\n",
    "\n",
    "ROUTER_PROMPT = \"\"\"Eres un clasificador de queries sobre comics. \n",
    "Clasifica la query del usuario en una de estas rutas:\n",
    "- \"batman\": preguntas especificas sobre Batman, Gotham, sus villanos, etc.\n",
    "- \"spiderman\": preguntas especificas sobre Spider-Man, Nueva York, sus villanos, etc.\n",
    "- \"comparacion\": preguntas que involucran a ambos personajes o comparaciones.\n",
    "\"\"\"\n",
    "\n",
    "# Test del router\n",
    "test_queries = [\n",
    "    \"Como se convirtio Bruce Wayne en Batman?\",\n",
    "    \"Que poderes tiene Spider-Man?\",\n",
    "    \"Quien es mas inteligente, Batman o Spider-Man?\",\n",
    "]\n",
    "\n",
    "print(\"Test del Router:\")\n",
    "for query in test_queries:\n",
    "    decision = router_llm.invoke([\n",
    "        SystemMessage(content=ROUTER_PROMPT),\n",
    "        HumanMessage(content=query),\n",
    "    ])\n",
    "    print(f\"\\n  Query: {query}\")\n",
    "    print(f\"  Ruta: {decision.ruta} (confianza: {decision.confianza:.1%})\")\n",
    "    print(f\"  Razon: {decision.razonamiento}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000005",
   "metadata": {},
   "source": [
    "## 2. Especialistas por Personaje\n",
    "\n",
    "Cada especialista tiene acceso a los datos de su personaje y un system prompt especifico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESPECIALISTAS\n",
    "# ============================================================\n",
    "\n",
    "def cargar_comics(personaje: str) -> list[dict]:\n",
    "    \"\"\"Carga los comics de un personaje.\"\"\"\n",
    "    ruta = f\"../data/{personaje}_comics.json\"\n",
    "    with open(ruta) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def crear_contexto(comics: list[dict], max_chars: int = 3000) -> str:\n",
    "    \"\"\"Crea contexto resumido de los comics.\"\"\"\n",
    "    contexto = []\n",
    "    chars = 0\n",
    "    for comic in comics:\n",
    "        texto = f\"[{comic['titulo']}]: {comic['contenido'][:300]}\"\n",
    "        if chars + len(texto) > max_chars:\n",
    "            break\n",
    "        contexto.append(texto)\n",
    "        chars += len(texto)\n",
    "    return \"\\n\\n\".join(contexto)\n",
    "\n",
    "\n",
    "PROMPTS_ESPECIALISTAS = {\n",
    "    \"batman\": \"Eres un experto en Batman y el universo DC. Responde basandote en los datos proporcionados. Se preciso y cita arcos narrativos especificos.\",\n",
    "    \"spiderman\": \"Eres un experto en Spider-Man y el universo Marvel. Responde basandote en los datos proporcionados. Se preciso y cita arcos narrativos especificos.\",\n",
    "    \"comparacion\": \"Eres un experto en comics que compara Batman y Spider-Man. Usa evidencia de ambos universos para dar respuestas balanceadas.\",\n",
    "}\n",
    "\n",
    "print(\"Especialistas configurados:\")\n",
    "for nombre in PROMPTS_ESPECIALISTAS:\n",
    "    print(f\"  - {nombre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000007",
   "metadata": {},
   "source": [
    "## 3. StateGraph con Conditional Edges\n",
    "\n",
    "```\n",
    "         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "         \u2502  START   \u2502\n",
    "         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n",
    "              \u2502\n",
    "         \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n",
    "         \u2502  Router   \u2502\n",
    "         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n",
    "              \u2502\n",
    "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "     \u25bc        \u25bc        \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Batman  \u2502 \u2502Spider\u2502 \u2502Comparacion \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u252c\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "     \u2502         \u2502            \u2502\n",
    "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u25bc\n",
    "         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "         \u2502    END     \u2502\n",
    "         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STATEGRAPH CON ROUTING\n",
    "# ============================================================\n",
    "\n",
    "class RouterState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    ruta: str\n",
    "    confianza: float\n",
    "\n",
    "\n",
    "def nodo_router(state: RouterState) -> dict:\n",
    "    \"\"\"Clasifica la query y decide la ruta.\"\"\"\n",
    "    query = state[\"messages\"][-1].content\n",
    "    decision = router_llm.invoke([\n",
    "        SystemMessage(content=ROUTER_PROMPT),\n",
    "        HumanMessage(content=query),\n",
    "    ])\n",
    "    return {\"ruta\": decision.ruta, \"confianza\": decision.confianza}\n",
    "\n",
    "\n",
    "def nodo_batman(state: RouterState) -> dict:\n",
    "    \"\"\"Especialista en Batman.\"\"\"\n",
    "    comics = cargar_comics(\"batman\")\n",
    "    contexto = crear_contexto(comics)\n",
    "    query = state[\"messages\"][-1].content if isinstance(state[\"messages\"][-1], HumanMessage) else state[\"messages\"][-1].content\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=f\"{PROMPTS_ESPECIALISTAS['batman']}\\n\\nContexto:\\n{contexto}\"),\n",
    "        HumanMessage(content=query),\n",
    "    ])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def nodo_spiderman(state: RouterState) -> dict:\n",
    "    \"\"\"Especialista en Spider-Man.\"\"\"\n",
    "    comics = cargar_comics(\"spiderman\")\n",
    "    contexto = crear_contexto(comics)\n",
    "    query = state[\"messages\"][-1].content if isinstance(state[\"messages\"][-1], HumanMessage) else state[\"messages\"][-1].content\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=f\"{PROMPTS_ESPECIALISTAS['spiderman']}\\n\\nContexto:\\n{contexto}\"),\n",
    "        HumanMessage(content=query),\n",
    "    ])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def nodo_comparacion(state: RouterState) -> dict:\n",
    "    \"\"\"Especialista en comparaciones.\"\"\"\n",
    "    batman_comics = cargar_comics(\"batman\")\n",
    "    spider_comics = cargar_comics(\"spiderman\")\n",
    "    contexto = \"BATMAN:\\n\" + crear_contexto(batman_comics, 1500) + \"\\n\\nSPIDER-MAN:\\n\" + crear_contexto(spider_comics, 1500)\n",
    "    query = state[\"messages\"][-1].content if isinstance(state[\"messages\"][-1], HumanMessage) else state[\"messages\"][-1].content\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=f\"{PROMPTS_ESPECIALISTAS['comparacion']}\\n\\nContexto:\\n{contexto}\"),\n",
    "        HumanMessage(content=query),\n",
    "    ])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def decidir_ruta(state: RouterState) -> str:\n",
    "    \"\"\"Funcion de routing condicional.\"\"\"\n",
    "    return state[\"ruta\"]\n",
    "\n",
    "\n",
    "# Construir grafo\n",
    "graph = StateGraph(RouterState)\n",
    "graph.add_node(\"router\", nodo_router)\n",
    "graph.add_node(\"batman\", nodo_batman)\n",
    "graph.add_node(\"spiderman\", nodo_spiderman)\n",
    "graph.add_node(\"comparacion\", nodo_comparacion)\n",
    "\n",
    "graph.add_edge(START, \"router\")\n",
    "graph.add_conditional_edges(\"router\", decidir_ruta, {\n",
    "    \"batman\": \"batman\",\n",
    "    \"spiderman\": \"spiderman\",\n",
    "    \"comparacion\": \"comparacion\",\n",
    "})\n",
    "graph.add_edge(\"batman\", END)\n",
    "graph.add_edge(\"spiderman\", END)\n",
    "graph.add_edge(\"comparacion\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "print(\"Grafo de routing compilado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PRUEBAS DEL ROUTING\n",
    "# ============================================================\n",
    "\n",
    "queries_test = [\n",
    "    \"Que paso en The Killing Joke?\",\n",
    "    \"Como funciona el sentido aracnido?\",\n",
    "    \"Quien tiene mejor filosofia de vida, Batman o Spider-Man?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRUEBAS DE ROUTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in queries_test:\n",
    "    result = app.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "    ruta = result.get(\"ruta\", \"?\")\n",
    "    respuesta = result[\"messages\"][-1].content\n",
    "    \n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Ruta: {ruta}\")\n",
    "    print(f\"Respuesta: {respuesta[:300]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000010",
   "metadata": {},
   "source": [
    "## 4. Visualizacion del Grafo\n",
    "\n",
    "LangGraph soporta exportar el grafo como Mermaid diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZACION MERMAID\n",
    "# ============================================================\n",
    "\n",
    "mermaid = app.get_graph().draw_mermaid()\n",
    "print(\"Diagrama Mermaid del grafo:\\n\")\n",
    "print(mermaid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1000012",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1. **Routing** clasifica el input antes de procesarlo, permitiendo especialistas dedicados\n",
    "2. **Pydantic** garantiza que la decision de routing sea estructurada y validada\n",
    "3. **Conditional edges** en LangGraph mapean strings a nodos destino\n",
    "4. El routing agrega una llamada LLM adicional, pero mejora la calidad de respuesta\n",
    "5. La confianza del router puede usarse para implementar fallbacks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}