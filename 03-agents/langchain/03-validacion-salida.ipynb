{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1000001",
   "metadata": {},
   "source": [
    "# 03 \u2014 Validacion de Salida\n",
    "\n",
    "**Objetivo**: Garantizar que las respuestas del agente cumplan con schemas Pydantic, implementar retry loops y guardrails de contenido.\n",
    "\n",
    "## Contenido\n",
    "1. Output validation con Pydantic\n",
    "2. Retry loop en LangGraph (max 3 intentos)\n",
    "3. Guardrails de contenido\n",
    "4. Metricas: valido al primer intento vs retry vs fallo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VALIDACION DE SALIDA\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000003",
   "metadata": {},
   "source": [
    "## 1. Schemas Pydantic para Output Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SCHEMAS DE VALIDACION\n",
    "# ============================================================\n",
    "\n",
    "class ComicResponse(BaseModel):\n",
    "    \"\"\"Respuesta validada sobre un comic.\"\"\"\n",
    "    personaje: str = Field(description=\"Nombre del personaje principal\")\n",
    "    arco_narrativo: str = Field(description=\"Nombre del arco o saga\")\n",
    "    resumen: str = Field(description=\"Resumen de la respuesta (50-200 palabras)\")\n",
    "    fuentes_citadas: list[str] = Field(description=\"Lista de fuentes o arcos referenciados\", min_length=1)\n",
    "    confianza: float = Field(description=\"Confianza en la respuesta (0.0 a 1.0)\", ge=0.0, le=1.0)\n",
    "\n",
    "    @field_validator(\"resumen\")\n",
    "    @classmethod\n",
    "    def resumen_no_vacio(cls, v: str) -> str:\n",
    "        if len(v.split()) < 10:\n",
    "            raise ValueError(\"El resumen debe tener al menos 10 palabras\")\n",
    "        return v\n",
    "\n",
    "\n",
    "# Test: el modelo puede generar respuestas estructuradas\n",
    "structured_llm = llm.with_structured_output(ComicResponse)\n",
    "\n",
    "response = structured_llm.invoke(\n",
    "    \"Describe el arco de Knightfall de Batman en detalle\"\n",
    ")\n",
    "\n",
    "print(\"Respuesta validada:\")\n",
    "print(f\"  Personaje: {response.personaje}\")\n",
    "print(f\"  Arco: {response.arco_narrativo}\")\n",
    "print(f\"  Resumen: {response.resumen[:100]}...\")\n",
    "print(f\"  Fuentes: {response.fuentes_citadas}\")\n",
    "print(f\"  Confianza: {response.confianza:.1%}\")\n",
    "print(f\"  Tipo: {type(response).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000005",
   "metadata": {},
   "source": [
    "## 2. Retry Loop en LangGraph\n",
    "\n",
    "Si la respuesta no pasa validacion, el agente reintenta con feedback sobre el error.\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Generar  \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Validar   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502 Decidir  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "     \u25b2                                   \u2502\n",
    "     \u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    valido  \u2502  invalido\n",
    "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502  Corregir \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     (max 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RETRY LOOP CON LANGGRAPH\n",
    "# ============================================================\n",
    "\n",
    "class ValidationState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    query: str\n",
    "    respuesta_raw: str\n",
    "    respuesta_validada: dict | None\n",
    "    intentos: int\n",
    "    errores: list[str]\n",
    "    status: str  # \"pending\", \"valid\", \"failed\"\n",
    "\n",
    "\n",
    "def nodo_generar(state: ValidationState) -> dict:\n",
    "    \"\"\"Genera una respuesta del LLM.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    errores_previos = state.get(\"errores\", [])\n",
    "\n",
    "    system = \"Eres un experto en comics. Responde en espa\\u00f1ol con informacion precisa.\"\n",
    "    if errores_previos:\n",
    "        feedback = \"\\n\".join(f\"- {e}\" for e in errores_previos[-2:])\n",
    "        system += f\"\\n\\nINTENTOS ANTERIORES FALLARON. Corrige estos errores:\\n{feedback}\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system),\n",
    "        HumanMessage(content=query),\n",
    "    ])\n",
    "    return {\n",
    "        \"respuesta_raw\": response.content,\n",
    "        \"intentos\": state.get(\"intentos\", 0) + 1,\n",
    "    }\n",
    "\n",
    "\n",
    "def nodo_validar(state: ValidationState) -> dict:\n",
    "    \"\"\"Valida la respuesta contra el schema Pydantic.\"\"\"\n",
    "    try:\n",
    "        response = structured_llm.invoke(\n",
    "            f\"Estructura esta informacion sobre comics:\\n{state['respuesta_raw']}\"\n",
    "        )\n",
    "        return {\n",
    "            \"respuesta_validada\": response.model_dump(),\n",
    "            \"status\": \"valid\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"errores\": state.get(\"errores\", []) + [str(e)],\n",
    "            \"status\": \"invalid\",\n",
    "        }\n",
    "\n",
    "\n",
    "def decidir_retry(state: ValidationState) -> str:\n",
    "    \"\"\"Decide si reintentar o terminar.\"\"\"\n",
    "    if state.get(\"status\") == \"valid\":\n",
    "        return \"fin\"\n",
    "    if state.get(\"intentos\", 0) >= 3:\n",
    "        return \"fallo\"\n",
    "    return \"reintentar\"\n",
    "\n",
    "\n",
    "# Construir grafo\n",
    "graph = StateGraph(ValidationState)\n",
    "graph.add_node(\"generar\", nodo_generar)\n",
    "graph.add_node(\"validar\", nodo_validar)\n",
    "\n",
    "graph.add_edge(START, \"generar\")\n",
    "graph.add_edge(\"generar\", \"validar\")\n",
    "graph.add_conditional_edges(\"validar\", decidir_retry, {\n",
    "    \"fin\": END,\n",
    "    \"fallo\": END,\n",
    "    \"reintentar\": \"generar\",\n",
    "})\n",
    "\n",
    "retry_app = graph.compile()\n",
    "print(\"Grafo de retry compilado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TEST DEL RETRY LOOP\n",
    "# ============================================================\n",
    "\n",
    "result = retry_app.invoke({\n",
    "    \"messages\": [],\n",
    "    \"query\": \"Explica el arco de Venom en Spider-Man\",\n",
    "    \"respuesta_raw\": \"\",\n",
    "    \"respuesta_validada\": None,\n",
    "    \"intentos\": 0,\n",
    "    \"errores\": [],\n",
    "    \"status\": \"pending\",\n",
    "})\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTADO DEL RETRY LOOP\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Status: {result['status']}\")\n",
    "print(f\"Intentos: {result['intentos']}\")\n",
    "print(f\"Errores acumulados: {len(result.get('errores', []))}\")\n",
    "if result.get(\"respuesta_validada\"):\n",
    "    print(f\"\\nRespuesta validada:\")\n",
    "    for k, v in result[\"respuesta_validada\"].items():\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000008",
   "metadata": {},
   "source": [
    "## 3. Guardrails de Contenido\n",
    "\n",
    "Ademas de validacion de formato, necesitamos validar el **contenido** de la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDRAILS DE CONTENIDO\n",
    "# ============================================================\n",
    "\n",
    "class ContentGuardrail(BaseModel):\n",
    "    \"\"\"Evaluacion de calidad del contenido.\"\"\"\n",
    "    es_relevante: bool = Field(description=\"La respuesta es relevante a la pregunta\")\n",
    "    contiene_alucinacion: bool = Field(description=\"La respuesta contiene informacion inventada\")\n",
    "    es_segura: bool = Field(description=\"La respuesta no contiene contenido danino\")\n",
    "    score_calidad: int = Field(description=\"Score de calidad 1-5\", ge=1, le=5)\n",
    "\n",
    "\n",
    "guardrail_llm = llm.with_structured_output(ContentGuardrail)\n",
    "\n",
    "\n",
    "def evaluar_contenido(pregunta: str, respuesta: str) -> ContentGuardrail:\n",
    "    \"\"\"Evalua la calidad del contenido de una respuesta.\"\"\"\n",
    "    prompt = f\"\"\"Evalua la siguiente respuesta:\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "Respuesta: {respuesta}\n",
    "\n",
    "Determina:\n",
    "1. Si la respuesta es relevante a la pregunta\n",
    "2. Si contiene alucinaciones (informacion inventada o falsa)\n",
    "3. Si es segura (no contiene contenido danino)\n",
    "4. Score de calidad general (1-5)\n",
    "\"\"\"\n",
    "    return guardrail_llm.invoke(prompt)\n",
    "\n",
    "\n",
    "# Test con respuesta correcta\n",
    "eval_ok = evaluar_contenido(\n",
    "    \"Quien es el Joker?\",\n",
    "    \"El Joker es el archivillano de Batman, conocido por su apariencia de payaso, su risa maniaca, y su filosofia del caos.\"\n",
    ")\n",
    "print(\"Evaluacion de respuesta correcta:\")\n",
    "print(f\"  Relevante: {eval_ok.es_relevante}\")\n",
    "print(f\"  Alucinacion: {eval_ok.contiene_alucinacion}\")\n",
    "print(f\"  Segura: {eval_ok.es_segura}\")\n",
    "print(f\"  Score: {eval_ok.score_calidad}/5\")\n",
    "\n",
    "# Test con respuesta alucinada\n",
    "eval_bad = evaluar_contenido(\n",
    "    \"Quien es el Joker?\",\n",
    "    \"El Joker fue creado en 1975 por Stan Lee como villano de Spider-Man. Su nombre real es Jack Napier Lopez y nacio en Mexico.\"\n",
    ")\n",
    "print(\"\\nEvaluacion de respuesta alucinada:\")\n",
    "print(f\"  Relevante: {eval_bad.es_relevante}\")\n",
    "print(f\"  Alucinacion: {eval_bad.contiene_alucinacion}\")\n",
    "print(f\"  Segura: {eval_bad.es_segura}\")\n",
    "print(f\"  Score: {eval_bad.score_calidad}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000010",
   "metadata": {},
   "source": [
    "## 4. Metricas de Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# METRICAS DE VALIDACION (batch)\n",
    "# ============================================================\n",
    "\n",
    "queries_test = [\n",
    "    \"Que es la Batcueva?\",\n",
    "    \"Como funciona el sentido aracnido?\",\n",
    "    \"Explica el arco de Year One.\",\n",
    "    \"Que paso en Civil War con Spider-Man?\",\n",
    "    \"Quien es Catwoman?\",\n",
    "]\n",
    "\n",
    "resultados = {\"primer_intento\": 0, \"retry\": 0, \"fallo\": 0}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BATCH VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in queries_test:\n",
    "    result = retry_app.invoke({\n",
    "        \"messages\": [],\n",
    "        \"query\": query,\n",
    "        \"respuesta_raw\": \"\",\n",
    "        \"respuesta_validada\": None,\n",
    "        \"intentos\": 0,\n",
    "        \"errores\": [],\n",
    "        \"status\": \"pending\",\n",
    "    })\n",
    "\n",
    "    if result[\"status\"] == \"valid\":\n",
    "        if result[\"intentos\"] == 1:\n",
    "            resultados[\"primer_intento\"] += 1\n",
    "            tag = \"1er intento\"\n",
    "        else:\n",
    "            resultados[\"retry\"] += 1\n",
    "            tag = f\"retry ({result['intentos']} intentos)\"\n",
    "    else:\n",
    "        resultados[\"fallo\"] += 1\n",
    "        tag = \"FALLO\"\n",
    "\n",
    "    print(f\"  [{tag:15s}] {query}\")\n",
    "\n",
    "print(f\"\\nResumen:\")\n",
    "print(f\"  Valido al 1er intento: {resultados['primer_intento']}/{len(queries_test)}\")\n",
    "print(f\"  Necesito retry:        {resultados['retry']}/{len(queries_test)}\")\n",
    "print(f\"  Fallo permanente:      {resultados['fallo']}/{len(queries_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1000012",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1. **Pydantic** valida estructura Y contenido de las respuestas del LLM\n",
    "2. **Retry loops** mejoran la tasa de exito pero agregan latencia y costo\n",
    "3. **Guardrails** son la ultima linea de defensa contra alucinaciones\n",
    "4. En produccion, la combinacion schema + guardrail + retry da >95% de respuestas validas\n",
    "5. Siempre definir un `max_retries` para evitar loops infinitos y costos descontrolados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}