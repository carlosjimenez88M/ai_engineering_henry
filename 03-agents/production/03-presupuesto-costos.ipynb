{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 03 — Presupuesto y Control de Costos\n",
    "\n",
    "**Objetivo**: Implementar control de costos con limites por request y sesion, terminacion graceful, y estrategias de optimizacion.\n",
    "\n",
    "## Contenido\n",
    "1. Clase `TokenBudget`\n",
    "2. Limites por request y sesion\n",
    "3. Terminacion graceful\n",
    "4. Estrategias de optimizacion\n",
    "5. Dashboard de costos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-5-mini\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PRESUPUESTO Y CONTROL DE COSTOS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASE TokenBudget\n",
    "# ============================================================\n",
    "\n",
    "class TokenBudget:\n",
    "    \"\"\"Control de presupuesto de tokens por request y sesion.\"\"\"\n",
    "    \n",
    "    PRICING = {\n",
    "        \"gpt-5-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-5\": {\"input\": 2.00, \"output\": 8.00},\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-5-mini\",\n",
    "        max_tokens_per_request: int = 4000,\n",
    "        max_tokens_per_session: int = 50000,\n",
    "        max_cost_per_session: float = 0.10,  # USD\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.max_tokens_per_request = max_tokens_per_request\n",
    "        self.max_tokens_per_session = max_tokens_per_session\n",
    "        self.max_cost_per_session = max_cost_per_session\n",
    "        \n",
    "        self.session_tokens = 0\n",
    "        self.session_cost = 0.0\n",
    "        self.requests: list[dict] = []\n",
    "    \n",
    "    def check_budget(self, estimated_tokens: int = 0) -> dict:\n",
    "        \"\"\"Verifica si hay presupuesto disponible.\"\"\"\n",
    "        remaining_tokens = self.max_tokens_per_session - self.session_tokens\n",
    "        remaining_cost = self.max_cost_per_session - self.session_cost\n",
    "        \n",
    "        can_proceed = (\n",
    "            remaining_tokens > 0\n",
    "            and remaining_cost > 0\n",
    "            and (estimated_tokens == 0 or estimated_tokens <= self.max_tokens_per_request)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"can_proceed\": can_proceed,\n",
    "            \"remaining_tokens\": remaining_tokens,\n",
    "            \"remaining_cost_usd\": round(remaining_cost, 6),\n",
    "            \"session_usage_pct\": round(self.session_tokens / self.max_tokens_per_session * 100, 1),\n",
    "            \"cost_usage_pct\": round(self.session_cost / self.max_cost_per_session * 100, 1),\n",
    "        }\n",
    "    \n",
    "    def record(self, input_tokens: int, output_tokens: int, label: str = \"\") -> dict:\n",
    "        \"\"\"Registra el uso de una request.\"\"\"\n",
    "        prices = self.PRICING.get(self.model, self.PRICING[\"gpt-5-mini\"])\n",
    "        cost = input_tokens * prices[\"input\"] / 1_000_000 + output_tokens * prices[\"output\"] / 1_000_000\n",
    "        \n",
    "        total_tokens = input_tokens + output_tokens\n",
    "        self.session_tokens += total_tokens\n",
    "        self.session_cost += cost\n",
    "        \n",
    "        entry = {\n",
    "            \"label\": label,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"cost_usd\": round(cost, 6),\n",
    "            \"cumulative_tokens\": self.session_tokens,\n",
    "            \"cumulative_cost\": round(self.session_cost, 6),\n",
    "        }\n",
    "        self.requests.append(entry)\n",
    "        return entry\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        budget_check = self.check_budget()\n",
    "        return (\n",
    "            f\"Requests: {len(self.requests)} | \"\n",
    "            f\"Tokens: {self.session_tokens:,}/{self.max_tokens_per_session:,} ({budget_check['session_usage_pct']:.1f}%) | \"\n",
    "            f\"Costo: ${self.session_cost:.6f}/${self.max_cost_per_session} ({budget_check['cost_usage_pct']:.1f}%)\"\n",
    "        )\n",
    "\n",
    "\n",
    "budget = TokenBudget(\n",
    "    model=MODEL,\n",
    "    max_tokens_per_request=4000,\n",
    "    max_tokens_per_session=50000,\n",
    "    max_cost_per_session=0.05,\n",
    ")\n",
    "\n",
    "print(\"TokenBudget inicializado:\")\n",
    "print(f\"  Max tokens/request: {budget.max_tokens_per_request:,}\")\n",
    "print(f\"  Max tokens/sesion: {budget.max_tokens_per_session:,}\")\n",
    "print(f\"  Max costo/sesion: ${budget.max_cost_per_session}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLAMADAS CON PRESUPUESTO\n",
    "# ============================================================\n",
    "\n",
    "def llm_call_budgeted(prompt: str, budget: TokenBudget, label: str = \"\") -> dict:\n",
    "    \"\"\"Llamada a LLM con control de presupuesto.\"\"\"\n",
    "    # Check budget\n",
    "    check = budget.check_budget()\n",
    "    if not check[\"can_proceed\"]:\n",
    "        return {\n",
    "            \"status\": \"budget_exceeded\",\n",
    "            \"content\": f\"Presupuesto agotado. Tokens restantes: {check['remaining_tokens']}, Costo restante: ${check['remaining_cost_usd']}\",\n",
    "        }\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Responde en español, brevemente.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    \n",
    "    # Record usage\n",
    "    entry = budget.record(\n",
    "        response.usage.prompt_tokens,\n",
    "        response.usage.completion_tokens,\n",
    "        label=label,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"content\": response.choices[0].message.content,\n",
    "        **entry,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test: 10 llamadas\n",
    "print(\"=\" * 60)\n",
    "print(\"LLAMADAS CON PRESUPUESTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "preguntas = [\n",
    "    \"Que es Batman?\",\n",
    "    \"Que es Spider-Man?\",\n",
    "    \"Que es un agente de IA?\",\n",
    "    \"Que es RAG?\",\n",
    "    \"Que es LangGraph?\",\n",
    "    \"Que es ChromaDB?\",\n",
    "    \"Que es un embedding?\",\n",
    "    \"Que es un transformer?\",\n",
    "    \"Que es fine-tuning?\",\n",
    "    \"Que es prompt engineering?\",\n",
    "]\n",
    "\n",
    "for i, pregunta in enumerate(preguntas):\n",
    "    result = llm_call_budgeted(pregunta, budget, label=f\"q{i+1}\")\n",
    "    if result[\"status\"] == \"budget_exceeded\":\n",
    "        print(f\"  [{i+1:2d}] PRESUPUESTO AGOTADO\")\n",
    "        break\n",
    "    print(f\"  [{i+1:2d}] {result['total_tokens']:4d} tokens | ${result['cost_usd']:.6f} | acum: ${result['cumulative_cost']:.6f}\")\n",
    "\n",
    "print(f\"\\n{budget.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8d9c0",
   "metadata": {},
   "source": [
    "## 2. Estrategias de Optimizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTRATEGIAS DE OPTIMIZACION\n",
    "# ============================================================\n",
    "\n",
    "# 1. Cache simple (misma pregunta → misma respuesta)\n",
    "response_cache: dict[str, str] = {}\n",
    "\n",
    "def llm_call_cached(prompt: str) -> dict:\n",
    "    \"\"\"Llamada con cache.\"\"\"\n",
    "    if prompt in response_cache:\n",
    "        return {\"status\": \"cache_hit\", \"content\": response_cache[prompt], \"tokens\": 0, \"cost\": 0}\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    response_cache[prompt] = content\n",
    "    return {\n",
    "        \"status\": \"cache_miss\",\n",
    "        \"content\": content,\n",
    "        \"tokens\": response.usage.total_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "# 2. Prompt corto vs largo\n",
    "def comparar_prompts(pregunta: str) -> dict:\n",
    "    \"\"\"Compara prompts cortos vs largos.\"\"\"\n",
    "    # Prompt largo\n",
    "    r_largo = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un asistente experto en comics de superheroes. Tu especialidad incluye Batman de DC Comics y Spider-Man de Marvel Comics. Responde de forma detallada, citando arcos narrativos y proporcionando contexto historico cuando sea relevante. Responde siempre en español.\"},\n",
    "            {\"role\": \"user\", \"content\": pregunta},\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    \n",
    "    # Prompt corto\n",
    "    r_corto = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Experto en comics. Responde en español.\"},\n",
    "            {\"role\": \"user\", \"content\": pregunta},\n",
    "        ],\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"prompt_largo\": {\"tokens\": r_largo.usage.total_tokens, \"content\": r_largo.choices[0].message.content[:100]},\n",
    "        \"prompt_corto\": {\"tokens\": r_corto.usage.total_tokens, \"content\": r_corto.choices[0].message.content[:100]},\n",
    "        \"ahorro_tokens\": r_largo.usage.total_tokens - r_corto.usage.total_tokens,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"=\" * 60)\n",
    "print(\"ESTRATEGIAS DE OPTIMIZACION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cache test\n",
    "print(\"\\n1. CACHE:\")\n",
    "for i in range(3):\n",
    "    r = llm_call_cached(\"Que es Batman?\")\n",
    "    print(f\"  Intento {i+1}: {r['status']:12s} | tokens: {r['tokens']}\")\n",
    "\n",
    "# Prompt comparison\n",
    "print(\"\\n2. PROMPT CORTO vs LARGO:\")\n",
    "comp = comparar_prompts(\"Quien es el Joker?\")\n",
    "print(f\"  Largo: {comp['prompt_largo']['tokens']} tokens\")\n",
    "print(f\"  Corto: {comp['prompt_corto']['tokens']} tokens\")\n",
    "print(f\"  Ahorro: {comp['ahorro_tokens']} tokens ({comp['ahorro_tokens']/max(1,comp['prompt_largo']['tokens'])*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## 3. Dashboard de Costos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DASHBOARD DE COSTOS\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Panel 1: Tokens acumulados por request\n",
    "tokens_acum = [r[\"cumulative_tokens\"] for r in budget.requests]\n",
    "axes[0].plot(range(1, len(tokens_acum) + 1), tokens_acum, \"o-\", color=\"#2196F3\")\n",
    "axes[0].axhline(budget.max_tokens_per_session, color=\"red\", linestyle=\"--\", label=f\"Limite: {budget.max_tokens_per_session:,}\")\n",
    "axes[0].fill_between(range(1, len(tokens_acum) + 1), tokens_acum, alpha=0.1, color=\"#2196F3\")\n",
    "axes[0].set_xlabel(\"Request #\")\n",
    "axes[0].set_ylabel(\"Tokens acumulados\")\n",
    "axes[0].set_title(\"Consumo de Tokens\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Panel 2: Costo acumulado\n",
    "costos_acum = [r[\"cumulative_cost\"] for r in budget.requests]\n",
    "axes[1].plot(range(1, len(costos_acum) + 1), costos_acum, \"s-\", color=\"#4CAF50\")\n",
    "axes[1].axhline(budget.max_cost_per_session, color=\"red\", linestyle=\"--\", label=f\"Limite: ${budget.max_cost_per_session}\")\n",
    "axes[1].fill_between(range(1, len(costos_acum) + 1), costos_acum, alpha=0.1, color=\"#4CAF50\")\n",
    "axes[1].set_xlabel(\"Request #\")\n",
    "axes[1].set_ylabel(\"Costo acumulado (USD)\")\n",
    "axes[1].set_title(\"Costo Acumulado\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Panel 3: Tokens por request\n",
    "tokens_per_req = [r[\"total_tokens\"] for r in budget.requests]\n",
    "axes[2].bar(range(1, len(tokens_per_req) + 1), tokens_per_req, color=\"#FF9800\", alpha=0.8)\n",
    "axes[2].axhline(np.mean(tokens_per_req), color=\"red\", linestyle=\"--\", label=f\"Media: {np.mean(tokens_per_req):.0f}\")\n",
    "axes[2].set_xlabel(\"Request #\")\n",
    "axes[2].set_ylabel(\"Tokens\")\n",
    "axes[2].set_title(\"Tokens por Request\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../data/dashboard_costos.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Resumen final: {budget.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1. **TokenBudget** previene gastos descontrolados con limites por request y sesion\n",
    "2. **Terminacion graceful**: el sistema avisa cuando el presupuesto se agota, no crashea\n",
    "3. **Cache** es la optimizacion mas efectiva para preguntas repetitivas (0 tokens)\n",
    "4. **Prompts cortos** ahorran tokens sin perder calidad significativa\n",
    "5. **Dashboard** de costos es esencial para monitoreo en produccion\n",
    "6. Regla practica: configurar el presupuesto en el **10% del maximo tolerable** como buffer de seguridad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}