{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 04 — Alertas y Monitoreo de Calidad\n",
    "\n",
    "**Objetivo**: Implementar quality signals, ventana deslizante, umbrales de alerta, y tests canary.\n",
    "\n",
    "## Contenido\n",
    "1. Quality signals (judge score, format compliance, hallucination)\n",
    "2. Ventana deslizante de metricas\n",
    "3. Umbrales de alerta\n",
    "4. Tests canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-5-mini\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ALERTAS Y MONITOREO DE CALIDAD\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUALITY SIGNALS\n",
    "# ============================================================\n",
    "\n",
    "class QualityMonitor:\n",
    "    \"\"\"Monitor de calidad con ventana deslizante.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 20):\n",
    "        self.window_size = window_size\n",
    "        self.scores: deque[float] = deque(maxlen=window_size)\n",
    "        self.format_ok: deque[bool] = deque(maxlen=window_size)\n",
    "        self.latencies: deque[float] = deque(maxlen=window_size)\n",
    "        self.history: list[dict] = []\n",
    "        \n",
    "        # Umbrales de alerta\n",
    "        self.thresholds = {\n",
    "            \"min_avg_score\": 3.0,       # Score minimo promedio\n",
    "            \"max_format_fail_rate\": 0.2, # Max 20% de fallos de formato\n",
    "            \"max_p95_latency\": 5000,     # Max 5s en p95\n",
    "        }\n",
    "    \n",
    "    def record(self, score: float, format_valid: bool, latency_ms: float, label: str = \"\"):\n",
    "        \"\"\"Registra una observacion.\"\"\"\n",
    "        self.scores.append(score)\n",
    "        self.format_ok.append(format_valid)\n",
    "        self.latencies.append(latency_ms)\n",
    "        \n",
    "        entry = {\n",
    "            \"label\": label,\n",
    "            \"score\": score,\n",
    "            \"format_valid\": format_valid,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"timestamp\": time.time(),\n",
    "        }\n",
    "        self.history.append(entry)\n",
    "        \n",
    "        # Check alertas\n",
    "        alerts = self.check_alerts()\n",
    "        if alerts:\n",
    "            for alert in alerts:\n",
    "                print(f\"  ALERTA: {alert}\")\n",
    "        \n",
    "        return entry\n",
    "    \n",
    "    def check_alerts(self) -> list[str]:\n",
    "        \"\"\"Verifica umbrales y genera alertas.\"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        if len(self.scores) >= 5:\n",
    "            avg_score = np.mean(list(self.scores))\n",
    "            if avg_score < self.thresholds[\"min_avg_score\"]:\n",
    "                alerts.append(f\"Score promedio bajo: {avg_score:.1f} < {self.thresholds['min_avg_score']}\")\n",
    "        \n",
    "        if len(self.format_ok) >= 5:\n",
    "            fail_rate = 1 - (sum(self.format_ok) / len(self.format_ok))\n",
    "            if fail_rate > self.thresholds[\"max_format_fail_rate\"]:\n",
    "                alerts.append(f\"Tasa de fallo de formato alta: {fail_rate:.0%} > {self.thresholds['max_format_fail_rate']:.0%}\")\n",
    "        \n",
    "        if len(self.latencies) >= 5:\n",
    "            p95 = np.percentile(list(self.latencies), 95)\n",
    "            if p95 > self.thresholds[\"max_p95_latency\"]:\n",
    "                alerts.append(f\"Latencia p95 alta: {p95:.0f}ms > {self.thresholds['max_p95_latency']}ms\")\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def summary(self) -> dict:\n",
    "        if not self.scores:\n",
    "            return {\"status\": \"no_data\"}\n",
    "        return {\n",
    "            \"window_size\": len(self.scores),\n",
    "            \"avg_score\": round(np.mean(list(self.scores)), 2),\n",
    "            \"format_pass_rate\": round(sum(self.format_ok) / len(self.format_ok) * 100, 1),\n",
    "            \"latency_p50\": round(np.percentile(list(self.latencies), 50), 0),\n",
    "            \"latency_p95\": round(np.percentile(list(self.latencies), 95), 0),\n",
    "        }\n",
    "\n",
    "\n",
    "monitor = QualityMonitor(window_size=20)\n",
    "print(\"QualityMonitor inicializado\")\n",
    "print(f\"Umbrales: {monitor.thresholds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SIMULACION DE MONITOREO\n",
    "# ============================================================\n",
    "\n",
    "preguntas_monitor = [\n",
    "    \"Que es Batman?\",\n",
    "    \"Explica el sentido aracnido\",\n",
    "    \"Que paso en Year One?\",\n",
    "    \"Quien es Venom?\",\n",
    "    \"Que es el Tribunal de los Buhos?\",\n",
    "    \"Como murio Gwen Stacy?\",\n",
    "    \"Que es la Batcueva?\",\n",
    "    \"Explica la Saga del Clon\",\n",
    "    \"Que filosofia tiene Batman?\",\n",
    "    \"Describe los poderes de Spider-Man\",\n",
    "    \"Que es Knightfall?\",\n",
    "    \"Como funciona Civil War?\",\n",
    "    \"Quien es Catwoman?\",\n",
    "    \"Que es Spider-Verse?\",\n",
    "    \"Describe la Liga de la Justicia\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SIMULACION DE MONITOREO (15 requests)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, pregunta in enumerate(preguntas_monitor):\n",
    "    t0 = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Responde en 2-3 oraciones sobre comics, en español.\"},\n",
    "            {\"role\": \"user\", \"content\": pregunta},\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    latencia = (time.time() - t0) * 1000\n",
    "    \n",
    "    respuesta = response.choices[0].message.content\n",
    "    \n",
    "    # Evaluar calidad (simplificado)\n",
    "    score = min(5, max(1, len(respuesta.split()) // 10))  # Heuristico simple\n",
    "    format_ok = len(respuesta) > 20 and len(respuesta) < 2000\n",
    "    \n",
    "    monitor.record(score, format_ok, latencia, label=f\"q{i+1}\")\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        s = monitor.summary()\n",
    "        print(f\"\\n  Checkpoint {i+1}: avg_score={s['avg_score']}, format_pass={s['format_pass_rate']}%, p50={s['latency_p50']}ms\")\n",
    "\n",
    "print(f\"\\nResumen final: {monitor.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e8d9c0",
   "metadata": {},
   "source": [
    "## 2. Tests Canary\n",
    "\n",
    "Preguntas con respuesta conocida para detectar regresiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TESTS CANARY\n",
    "# ============================================================\n",
    "\n",
    "canary_tests = [\n",
    "    {\n",
    "        \"pregunta\": \"En que ciudad opera Batman?\",\n",
    "        \"keywords_esperadas\": [\"gotham\"],\n",
    "        \"tipo\": \"factual\",\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"Cual es el nombre real de Spider-Man?\",\n",
    "        \"keywords_esperadas\": [\"peter\", \"parker\"],\n",
    "        \"tipo\": \"factual\",\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"Que regla fundamental tiene Batman sobre matar?\",\n",
    "        \"keywords_esperadas\": [\"no matar\", \"no mata\"],\n",
    "        \"tipo\": \"conceptual\",\n",
    "    },\n",
    "    {\n",
    "        \"pregunta\": \"Quien es el tio de Peter Parker?\",\n",
    "        \"keywords_esperadas\": [\"ben\"],\n",
    "        \"tipo\": \"factual\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def run_canary_tests(tests: list[dict]) -> dict:\n",
    "    \"\"\"Ejecuta tests canary y reporta resultados.\"\"\"\n",
    "    results = {\"passed\": 0, \"failed\": 0, \"details\": []}\n",
    "    \n",
    "    for test in tests:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Responde en español, brevemente.\"},\n",
    "                {\"role\": \"user\", \"content\": test[\"pregunta\"]},\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "        )\n",
    "        \n",
    "        respuesta = response.choices[0].message.content.lower()\n",
    "        keywords_found = any(kw in respuesta for kw in test[\"keywords_esperadas\"])\n",
    "        \n",
    "        if keywords_found:\n",
    "            results[\"passed\"] += 1\n",
    "            status = \"PASS\"\n",
    "        else:\n",
    "            results[\"failed\"] += 1\n",
    "            status = \"FAIL\"\n",
    "        \n",
    "        results[\"details\"].append({\n",
    "            \"pregunta\": test[\"pregunta\"],\n",
    "            \"status\": status,\n",
    "            \"respuesta\": respuesta[:100],\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TESTS CANARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "canary_results = run_canary_tests(canary_tests)\n",
    "\n",
    "for detail in canary_results[\"details\"]:\n",
    "    print(f\"  [{detail['status']:4s}] {detail['pregunta']}\")\n",
    "    print(f\"         {detail['respuesta'][:80]}...\")\n",
    "\n",
    "print(f\"\\nResultado: {canary_results['passed']}/{len(canary_tests)} passed\")\n",
    "\n",
    "if canary_results[\"failed\"] > 0:\n",
    "    print(\"  ALERTA: Tests canary fallidos detectados!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6c7d8",
   "metadata": {},
   "source": [
    "## 3. Dashboard de Calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DASHBOARD DE CALIDAD\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Panel 1: Score de calidad over time\n",
    "scores = [h[\"score\"] for h in monitor.history]\n",
    "axes[0].plot(range(1, len(scores) + 1), scores, \"o-\", color=\"#2196F3\", markersize=4)\n",
    "axes[0].axhline(monitor.thresholds[\"min_avg_score\"], color=\"red\", linestyle=\"--\", label=f\"Umbral: {monitor.thresholds['min_avg_score']}\")\n",
    "avg = np.mean(scores)\n",
    "axes[0].axhline(avg, color=\"green\", linestyle=\":\", label=f\"Media: {avg:.1f}\")\n",
    "axes[0].set_xlabel(\"Request #\")\n",
    "axes[0].set_ylabel(\"Quality Score\")\n",
    "axes[0].set_title(\"Quality Score (ventana deslizante)\")\n",
    "axes[0].set_ylim(0, 5.5)\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# Panel 2: Latencia over time\n",
    "lats = [h[\"latency_ms\"] for h in monitor.history]\n",
    "axes[1].fill_between(range(1, len(lats) + 1), lats, alpha=0.3, color=\"#FF9800\")\n",
    "axes[1].plot(range(1, len(lats) + 1), lats, \"-\", color=\"#FF9800\")\n",
    "axes[1].axhline(monitor.thresholds[\"max_p95_latency\"], color=\"red\", linestyle=\"--\", label=f\"Umbral p95: {monitor.thresholds['max_p95_latency']}ms\")\n",
    "axes[1].set_xlabel(\"Request #\")\n",
    "axes[1].set_ylabel(\"Latencia (ms)\")\n",
    "axes[1].set_title(\"Latencia por Request\")\n",
    "axes[1].legend(fontsize=8)\n",
    "\n",
    "# Panel 3: Canary results\n",
    "labels = [d[\"pregunta\"][:20] + \"...\" for d in canary_results[\"details\"]]\n",
    "colors = [\"#4CAF50\" if d[\"status\"] == \"PASS\" else \"#FF5722\" for d in canary_results[\"details\"]]\n",
    "axes[2].barh(labels, [1] * len(labels), color=colors, alpha=0.8)\n",
    "axes[2].set_xlabel(\"Status\")\n",
    "axes[2].set_title(\"Tests Canary\")\n",
    "axes[2].set_xlim(0, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../data/dashboard_calidad.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1. **Quality signals** cuantifican la salud del sistema en tiempo real\n",
    "2. **Ventana deslizante** suaviza el ruido y detecta tendencias\n",
    "3. **Umbrales de alerta** automatizan la deteccion de degradacion\n",
    "4. **Tests canary** son preguntas con respuesta conocida que detectan regresiones\n",
    "5. Ejecutar canary tests **periodicamente** (cada N requests o cada hora)\n",
    "6. **Dashboard** combina todas las metricas para toma de decisiones\n",
    "7. En produccion, conectar alertas a Slack/PagerDuty para respuesta inmediata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}