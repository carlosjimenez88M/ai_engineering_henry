{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Costo, Latencia y Alucinacion\n",
    "\n",
    "**Objetivo**: Cuantificar las tres metricas criticas de cualquier sistema LLM: cuanto cuesta, cuanto tarda, y cuanto alucina.\n",
    "\n",
    "## Contenido\n",
    "1. Token economics de gpt-5-mini\n",
    "2. Clase `CostTracker`\n",
    "3. Distribucion de latencia (p50/p95/p99)\n",
    "4. Alucinacion sin/con contexto (RAG basico)\n",
    "5. LLM-as-judge para hallucination scoring\n",
    "6. Dashboard con matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "MODEL = \"gpt-5-mini\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"METRICAS: COSTO, LATENCIA, ALUCINACION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Token Economics\n",
    "\n",
    "Modelo `gpt-5-mini`:\n",
    "- Input: $0.15 / 1M tokens\n",
    "- Output: $0.60 / 1M tokens\n",
    "- Cached input: $0.075 / 1M tokens\n",
    "\n",
    "Referencia para comparar:\n",
    "- `gpt-5`: $2.00 / $8.00 por 1M tokens (13x mas caro)\n",
    "- `gpt-4o-mini`: $0.15 / $0.60 por 1M tokens (referencia anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CostTracker: rastreo de costos por request\n",
    "# ============================================================\n",
    "\n",
    "class CostTracker:\n",
    "    \"\"\"Rastrea costos acumulados de llamadas a OpenAI.\"\"\"\n",
    "\n",
    "    # Precios por millon de tokens\n",
    "    PRICING = {\n",
    "        \"gpt-5-mini\": {\"input\": 0.15, \"output\": 0.60},\n",
    "        \"gpt-5\": {\"input\": 2.00, \"output\": 8.00},\n",
    "    }\n",
    "\n",
    "    def __init__(self, model: str = \"gpt-5-mini\"):\n",
    "        self.model = model\n",
    "        self.calls: list[dict] = []\n",
    "\n",
    "    def track(self, usage, latency_ms: float, label: str = \"\") -> dict:\n",
    "        \"\"\"Registra una llamada y calcula costo.\"\"\"\n",
    "        prices = self.PRICING.get(self.model, self.PRICING[\"gpt-5-mini\"])\n",
    "        costo_input = usage.prompt_tokens * prices[\"input\"] / 1_000_000\n",
    "        costo_output = usage.completion_tokens * prices[\"output\"] / 1_000_000\n",
    "\n",
    "        entry = {\n",
    "            \"label\": label,\n",
    "            \"input_tokens\": usage.prompt_tokens,\n",
    "            \"output_tokens\": usage.completion_tokens,\n",
    "            \"total_tokens\": usage.total_tokens,\n",
    "            \"costo_input\": costo_input,\n",
    "            \"costo_output\": costo_output,\n",
    "            \"costo_total\": costo_input + costo_output,\n",
    "            \"latencia_ms\": latency_ms,\n",
    "        }\n",
    "        self.calls.append(entry)\n",
    "        return entry\n",
    "\n",
    "    @property\n",
    "    def total_costo(self) -> float:\n",
    "        return sum(c[\"costo_total\"] for c in self.calls)\n",
    "\n",
    "    @property\n",
    "    def total_tokens(self) -> int:\n",
    "        return sum(c[\"total_tokens\"] for c in self.calls)\n",
    "\n",
    "    def resumen(self) -> str:\n",
    "        latencias = [c[\"latencia_ms\"] for c in self.calls]\n",
    "        return (\n",
    "            f\"Llamadas: {len(self.calls)} | \"\n",
    "            f\"Tokens: {self.total_tokens:,} | \"\n",
    "            f\"Costo: ${self.total_costo:.6f} | \"\n",
    "            f\"Latencia media: {np.mean(latencias):.0f}ms\"\n",
    "        )\n",
    "\n",
    "\n",
    "tracker = CostTracker(MODEL)\n",
    "print(\"CostTracker inicializado\")\n",
    "print(f\"Precios {MODEL}: {CostTracker.PRICING[MODEL]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Distribucion de Latencia\n",
    "\n",
    "Ejecutamos N llamadas identicas para medir la variabilidad de latencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MEDICION DE LATENCIA (10 llamadas)\n",
    "# ============================================================\n",
    "\n",
    "preguntas = [\n",
    "    \"Que es un transformer?\",\n",
    "    \"Explica backpropagation en una oracion.\",\n",
    "    \"Que es attention en deep learning?\",\n",
    "    \"Define overfitting.\",\n",
    "    \"Que es gradient descent?\",\n",
    "    \"Que es un embedding?\",\n",
    "    \"Que es fine-tuning?\",\n",
    "    \"Que es tokenizacion?\",\n",
    "    \"Que es un prompt?\",\n",
    "    \"Que es inference en ML?\",\n",
    "]\n",
    "\n",
    "print(\"Ejecutando 10 llamadas para medir latencia...\\n\")\n",
    "\n",
    "for i, pregunta in enumerate(preguntas):\n",
    "    t0 = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Responde en 1-2 oraciones en español.\"},\n",
    "            {\"role\": \"user\", \"content\": pregunta},\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "    )\n",
    "    latencia = (time.time() - t0) * 1000\n",
    "    entry = tracker.track(response.usage, latencia, label=f\"q{i+1}\")\n",
    "    print(f\"  [{i+1:2d}] {latencia:6.0f}ms | {entry['total_tokens']:4d} tokens | ${entry['costo_total']:.6f}\")\n",
    "\n",
    "print(f\"\\n{tracker.resumen()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PERCENTILES DE LATENCIA\n",
    "# ============================================================\n",
    "\n",
    "latencias = [c[\"latencia_ms\"] for c in tracker.calls]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUCION DE LATENCIA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  p50 (mediana):  {np.percentile(latencias, 50):6.0f} ms\")\n",
    "print(f\"  p90:            {np.percentile(latencias, 90):6.0f} ms\")\n",
    "print(f\"  p95:            {np.percentile(latencias, 95):6.0f} ms\")\n",
    "print(f\"  p99:            {np.percentile(latencias, 99):6.0f} ms\")\n",
    "print(f\"  min:            {min(latencias):6.0f} ms\")\n",
    "print(f\"  max:            {max(latencias):6.0f} ms\")\n",
    "print(f\"  std:            {np.std(latencias):6.0f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Alucinacion: Sin Contexto vs Con Contexto\n",
    "\n",
    "Comparamos las respuestas del LLM cuando:\n",
    "- **Sin contexto**: responde solo con sus parametros (propenso a alucinar)\n",
    "- **Con contexto**: recibe informacion de referencia (RAG basico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DEMO DE ALUCINACION\n",
    "# ============================================================\n",
    "\n",
    "contexto_real = \"\"\"\n",
    "LangGraph es una libreria de LangChain para construir aplicaciones con LLMs \n",
    "usando grafos dirigidos. Permite definir nodos (funciones), edges (transiciones), \n",
    "y estados tipados. Fue lanzada en 2024. Soporta ciclos, lo que la diferencia \n",
    "de DAGs tradicionales. Usa StateGraph como clase principal.\n",
    "\"\"\"\n",
    "\n",
    "pregunta_test = \"Quien creo LangGraph y en que año? Que version actual tiene?\"\n",
    "\n",
    "# Sin contexto (propenso a alucinar)\n",
    "t0 = time.time()\n",
    "r_sin = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Responde en español con datos especificos.\"},\n",
    "        {\"role\": \"user\", \"content\": pregunta_test},\n",
    "    ],\n",
    ")\n",
    "lat_sin = (time.time() - t0) * 1000\n",
    "tracker.track(r_sin.usage, lat_sin, label=\"sin_contexto\")\n",
    "\n",
    "# Con contexto (RAG basico)\n",
    "t0 = time.time()\n",
    "r_con = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": f\"Responde SOLO con la informacion del contexto. Si no esta en el contexto, di 'No tengo esa informacion.'\\n\\nContexto:\\n{contexto_real}\"},\n",
    "        {\"role\": \"user\", \"content\": pregunta_test},\n",
    "    ],\n",
    ")\n",
    "lat_con = (time.time() - t0) * 1000\n",
    "tracker.track(r_con.usage, lat_con, label=\"con_contexto\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARACION: SIN vs CON CONTEXTO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nPregunta: {pregunta_test}\")\n",
    "print(f\"\\n--- SIN CONTEXTO (puede alucinar) ---\")\n",
    "print(r_sin.choices[0].message.content)\n",
    "print(f\"\\n--- CON CONTEXTO (RAG basico) ---\")\n",
    "print(r_con.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM-as-Judge: Hallucination Scoring\n",
    "\n",
    "Usamos un segundo LLM para evaluar si la respuesta original alucino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LLM-AS-JUDGE: evaluacion de alucinacion\n",
    "# ============================================================\n",
    "\n",
    "def evaluar_alucinacion(pregunta: str, respuesta: str, contexto: str | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Usa un LLM como juez para evaluar alucinacion.\n",
    "\n",
    "    Returns:\n",
    "        Dict con score (1-5), justificacion, y metricas.\n",
    "    \"\"\"\n",
    "    if contexto:\n",
    "        prompt_juez = f\"\"\"Evalua si la siguiente respuesta contiene alucinaciones \n",
    "(informacion inventada o no respaldada por el contexto).\n",
    "\n",
    "Contexto de referencia:\n",
    "{contexto}\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "Respuesta: {respuesta}\n",
    "\n",
    "Evalua con un score de 1 a 5:\n",
    "1 = Completamente alucinada (todo inventado)\n",
    "2 = Mayormente alucinada\n",
    "3 = Parcialmente correcta, parcialmente alucinada\n",
    "4 = Mayormente correcta\n",
    "5 = Sin alucinacion (todo respaldado por el contexto)\n",
    "\n",
    "Responde en JSON: {{\"score\": N, \"justificacion\": \"...\"}}\"\"\"\n",
    "    else:\n",
    "        prompt_juez = f\"\"\"Evalua la plausibilidad de la siguiente respuesta.\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "Respuesta: {respuesta}\n",
    "\n",
    "Score 1-5 (1=probablemente inventado, 5=probablemente correcto).\n",
    "Responde en JSON: {{\"score\": N, \"justificacion\": \"...\"}}\"\"\"\n",
    "\n",
    "    t0 = time.time()\n",
    "    r = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Eres un evaluador critico. Responde solo en JSON valido.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt_juez},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    lat = (time.time() - t0) * 1000\n",
    "    tracker.track(r.usage, lat, label=\"judge\")\n",
    "\n",
    "    try:\n",
    "        resultado = json.loads(r.choices[0].message.content)\n",
    "    except json.JSONDecodeError:\n",
    "        resultado = {\"score\": 0, \"justificacion\": \"Error parsing JSON\"}\n",
    "\n",
    "    return resultado\n",
    "\n",
    "\n",
    "# Evaluar ambas respuestas\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUACION LLM-AS-JUDGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "eval_sin = evaluar_alucinacion(pregunta_test, r_sin.choices[0].message.content, contexto_real)\n",
    "print(f\"\\nSIN contexto → Score: {eval_sin.get('score', '?')}/5\")\n",
    "print(f\"  Justificacion: {eval_sin.get('justificacion', 'N/A')}\")\n",
    "\n",
    "eval_con = evaluar_alucinacion(pregunta_test, r_con.choices[0].message.content, contexto_real)\n",
    "print(f\"\\nCON contexto → Score: {eval_con.get('score', '?')}/5\")\n",
    "print(f\"  Justificacion: {eval_con.get('justificacion', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dashboard de Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DASHBOARD: 3 paneles\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Panel 1: Distribucion de latencia\n",
    "latencias_q = [c[\"latencia_ms\"] for c in tracker.calls if c[\"label\"].startswith(\"q\")]\n",
    "axes[0].hist(latencias_q, bins=8, color=\"#2196F3\", edgecolor=\"white\", alpha=0.8)\n",
    "axes[0].axvline(np.percentile(latencias_q, 50), color=\"red\", linestyle=\"--\", label=f\"p50={np.percentile(latencias_q, 50):.0f}ms\")\n",
    "axes[0].axvline(np.percentile(latencias_q, 95), color=\"orange\", linestyle=\"--\", label=f\"p95={np.percentile(latencias_q, 95):.0f}ms\")\n",
    "axes[0].set_xlabel(\"Latencia (ms)\")\n",
    "axes[0].set_ylabel(\"Frecuencia\")\n",
    "axes[0].set_title(\"Distribucion de Latencia\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Panel 2: Costo por llamada\n",
    "costos = [c[\"costo_total\"] * 1000 for c in tracker.calls if c[\"label\"].startswith(\"q\")]\n",
    "axes[1].bar(range(1, len(costos) + 1), costos, color=\"#4CAF50\", alpha=0.8)\n",
    "axes[1].set_xlabel(\"Llamada #\")\n",
    "axes[1].set_ylabel(\"Costo (milésimas USD)\")\n",
    "axes[1].set_title(\"Costo por Llamada\")\n",
    "axes[1].axhline(np.mean(costos), color=\"red\", linestyle=\"--\", label=f\"Media={np.mean(costos):.3f}\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Panel 3: Hallucination scores\n",
    "scores = [eval_sin.get(\"score\", 0), eval_con.get(\"score\", 0)]\n",
    "bars = axes[2].bar([\"Sin contexto\", \"Con contexto (RAG)\"], scores, color=[\"#FF5722\", \"#4CAF50\"], alpha=0.8)\n",
    "axes[2].set_ylabel(\"Hallucination Score (1-5)\")\n",
    "axes[2].set_title(\"Alucinacion: Sin vs Con Contexto\")\n",
    "axes[2].set_ylim(0, 5.5)\n",
    "for bar, score in zip(bars, scores):\n",
    "    axes[2].text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.1,\n",
    "                 f\"{score}/5\", ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../data/dashboard_metricas.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResumen final: {tracker.resumen()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1. **Costo**: gpt-5-mini es extremadamente barato (~$0.0001 por llamada simple). Pero en agentes multi-paso, el costo se multiplica.\n",
    "2. **Latencia**: La variabilidad es alta (p95 puede ser 3-5x el p50). Disenar para el peor caso, no el promedio.\n",
    "3. **Alucinacion**: Sin contexto, el LLM inventa datos especificos (versiones, fechas). Con RAG, se limita a lo que sabe.\n",
    "4. **LLM-as-Judge**: Es una herramienta poderosa para evaluacion automatica, pero agrega costo y latencia adicional.\n",
    "5. **Dashboard**: Visualizar metricas es esencial para tomar decisiones informadas sobre arquitectura."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}