{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Resolucion de Conflictos entre Agentes\n",
    "\n",
    "**Objetivo**: Implementar 3 estrategias para resolver discrepancias entre agentes: votacion, debate, y juez.\n",
    "\n",
    "## Contenido\n",
    "1. Votacion por mayoria (3 agentes)\n",
    "2. Debate con rondas hasta convergencia\n",
    "3. Juez evaluador\n",
    "4. Comparacion de estrategias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0.7)  # Temperatura alta para diversidad\n",
    "llm_judge = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)  # Temperatura baja para juez\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESOLUCION DE CONFLICTOS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Votacion por Mayoria\n",
    "\n",
    "3 agentes con perspectivas diferentes votan. La mayoria gana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTRATEGIA 1: VOTACION POR MAYORIA\n",
    "# ============================================================\n",
    "\n",
    "class Voto(BaseModel):\n",
    "    \"\"\"Voto de un agente.\"\"\"\n",
    "    eleccion: str = Field(description=\"Batman o Spider-Man\")\n",
    "    justificacion: str = Field(description=\"Justificacion breve del voto\")\n",
    "    confianza: float = Field(description=\"Confianza en el voto (0-1)\", ge=0.0, le=1.0)\n",
    "\n",
    "\n",
    "perspectivas = {\n",
    "    \"estratega\": \"Evalua desde la perspectiva de estrategia militar y planificacion tactica.\",\n",
    "    \"cientifico\": \"Evalua desde la perspectiva cientifica, innovacion tecnologica y adaptabilidad.\",\n",
    "    \"lider\": \"Evalua desde la perspectiva de liderazgo, carisma y capacidad de inspirar.\",\n",
    "}\n",
    "\n",
    "voter_llm = llm.with_structured_output(Voto)\n",
    "\n",
    "\n",
    "def votacion(pregunta: str) -> dict:\n",
    "    \"\"\"Resuelve por votacion de 3 agentes.\"\"\"\n",
    "    votos = []\n",
    "    for perspectiva, descripcion in perspectivas.items():\n",
    "        voto = voter_llm.invoke(\n",
    "            f\"Perspectiva: {descripcion}\\n\\n{pregunta}\\n\\nElige entre Batman o Spider-Man.\"\n",
    "        )\n",
    "        votos.append({\"perspectiva\": perspectiva, **voto.model_dump()})\n",
    "    \n",
    "    # Conteo\n",
    "    conteo = {}\n",
    "    for v in votos:\n",
    "        conteo[v[\"eleccion\"]] = conteo.get(v[\"eleccion\"], 0) + 1\n",
    "    \n",
    "    ganador = max(conteo, key=conteo.get)\n",
    "    \n",
    "    return {\n",
    "        \"estrategia\": \"votacion\",\n",
    "        \"votos\": votos,\n",
    "        \"conteo\": conteo,\n",
    "        \"ganador\": ganador,\n",
    "        \"unanime\": len(set(v[\"eleccion\"] for v in votos)) == 1,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test\n",
    "pregunta_test = \"Quien seria mejor lider en una crisis global: Batman o Spider-Man?\"\n",
    "result_voto = votacion(pregunta_test)\n",
    "\n",
    "print(\"VOTACION POR MAYORIA\")\n",
    "print(f\"Pregunta: {pregunta_test}\")\n",
    "for v in result_voto[\"votos\"]:\n",
    "    print(f\"  [{v['perspectiva']:12s}] {v['eleccion']:12s} (confianza: {v['confianza']:.0%})\")\n",
    "    print(f\"                  {v['justificacion'][:100]}\")\n",
    "print(f\"Resultado: {result_voto['ganador']} ({result_voto['conteo']})\")\n",
    "print(f\"Unanime: {result_voto['unanime']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Debate con Rondas\n",
    "\n",
    "Dos agentes debaten, presentando argumentos y contra-argumentos, hasta que convergen o se agotan las rondas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTRATEGIA 2: DEBATE\n",
    "# ============================================================\n",
    "\n",
    "class ArgumentoDebate(BaseModel):\n",
    "    \"\"\"Argumento en un debate.\"\"\"\n",
    "    posicion: str = Field(description=\"Batman o Spider-Man\")\n",
    "    argumento: str = Field(description=\"Argumento principal\")\n",
    "    contra_argumento: str = Field(description=\"Respuesta al argumento del oponente\")\n",
    "    cambio_posicion: bool = Field(description=\"Si cambio de posicion tras los argumentos del oponente\")\n",
    "\n",
    "debate_llm = llm.with_structured_output(ArgumentoDebate)\n",
    "\n",
    "\n",
    "def debate(pregunta: str, max_rondas: int = 3) -> dict:\n",
    "    \"\"\"Resuelve por debate entre dos agentes.\"\"\"\n",
    "    historial = []\n",
    "    posiciones = {\"defensor_batman\": \"Batman\", \"defensor_spiderman\": \"Spider-Man\"}\n",
    "    \n",
    "    for ronda in range(max_rondas):\n",
    "        ronda_args = {}\n",
    "        for agente, posicion_inicial in posiciones.items():\n",
    "            historial_texto = \"\\n\".join([\n",
    "                f\"Ronda {h['ronda']}, {h['agente']}: {h['argumento'][:150]}\"\n",
    "                for h in historial\n",
    "            ]) if historial else \"Sin historial previo.\"\n",
    "            \n",
    "            arg = debate_llm.invoke(\n",
    "                f\"Eres {agente} defendiendo a {posicion_inicial}.\\n\"\n",
    "                f\"Pregunta: {pregunta}\\n\\n\"\n",
    "                f\"Historial del debate:\\n{historial_texto}\\n\\n\"\n",
    "                f\"Presenta tu argumento y responde al oponente.\"\n",
    "            )\n",
    "            \n",
    "            ronda_args[agente] = arg\n",
    "            historial.append({\n",
    "                \"ronda\": ronda + 1,\n",
    "                \"agente\": agente,\n",
    "                \"posicion\": arg.posicion,\n",
    "                \"argumento\": arg.argumento,\n",
    "                \"cambio\": arg.cambio_posicion,\n",
    "            })\n",
    "        \n",
    "        # Verificar convergencia\n",
    "        posiciones_actuales = [a.posicion for a in ronda_args.values()]\n",
    "        if len(set(posiciones_actuales)) == 1:\n",
    "            return {\n",
    "                \"estrategia\": \"debate\",\n",
    "                \"rondas\": ronda + 1,\n",
    "                \"convergencia\": True,\n",
    "                \"ganador\": posiciones_actuales[0],\n",
    "                \"historial\": historial,\n",
    "            }\n",
    "    \n",
    "    # Sin convergencia: la posicion del ultimo argumento\n",
    "    return {\n",
    "        \"estrategia\": \"debate\",\n",
    "        \"rondas\": max_rondas,\n",
    "        \"convergencia\": False,\n",
    "        \"ganador\": \"Sin consenso\",\n",
    "        \"historial\": historial,\n",
    "    }\n",
    "\n",
    "\n",
    "result_debate = debate(pregunta_test)\n",
    "\n",
    "print(\"\\nDEBATE\")\n",
    "for h in result_debate[\"historial\"]:\n",
    "    cambio = \" [CAMBIO]\" if h[\"cambio\"] else \"\"\n",
    "    print(f\"  Ronda {h['ronda']}, {h['agente']:25s}: {h['posicion']}{cambio}\")\n",
    "    print(f\"    {h['argumento'][:120]}...\")\n",
    "print(f\"Resultado: {result_debate['ganador']} (convergencia: {result_debate['convergencia']}, rondas: {result_debate['rondas']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Juez Evaluador\n",
    "\n",
    "Un tercer agente evalua los argumentos de ambos lados y emite un veredicto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTRATEGIA 3: JUEZ\n",
    "# ============================================================\n",
    "\n",
    "class Veredicto(BaseModel):\n",
    "    \"\"\"Veredicto del juez.\"\"\"\n",
    "    ganador: str = Field(description=\"Batman o Spider-Man\")\n",
    "    score_batman: int = Field(description=\"Score de Batman (1-10)\", ge=1, le=10)\n",
    "    score_spiderman: int = Field(description=\"Score de Spider-Man (1-10)\", ge=1, le=10)\n",
    "    razonamiento: str = Field(description=\"Razonamiento detallado del juez\")\n",
    "    aspectos_evaluados: list[str] = Field(description=\"Aspectos que se evaluaron\")\n",
    "\n",
    "judge_structured = llm_judge.with_structured_output(Veredicto)\n",
    "\n",
    "\n",
    "def juez(pregunta: str) -> dict:\n",
    "    \"\"\"Resuelve usando un juez que evalua argumentos de ambos lados.\"\"\"\n",
    "    # Generar argumentos de cada lado\n",
    "    arg_batman = llm.invoke([\n",
    "        SystemMessage(content=\"Presenta los mejores argumentos a favor de Batman.\"),\n",
    "        HumanMessage(content=pregunta),\n",
    "    ])\n",
    "    \n",
    "    arg_spider = llm.invoke([\n",
    "        SystemMessage(content=\"Presenta los mejores argumentos a favor de Spider-Man.\"),\n",
    "        HumanMessage(content=pregunta),\n",
    "    ])\n",
    "    \n",
    "    # Juez evalua\n",
    "    veredicto = judge_structured.invoke(\n",
    "        f\"Pregunta: {pregunta}\\n\\n\"\n",
    "        f\"Argumentos a favor de Batman:\\n{arg_batman.content}\\n\\n\"\n",
    "        f\"Argumentos a favor de Spider-Man:\\n{arg_spider.content}\\n\\n\"\n",
    "        f\"Evalua ambos argumentos de forma imparcial y emite un veredicto.\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"estrategia\": \"juez\",\n",
    "        \"veredicto\": veredicto.model_dump(),\n",
    "        \"ganador\": veredicto.ganador,\n",
    "        \"arg_batman\": arg_batman.content[:300],\n",
    "        \"arg_spider\": arg_spider.content[:300],\n",
    "    }\n",
    "\n",
    "\n",
    "result_juez = juez(pregunta_test)\n",
    "\n",
    "print(\"\\nJUEZ EVALUADOR\")\n",
    "v = result_juez[\"veredicto\"]\n",
    "print(f\"  Batman: {v['score_batman']}/10\")\n",
    "print(f\"  Spider-Man: {v['score_spiderman']}/10\")\n",
    "print(f\"  Ganador: {v['ganador']}\")\n",
    "print(f\"  Aspectos: {v['aspectos_evaluados']}\")\n",
    "print(f\"  Razonamiento: {v['razonamiento'][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparacion de Estrategias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPARACION CON 5 PREGUNTAS\n",
    "# ============================================================\n",
    "\n",
    "preguntas_controversiales = [\n",
    "    \"Quien tiene mejor filosofia moral: Batman o Spider-Man?\",\n",
    "    \"Quien es mejor detective: Batman o Spider-Man?\",\n",
    "    \"Quien maneja mejor la perdida personal?\",\n",
    "    \"Quien tiene mejores aliados y por que?\",\n",
    "    \"Quien seria mas efectivo en el universo del otro?\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPARACION DE ESTRATEGIAS (5 preguntas)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "resultados = []\n",
    "for pregunta in preguntas_controversiales:\n",
    "    print(f\"\\nQ: {pregunta}\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    r_voto = votacion(pregunta)\n",
    "    lat_voto = (time.time() - t0) * 1000\n",
    "    \n",
    "    t0 = time.time()\n",
    "    r_juez = juez(pregunta)\n",
    "    lat_juez = (time.time() - t0) * 1000\n",
    "    \n",
    "    print(f\"  Votacion: {r_voto['ganador']:12s} (unanime: {r_voto['unanime']}, {lat_voto:.0f}ms)\")\n",
    "    print(f\"  Juez:     {r_juez['ganador']:12s} ({lat_juez:.0f}ms)\")\n",
    "    \n",
    "    resultados.append({\n",
    "        \"pregunta\": pregunta[:40],\n",
    "        \"votacion\": r_voto[\"ganador\"],\n",
    "        \"juez\": r_juez[\"ganador\"],\n",
    "        \"acuerdo\": r_voto[\"ganador\"] == r_juez[\"ganador\"],\n",
    "    })\n",
    "\n",
    "acuerdos = sum(1 for r in resultados if r[\"acuerdo\"])\n",
    "print(f\"\\nAcuerdo entre estrategias: {acuerdos}/{len(resultados)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1. **Votacion**: Rapida, democratica, pero puede fallar si los votantes tienen sesgos similares\n",
    "2. **Debate**: Explora argumentos en profundidad, pero costoso en tokens y tiempo\n",
    "3. **Juez**: Balance entre calidad y costo, pero depende de un solo punto de vista\n",
    "4. Ninguna estrategia es universalmente superior — elegir segun el caso de uso\n",
    "5. El **acuerdo entre estrategias** es una senal de robustez en la decision\n",
    "6. Para produccion, combinar votacion rapida + juez en caso de empate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 5
}