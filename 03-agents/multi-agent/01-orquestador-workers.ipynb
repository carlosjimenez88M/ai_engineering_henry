{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 â€” Patron Orquestador-Workers\n",
    "\n",
    "**Objetivo**: Implementar el patron donde un agente orquestador descompone tareas complejas en subtareas y las despacha a workers especializados.\n",
    "\n",
    "## Contenido\n",
    "1. Dise\u00f1o del orquestador\n",
    "2. Workers especializados (Batman, Spider-Man, Sintetizador)\n",
    "3. Despacho con `Send()` de LangGraph\n",
    "4. Sintetizacion de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "from pydantic import BaseModel, Field\n",
    "import chromadb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PATRON ORQUESTADOR-WORKERS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SETUP: Cargar comics en ChromaDB\n",
    "# ============================================================\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "for personaje in [\"batman\", \"spiderman\"]:\n",
    "    with open(f\"../data/{personaje}_comics.json\") as f:\n",
    "        comics = json.load(f)\n",
    "    \n",
    "    chunks, metas = [], []\n",
    "    for comic in comics:\n",
    "        for i, chunk in enumerate(splitter.split_text(comic[\"contenido\"])):\n",
    "            chunks.append(chunk)\n",
    "            metas.append({\"personaje\": personaje, \"arco\": comic[\"arco\"], \"tema\": comic[\"tema\"], \"titulo\": comic[\"titulo\"]})\n",
    "    \n",
    "    embs = embeddings.embed_documents(chunks)\n",
    "    col = chroma_client.create_collection(name=f\"orch_{personaje}\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "    col.add(ids=[f\"{personaje}_{i}\" for i in range(len(chunks))], embeddings=embs, documents=chunks, metadatas=metas)\n",
    "    print(f\"Coleccion orch_{personaje}: {col.count()} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dise\u00f1o del Orquestador\n",
    "\n",
    "El orquestador recibe una pregunta compleja y la descompone en subtareas.\n",
    "\n",
    "```\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502 Orquestador  \u2502\n",
    "                    \u2502 (descompone) \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                           \u2502\n",
    "              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "              \u25bc            \u25bc            \u25bc\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502 Worker   \u2502 \u2502 Worker   \u2502 \u2502 Worker   \u2502\n",
    "        \u2502 Batman   \u2502 \u2502 Spider   \u2502 \u2502 General  \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "             \u2502             \u2502            \u2502\n",
    "             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                           \u25bc\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502 Sintetizador \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODELO DE DESCOMPOSICION\n",
    "# ============================================================\n",
    "\n",
    "class Subtarea(BaseModel):\n",
    "    \"\"\"Una subtarea generada por el orquestador.\"\"\"\n",
    "    id: int = Field(description=\"ID de la subtarea\")\n",
    "    descripcion: str = Field(description=\"Descripcion de lo que debe investigar\")\n",
    "    worker: Literal[\"batman\", \"spiderman\", \"general\"] = Field(description=\"Worker asignado\")\n",
    "\n",
    "class PlanOrquestacion(BaseModel):\n",
    "    \"\"\"Plan de descomposicion del orquestador.\"\"\"\n",
    "    pregunta_original: str = Field(description=\"La pregunta original del usuario\")\n",
    "    subtareas: list[Subtarea] = Field(description=\"Lista de subtareas a ejecutar\")\n",
    "    estrategia: str = Field(description=\"Breve descripcion de la estrategia de descomposicion\")\n",
    "\n",
    "orchestrator_llm = llm.with_structured_output(PlanOrquestacion)\n",
    "\n",
    "# Test\n",
    "plan = orchestrator_llm.invoke(\n",
    "    \"Descompone esta pregunta en subtareas para workers especializados en Batman y Spider-Man:\\n\\n\"\n",
    "    \"Quien seria mejor lider en una crisis: Batman o Spider-Man? Considera su experiencia con equipos, \"\n",
    "    \"su estilo de liderazgo, y momentos donde demostraron liderazgo.\"\n",
    ")\n",
    "\n",
    "print(\"Plan de orquestacion:\")\n",
    "print(f\"  Estrategia: {plan.estrategia}\")\n",
    "for st in plan.subtareas:\n",
    "    print(f\"  [{st.worker:10s}] Subtarea {st.id}: {st.descripcion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTADOS Y WORKERS\n",
    "# ============================================================\n",
    "\n",
    "class OrchestratorState(TypedDict):\n",
    "    pregunta: str\n",
    "    plan: dict | None\n",
    "    resultados_workers: list[dict]\n",
    "    respuesta_final: str\n",
    "\n",
    "\n",
    "class WorkerState(TypedDict):\n",
    "    subtarea: str\n",
    "    worker_type: str\n",
    "    resultado: str\n",
    "\n",
    "\n",
    "def nodo_orquestador(state: OrchestratorState) -> dict:\n",
    "    \"\"\"Descompone la pregunta en subtareas.\"\"\"\n",
    "    plan = orchestrator_llm.invoke(\n",
    "        f\"Descompone esta pregunta en subtareas para workers especializados en Batman y Spider-Man:\\n\\n{state['pregunta']}\"\n",
    "    )\n",
    "    return {\"plan\": plan.model_dump()}\n",
    "\n",
    "\n",
    "def dispatch_workers(state: OrchestratorState) -> list[Send]:\n",
    "    \"\"\"Despacha subtareas a workers usando Send().\"\"\"\n",
    "    plan = state[\"plan\"]\n",
    "    sends = []\n",
    "    for subtarea in plan[\"subtareas\"]:\n",
    "        sends.append(Send(\"worker\", {\n",
    "            \"subtarea\": subtarea[\"descripcion\"],\n",
    "            \"worker_type\": subtarea[\"worker\"],\n",
    "            \"resultado\": \"\",\n",
    "        }))\n",
    "    return sends\n",
    "\n",
    "\n",
    "def nodo_worker(state: WorkerState) -> dict:\n",
    "    \"\"\"Worker que ejecuta una subtarea.\"\"\"\n",
    "    worker_type = state[\"worker_type\"]\n",
    "    subtarea = state[\"subtarea\"]\n",
    "    \n",
    "    # Buscar en ChromaDB segun el tipo de worker\n",
    "    if worker_type in (\"batman\", \"spiderman\"):\n",
    "        col = chroma_client.get_collection(f\"orch_{worker_type}\")\n",
    "        emb = embeddings.embed_query(subtarea)\n",
    "        results = col.query(query_embeddings=[emb], n_results=3, include=[\"documents\", \"metadatas\"])\n",
    "        contexto = \"\\n\".join([f\"[{m['arco']}]: {d[:300]}\" for d, m in zip(results[\"documents\"][0], results[\"metadatas\"][0])])\n",
    "    else:\n",
    "        # Worker general busca en ambas\n",
    "        contexto_parts = []\n",
    "        for p in [\"batman\", \"spiderman\"]:\n",
    "            col = chroma_client.get_collection(f\"orch_{p}\")\n",
    "            emb = embeddings.embed_query(subtarea)\n",
    "            results = col.query(query_embeddings=[emb], n_results=2, include=[\"documents\", \"metadatas\"])\n",
    "            contexto_parts.extend([f\"[{m['personaje']}/{m['arco']}]: {d[:200]}\" for d, m in zip(results[\"documents\"][0], results[\"metadatas\"][0])])\n",
    "        contexto = \"\\n\".join(contexto_parts)\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=f\"Eres un worker especializado en {worker_type}. Responde esta subtarea basandote en el contexto.\\n\\nContexto:\\n{contexto}\"),\n",
    "        HumanMessage(content=subtarea),\n",
    "    ])\n",
    "    \n",
    "    return {\"resultado\": response.content}\n",
    "\n",
    "\n",
    "def collect_results(state: OrchestratorState) -> dict:\n",
    "    \"\"\"No-op: los resultados se acumulan automaticamente.\"\"\"\n",
    "    return {}\n",
    "\n",
    "\n",
    "def nodo_sintetizador(state: OrchestratorState) -> dict:\n",
    "    \"\"\"Sintetiza los resultados de todos los workers.\"\"\"\n",
    "    resultados = state.get(\"resultados_workers\", [])\n",
    "    \n",
    "    resumen_workers = \"\\n\\n\".join([\n",
    "        f\"Worker {i+1} ({r.get('worker_type', '?')}): {r.get('resultado', 'sin resultado')[:500]}\"\n",
    "        for i, r in enumerate(resultados)\n",
    "    ])\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"Eres un sintetizador. Combina las investigaciones de los workers en una respuesta coherente y completa. Cita las fuentes cuando sea posible. Responde en espa\\u00f1ol.\"),\n",
    "        HumanMessage(content=f\"Pregunta original: {state['pregunta']}\\n\\nResultados de investigacion:\\n{resumen_workers}\"),\n",
    "    ])\n",
    "    \n",
    "    return {\"respuesta_final\": response.content}\n",
    "\n",
    "\n",
    "print(\"Workers y sintetizador definidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONSTRUCCION Y EJECUCION\n",
    "# ============================================================\n",
    "\n",
    "# Simplified version without Send() for compatibility\n",
    "def run_orchestrator(pregunta: str) -> dict:\n",
    "    \"\"\"Ejecuta el patron orquestador-workers manualmente.\"\"\"\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # 1. Orquestador descompone\n",
    "    plan = orchestrator_llm.invoke(\n",
    "        f\"Descompone esta pregunta en subtareas para workers de Batman y Spider-Man:\\n\\n{pregunta}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Plan: {plan.estrategia}\")\n",
    "    for st in plan.subtareas:\n",
    "        print(f\"  [{st.worker}] {st.descripcion}\")\n",
    "    \n",
    "    # 2. Workers ejecutan en secuencia (en prod seria paralelo)\n",
    "    resultados = []\n",
    "    for subtarea in plan.subtareas:\n",
    "        worker_state = {\"subtarea\": subtarea.descripcion, \"worker_type\": subtarea.worker, \"resultado\": \"\"}\n",
    "        result = nodo_worker(worker_state)\n",
    "        resultados.append({**worker_state, **result})\n",
    "        print(f\"  Worker {subtarea.worker} completado ({len(result['resultado'])} chars)\")\n",
    "    \n",
    "    # 3. Sintetizador combina\n",
    "    synth_state = {\"pregunta\": pregunta, \"resultados_workers\": resultados, \"plan\": plan.model_dump(), \"respuesta_final\": \"\"}\n",
    "    final = nodo_sintetizador(synth_state)\n",
    "    \n",
    "    latencia = (time.time() - t0) * 1000\n",
    "    \n",
    "    return {\n",
    "        \"pregunta\": pregunta,\n",
    "        \"plan\": plan.model_dump(),\n",
    "        \"num_workers\": len(resultados),\n",
    "        \"respuesta\": final[\"respuesta_final\"],\n",
    "        \"latencia_ms\": round(latencia, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EJECUCION DEL ORQUESTADOR\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "result = run_orchestrator(\n",
    "    \"Quien seria mejor lider en una crisis: Batman o Spider-Man? \"\n",
    "    \"Considera su experiencia con equipos, estilo de liderazgo, y momentos clave.\"\n",
    ")\n",
    "\n",
    "print(f\"\\nRESPUESTA FINAL:\")\n",
    "print(result[\"respuesta\"][:600])\n",
    "print(f\"\\nMetricas: {result['num_workers']} workers, {result['latencia_ms']}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "1. El **orquestador** descompone preguntas complejas en subtareas manejables\n",
    "2. Los **workers** son especialistas que acceden a su propia base de conocimiento\n",
    "3. El **sintetizador** combina resultados en una respuesta coherente\n",
    "4. Este patron escala bien: agregar un nuevo especialista es agregar un worker\n",
    "5. En produccion, los workers pueden ejecutarse en **paralelo** usando `Send()`\n",
    "6. El costo es proporcional al numero de subtareas (mas descomposicion = mas caro)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 5
}