# Design Decisions - Brief Builder

Este documento registra las decisiones arquitecturales clave del sistema `brief_builder`, siguiendo el formato ADR (Architecture Decision Record). Cada decisi√≥n est√° fundamentada en principios de **"AI Engineering"** y **"Designing Machine Learning Systems"** de Chip Huyen.

---

## ADR-001: Retry Logic con Exponential Backoff

**Fecha:** 2026-02-03
**Estado:** Implementado en `retry.py`

### Contexto

Las APIs de LLM (como OpenAI) pueden fallar temporalmente por:
- Rate limiting (429)
- Errores de servicio (503, 504)
- Timeouts de red
- Mantenimiento temporal

Sin retry logic, cada fallo requiere intervenci√≥n manual, haciendo el sistema fr√°gil en producci√≥n.

### Decisi√≥n

Implementar retry con **exponential backoff** y **jitter**:
- M√°ximo 3 intentos por defecto
- Delay inicial: 1 segundo
- Multiplicador exponencial: 2x (1s ‚Üí 2s ‚Üí 4s)
- Jitter aleatorio para prevenir thundering herd
- Solo reintentar errores recuperables (429, 503, 504)

### Rationale

Chip Huyen (AI Engineering, Cap√≠tulo 5: "Resilient Systems"):
> "Resilience is critical for LLM systems. Exponential backoff prevents overwhelming failing services and gives them time to recover."

**Beneficios:**
- Sistema m√°s robusto ante fallos temporales
- Evita sobrecargar APIs con reintentos agresivos
- Jitter previene que m√∫ltiples clientes reintenten simult√°neamente
- Logging detallado para debugging

**Trade-offs aceptados:**
- Latencia mayor en casos de error (hasta ~7 segundos con 3 intentos)
- Complejidad adicional en el c√≥digo
- **Decisi√≥n:** El trade-off es aceptable porque la alternativa (fallos sin retry) es peor en producci√≥n

### Alternativas Consideradas

1. **Sin retry** ‚Üí ‚ùå Descartado: Demasiado fr√°gil
2. **Retry simple (fixed delay)** ‚Üí ‚ùå Descartado: Puede empeorar rate limiting
3. **Circuit breaker** ‚Üí üîÆ Futuro: √ötil cuando se escale a m√∫ltiples clientes

### Referencias

- Implementaci√≥n: `01_class/ai_engineering/brief_builder/retry.py:29`
- Tests: `01_class/ai_engineering/tests/test_main.py` (mocked)
- Chip Huyen: AI Engineering, Chapter 5

---

## ADR-002: Validaci√≥n de Inputs y Outputs

**Fecha:** 2026-02-03
**Estado:** Implementado en `validator.py`

### Contexto

Los sistemas AI son probabil√≠sticos y pueden:
- Aceptar inputs inv√°lidos que desperdician tokens ($$$)
- Generar outputs malformados o incompletos
- Fallar silenciosamente sin validaci√≥n

### Decisi√≥n

Validar agresivamente en ambos extremos:

**Input validation:**
- `temperature` en rango [0.0, 2.0]
- `context` no excede l√≠mite de tokens (~4000)
- `output_path` es escribible

**Output validation:**
- Formato markdown v√°lido
- Estructura del brief completa (secciones requeridas)
- Content no vac√≠o

### Rationale

Chip Huyen (Designing ML Systems, Cap√≠tulo 8: "Data Quality"):
> "Bad data in, bad predictions out. Validation at system boundaries is non-negotiable."

**Beneficios:**
- Detecta errores temprano (fail fast)
- Previene desperdiciar tokens ($) en inputs inv√°lidos
- Garantiza calidad m√≠nima del output
- Logs warnings cuando brief est√° incompleto

**Trade-offs:**
- Overhead de validaci√≥n (~5ms por request)
- C√≥digo adicional a mantener
- **Decisi√≥n:** Overhead m√≠nimo vs beneficio alto

### Implementaci√≥n

```python
# Validar ANTES de llamar API
validate_temperature(temperature)
validate_context_length(context)

# Validar DESPU√âS de recibir respuesta
if not validate_markdown_format(content):
    raise ValidationError("Invalid markdown")

structure_checks = validate_brief_structure(content)
if not structure_checks["is_complete"]:
    logger.warning("Brief incompleto: %s", missing_sections)
```

### Alternativas Consideradas

1. **Sin validaci√≥n** ‚Üí ‚ùå Descartado: Costos impredecibles, calidad baja
2. **Validaci√≥n solo en inputs** ‚Üí ‚ùå Insuficiente: No garantiza output √∫til
3. **Validaci√≥n estricta que falla** ‚Üí ‚ö†Ô∏è Parcial: Warning en lugar de error si brief √∫til pero incompleto

### Referencias

- Implementaci√≥n: `01_class/ai_engineering/brief_builder/validator.py`
- Tests: `01_class/ai_engineering/tests/test_validator.py` (29 tests)
- Chip Huyen: Designing ML Systems, Chapter 8

---

## ADR-003: Tracking de M√©tricas y Costos

**Fecha:** 2026-02-03
**Estado:** Implementado en `metrics.py`

### Contexto

Los sistemas AI tienen costos variables por request:
- Tokens de entrada (prompt)
- Tokens de salida (completion)
- Modelo utilizado (gpt-4o vs gpt-4o-mini = 10x diferencia)
- Latencia impacta UX

Sin tracking, es imposible:
- Estimar presupuestos
- Detectar anomal√≠as (costos altos)
- Optimizar prompts
- Debugging de producci√≥n

### Decisi√≥n

Trackear y loggear m√©tricas en cada request:
- Tokens (prompt, completion, total)
- Costo estimado (USD)
- Latencia (segundos)
- Model, temperature, timestamp
- Guardar en `.metrics.json` junto al brief

### Rationale

Chip Huyen (AI Engineering, Cap√≠tulo 7: "Monitoring and Observability"):
> "You can't improve what you don't measure. Cost and latency metrics are critical for LLM systems."

**Beneficios:**
- Budget awareness desde d√≠a 1
- Detectar prompts ineficientes (demasiados tokens)
- Comparar modelos (gpt-4o-mini vs gpt-4o)
- Hist√≥rico de costos para auditor√≠a
- Debugging: correlacionar latencia con errores

**Ejemplo de output:**
```json
{
  "model": "gpt-4o-mini",
  "temperature": 0.2,
  "prompt_tokens": 1234,
  "completion_tokens": 567,
  "total_tokens": 1801,
  "estimated_cost_usd": 0.000525,
  "latency_seconds": 3.42,
  "timestamp": "2026-02-03T18:30:00Z"
}
```

### Alternativas Consideradas

1. **Sin tracking** ‚Üí ‚ùå Descartado: Imposible gestionar costos
2. **Solo loggear en consola** ‚Üí ‚ùå Insuficiente: Se pierde el historial
3. **Enviar a servicio externo** ‚Üí üîÆ Futuro: √ötil para agregaci√≥n multi-usuario

### Referencias

- Implementaci√≥n: `01_class/ai_engineering/brief_builder/metrics.py`
- Uso: `01_class/ai_engineering/brief_builder/main.py:163-185`
- Pricing: https://openai.com/api/pricing/
- Chip Huyen: AI Engineering, Chapter 7

---

## ADR-004: Excepciones Espec√≠ficas vs Generic

**Fecha:** 2026-02-03
**Estado:** Implementado en `exceptions.py`

### Contexto

Los sistemas complejos tienen m√∫ltiples modos de fallo:
- Errores de configuraci√≥n (API key faltante)
- Errores de API (rate limit, timeout)
- Errores de validaci√≥n (input/output inv√°lido)

Usar `Exception` gen√©rico hace dif√≠cil:
- Diagnosticar problemas
- Aplicar retry selectivo
- Manejar errores diferentes de forma apropiada

### Decisi√≥n

Definir jerarqu√≠a de excepciones espec√≠ficas:

```python
BriefBuilderError (base)
‚îú‚îÄ‚îÄ APIError (API calls)
‚îú‚îÄ‚îÄ ValidationError (inputs/outputs)
‚îî‚îÄ‚îÄ ConfigurationError (settings)
```

Cada excepci√≥n incluye:
- Mensaje descriptivo
- Metadata relevante (status_code, field, value)
- `__str__` customizado para logging

### Rationale

Python Best Practices + Chip Huyen (AI Engineering):
> "Specific exceptions make debugging faster and enable selective error handling."

**Beneficios:**
- Catch selectivo: `except APIError` para retry, `except ValidationError` para skip
- Logs m√°s claros: "ValidationError: temperature=3.0" vs "Exception"
- Facilita debugging en producci√≥n
- Self-documenting code

**Ejemplo:**
```python
try:
    brief, metrics = generate_brief(context, temp=3.0)
except ValidationError as e:
    logger.error("Input inv√°lido: %s", e)
    # No reintentar - error del usuario
except APIError as e:
    logger.error("API fall√≥: %s", e)
    # Reintentar autom√°ticamente
```

### Alternativas Consideradas

1. **Solo `Exception`** ‚Üí ‚ùå Dificulta debugging
2. **C√≥digos de error num√©ricos** ‚Üí ‚ùå Menos pythonic
3. **Excepciones muy granulares** ‚Üí ‚ö†Ô∏è Over-engineering para este scope

### Referencias

- Implementaci√≥n: `01_class/ai_engineering/brief_builder/exceptions.py`
- Uso: `01_class/ai_engineering/brief_builder/main.py:260-290`
- PEP 8: Exception Naming Conventions

---

## ADR-005: Prompts Versionados en Git

**Fecha:** 2026-02-03
**Estado:** Implementado en `prompts.py`

### Contexto

Los prompts son **c√≥digo** en sistemas AI:
- Cambiar el prompt cambia el comportamiento del sistema
- Debugging requiere saber qu√© prompt se us√≥
- M√∫ltiples developers necesitan colaborar en prompts

Sin versionado:
- Imposible reproducir outputs antiguos
- No hay code review de prompts
- Regresiones dif√≠ciles de detectar

### Decisi√≥n

**Versionar prompts en Git** como c√≥digo normal:
- `prompts.py` contiene funciones que retornan strings
- Cambios de prompts = commits con descripci√≥n
- Code review obligatorio para cambios de prompts
- Fecha incluida en prompt (`date.today()`) para reproducibilidad

### Rationale

Chip Huyen (AI Engineering, Cap√≠tulo 3: "Reproducibility"):
> "Prompts are code. Version them like code. A system that can't reproduce results is a system you can't debug."

**Beneficios:**
- Reproducibilidad: `git checkout <commit>` reproduce output exacto
- Trazabilidad: `git blame` muestra qui√©n cambi√≥ qu√©
- Colaboraci√≥n: Pull requests para cambios de prompts
- Rollback f√°cil si prompt nuevo empeora calidad

**Ejemplo de commit message:**
```
feat(prompts): agregar secci√≥n de anti-patrones

- A√±ade requisito de 5 anti-patrones m√≠nimo
- Cada anti-patr√≥n incluye s√≠ntoma + impacto + mitigaci√≥n
- Mejora calidad del brief para estudiantes
```

### Alternativas Consideradas

1. **Prompts en base de datos** ‚Üí üîÆ Futuro: √ötil para A/B testing
2. **Hardcoded strings** ‚Üí ‚ùå Sin versionado, dif√≠cil de revisar
3. **Archivos .txt separados** ‚Üí ‚ö†Ô∏è Posible, pero menos ergon√≥mico

### Referencias

- Implementaci√≥n: `01_class/ai_engineering/brief_builder/prompts.py`
- Git history: `git log -- prompts.py`
- Chip Huyen: AI Engineering, Chapter 3

---

## ADR-006: Testing con Mocks vs Llamadas Reales

**Fecha:** 2026-02-03
**Estado:** Implementado en `tests/conftest.py`

### Contexto

Tests de sistemas AI pueden:
- Hacer llamadas reales a APIs ($$$ + lento + rate limits)
- Usar mocks que simulan respuestas (r√°pido + gratis)

### Decisi√≥n

**Usar mocks para tests unitarios** con fixture `mock_openai_client`:
- Simula respuestas de OpenAI sin llamadas reales
- Tests r√°pidos (<1s total)
- Sin costos ni rate limits
- Tests integraci√≥n marcados con `@pytest.mark.integration` (no ejecutados por defecto)

### Rationale

Testing Best Practices:
> "Unit tests should be fast, deterministic, and isolated."

**Beneficios:**
- Tests corren en CI/CD sin API keys
- Velocidad: 56 tests en 0.47s
- Costo: $0
- Determinismo: misma respuesta siempre
- Permite testear casos edge (errores, respuestas vac√≠as)

**Trade-offs:**
- Mocks pueden divergir de API real
- **Mitigaci√≥n:** Tests manuales ocasionales con API real

### Implementaci√≥n

```python
@pytest.fixture
def mock_openai_client(mocker):
    mock_response = mocker.Mock()
    mock_response.choices[0].message.content = "..."
    mock_response.usage.total_tokens = 1500
    mock_client = mocker.Mock()
    mock_client.chat.completions.create.return_value = mock_response
    return mock_client
```

### Referencias

- Implementaci√≥n: `01_class/ai_engineering/tests/conftest.py:14`
- Tests: Todos los tests en `test_main.py` usan mocks
- Coverage: 88% con solo mocks

---

## Principios Generales Aplicados

### 1. Separation of Concerns
- `config.py`: Solo configuraci√≥n
- `prompts.py`: Solo prompts
- `validator.py`: Solo validaci√≥n
- `retry.py`: Solo retry logic
- `metrics.py`: Solo tracking

**Beneficio:** Cada m√≥dulo es testeable independientemente

### 2. Fail Fast
- Validar temperatura en `parse_args` (antes de API call)
- Validar API key en `load_settings` (al inicio)
- Validar outputs inmediatamente despu√©s de recibir

**Beneficio:** Errores detectados temprano = debugging m√°s f√°cil

### 3. Explicit Better Than Implicit
- Par√°metros expl√≠citos (`temperature`, `context`)
- Excepciones espec√≠ficas con nombres claros
- Docstrings en todas las funciones p√∫blicas

**Beneficio:** C√≥digo self-documenting

### 4. Observability First
- Logs en cada paso cr√≠tico
- M√©tricas guardadas autom√°ticamente
- Colored output para escaneo r√°pido

**Beneficio:** Debugging r√°pido en producci√≥n

---

## Decisiones Pendientes (Future Work)

### FW-001: Circuit Breaker Pattern
**Cu√°ndo:** Al escalar a m√∫ltiples usuarios concurrentes
**Por qu√©:** Prevenir cascading failures si OpenAI cae

### FW-002: A/B Testing de Prompts
**Cu√°ndo:** Cuando queramos optimizar prompts con datos
**Por qu√©:** M√©tricas cuantitativas > opiniones

### FW-003: Caching de Responses
**Cu√°ndo:** Si vemos requests repetidos
**Por qu√©:** Reducir costos y latencia

### FW-004: Async API Calls
**Cu√°ndo:** Si necesitamos generar m√∫ltiples briefs en paralelo
**Por qu√©:** Mejor throughput

### FW-005: Prompt Registry Separado
**Cu√°ndo:** Al tener >10 prompts diferentes
**Por qu√©:** Facilita A/B testing y versionado granular

---

## Referencias

### Libros
- Chip Huyen - "AI Engineering" (2024)
- Chip Huyen - "Designing Machine Learning Systems" (2022)

### Art√≠culos
- [OpenAI Best Practices](https://platform.openai.com/docs/guides/production-best-practices)
- [Google's Rules of ML](https://developers.google.com/machine-learning/guides/rules-of-ml)
- [The Twelve-Factor App](https://12factor.net/)

### C√≥digo
- Repository: `ai_engineering_henry`
- M√≥dulo: `01_class/ai_engineering/brief_builder/`
- Tests: `01_class/ai_engineering/tests/`

---

**√öltima actualizaci√≥n:** 2026-02-03
**Autores:** Carlos Daniel (con Claude Sonnet 4.5)
**Revisores:** Henry Academy Team
