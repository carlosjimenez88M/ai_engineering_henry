{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Funciones y Modularidad\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de aprendizaje\n",
    "\n",
    "Al finalizar este notebook seras capaz de:\n",
    "\n",
    "1. **Definir funciones** con type hints, docstrings y parametros por defecto.\n",
    "2. **Entender scope y closures** aplicando la regla LEGB.\n",
    "3. **Detectar el anti-patron** de argumentos mutables por defecto y corregirlo.\n",
    "4. **Organizar codigo en modulos** con una estructura de proyecto limpia.\n",
    "5. **Aplicar buenas practicas** (DRY, responsabilidad unica, naming conventions).\n",
    "\n",
    "| Temas cubiertos | Referencia |\n",
    "|---|---|\n",
    "| Funciones y type hints | `03_funciones` |\n",
    "| Modulos y archivos | `06_modulos_y_archivos` |\n",
    "| Buenas practicas | `07_buenas_practicas` |\n",
    "\n",
    "**Tiempo estimado:** 60-90 minutos\n",
    "\n",
    "**Prerequisito:** Notebook 01 (variables, control de flujo, estructuras de datos)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Anatomia de una Funcion\n",
    "\n",
    "Cada funcion en Python sigue una estructura precisa. En ingenieria de ML, las funciones\n",
    "son los bloques fundamentales de cualquier pipeline.\n",
    "\n",
    "```\n",
    "    +-- keyword          +-- nombre          +-- parametros con type hints\n",
    "    |                    |                    |\n",
    "    v                    v                    v\n",
    "   def  calculate_accuracy(correct: int, total: int) -> float:\n",
    "        \"\"\"                                                      <-- docstring\n",
    "        Calcula la accuracy de un modelo.                        \n",
    "        \"\"\"                                                      \n",
    "        if total == 0:                                           <-- cuerpo\n",
    "            return 0.0                                           \n",
    "        return correct / total                                   <-- return\n",
    "             ^                    ^\n",
    "             |                    |\n",
    "             +-- valor de retorno +-- tipo de retorno: float\n",
    "```\n",
    "\n",
    "### Tipos de parametros\n",
    "\n",
    "| Tipo | Sintaxis | Ejemplo | Uso en ML |\n",
    "|---|---|---|---|\n",
    "| Posicional | `param` | `def f(x)` | Datos de entrada obligatorios |\n",
    "| Con valor por defecto | `param=valor` | `def f(lr=0.01)` | Hiperparametros |\n",
    "| Keyword-only | `*, param` | `def f(*, verbose=True)` | Flags de configuracion |\n",
    "| `*args` | `*args` | `def f(*layers)` | Capas variables en una red |\n",
    "| `**kwargs` | `**kwargs` | `def f(**config)` | Configuracion flexible |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definicion basica con type hints y docstrings ---\n",
    "\n",
    "def calculate_accuracy(correct: int, total: int) -> float:\n",
    "    \"\"\"Calcula la accuracy dado el numero de aciertos y el total.\n",
    "    \n",
    "    Args:\n",
    "        correct: Numero de predicciones correctas.\n",
    "        total: Numero total de predicciones.\n",
    "    \n",
    "    Returns:\n",
    "        Accuracy como un float entre 0.0 y 1.0.\n",
    "    \"\"\"\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def classify_score(score: float, threshold: float = 0.5) -> str:\n",
    "    \"\"\"Clasifica un score de modelo como POSITIVO o NEGATIVO.\n",
    "    \n",
    "    Args:\n",
    "        score: Probabilidad predicha por el modelo (0.0 a 1.0).\n",
    "        threshold: Umbral de decision (por defecto 0.5).\n",
    "    \n",
    "    Returns:\n",
    "        'POSITIVO' si score >= threshold, 'NEGATIVO' en caso contrario.\n",
    "    \"\"\"\n",
    "    return \"POSITIVO\" if score >= threshold else \"NEGATIVO\"\n",
    "\n",
    "\n",
    "# --- Uso ---\n",
    "acc = calculate_accuracy(correct=85, total=100)\n",
    "print(f\"Accuracy del modelo: {acc:.2%}\")\n",
    "\n",
    "# Con threshold por defecto (0.5)\n",
    "print(f\"Score 0.73 -> {classify_score(0.73)}\")\n",
    "print(f\"Score 0.32 -> {classify_score(0.32)}\")\n",
    "\n",
    "# Con threshold personalizado\n",
    "print(f\"Score 0.73 (umbral=0.8) -> {classify_score(0.73, threshold=0.8)}\")\n",
    "\n",
    "# Podemos inspeccionar los type hints y el docstring\n",
    "print(f\"\\nType hints: {calculate_accuracy.__annotations__}\")\n",
    "print(f\"Docstring:\\n{calculate_accuracy.__doc__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scope: La Regla LEGB\n",
    "\n",
    "Python busca variables en cuatro niveles, de adentro hacia afuera:\n",
    "\n",
    "```\n",
    "  +-----------------------------------------------------------+\n",
    "  |  Built-in (B)                                             |\n",
    "  |  print(), len(), range(), int(), str(), ...               |\n",
    "  |                                                           |\n",
    "  |  +-----------------------------------------------------+ |\n",
    "  |  |  Global (G)                                          | |\n",
    "  |  |  Variables definidas a nivel de modulo               | |\n",
    "  |  |  MODEL_VERSION = \"2.1\"                               | |\n",
    "  |  |                                                      | |\n",
    "  |  |  +------------------------------------------------+ | |\n",
    "  |  |  |  Enclosing (E)                                  | | |\n",
    "  |  |  |  Variables de la funcion externa (closures)     | | |\n",
    "  |  |  |  threshold = 0.5                                | | |\n",
    "  |  |  |                                                 | | |\n",
    "  |  |  |  +-------------------------------------------+ | | |\n",
    "  |  |  |  |  Local (L)                                 | | | |\n",
    "  |  |  |  |  Variables dentro de la funcion actual     | | | |\n",
    "  |  |  |  |  result = score > threshold                | | | |\n",
    "  |  |  |  +-------------------------------------------+ | | |\n",
    "  |  |  +------------------------------------------------+ | |\n",
    "  |  +-----------------------------------------------------+ |\n",
    "  +-----------------------------------------------------------+\n",
    "\n",
    "  Orden de busqueda:  L -> E -> G -> B\n",
    "```\n",
    "\n",
    "**Regla clave:** Python siempre busca desde el scope mas interno hacia el mas externo.\n",
    "Si no encuentra la variable en ninguno, lanza `NameError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Demostracion de la regla LEGB ---\n",
    "\n",
    "# Global (G)\n",
    "MODEL_NAME = \"ClassifierV1\"\n",
    "\n",
    "def show_scope():\n",
    "    # Local (L) - esta variable \"sombrea\" cualquier variable global con el mismo nombre\n",
    "    model_info = \"informacion local\"\n",
    "    print(f\"  Local -> model_info = '{model_info}'\")\n",
    "    print(f\"  Global -> MODEL_NAME = '{MODEL_NAME}'\")\n",
    "    print(f\"  Built-in -> len = {len}\")\n",
    "\n",
    "print(\"--- Scope basico ---\")\n",
    "show_scope()\n",
    "\n",
    "# --- Closures: Enclosing scope ---\n",
    "print(\"\\n--- Closures ---\")\n",
    "\n",
    "def create_threshold_classifier(threshold: float):\n",
    "    \"\"\"Retorna una funcion que clasifica usando el threshold capturado.\n",
    "    \n",
    "    Esto es un closure: la funcion interna 'recuerda' el valor de threshold\n",
    "    incluso despues de que create_threshold_classifier haya terminado.\n",
    "    \"\"\"\n",
    "    print(f\"  Creando clasificador con threshold={threshold}\")\n",
    "    \n",
    "    def classify(score: float) -> str:\n",
    "        # 'threshold' viene del Enclosing scope (E)\n",
    "        return \"POSITIVO\" if score >= threshold else \"NEGATIVO\"\n",
    "    \n",
    "    return classify\n",
    "\n",
    "# Creamos dos clasificadores con distintos umbrales\n",
    "clf_conservador = create_threshold_classifier(0.8)\n",
    "clf_agresivo = create_threshold_classifier(0.3)\n",
    "\n",
    "score_prueba = 0.65\n",
    "print(f\"\\n  Score: {score_prueba}\")\n",
    "print(f\"  Clasificador conservador (0.8): {clf_conservador(score_prueba)}\")\n",
    "print(f\"  Clasificador agresivo    (0.3): {clf_agresivo(score_prueba)}\")\n",
    "\n",
    "# Verificamos que el closure captura la variable\n",
    "print(f\"\\n  Variables capturadas por clf_conservador: {clf_conservador.__closure__[0].cell_contents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anti-patron: Argumentos Mutables por Defecto\n",
    "\n",
    "> **ADVERTENCIA:** Este es uno de los errores mas comunes y dificiles de detectar\n",
    "> en Python. Los valores por defecto mutables (`list`, `dict`, `set`) se crean\n",
    "> **una sola vez** cuando se define la funcion, NO cada vez que se llama.\n",
    "\n",
    "```\n",
    "  PELIGROSO                          CORRECTO\n",
    "  +-----------------------+          +---------------------------+\n",
    "  | def f(x, lst=[]):     |          | def f(x, lst=None):       |\n",
    "  |     lst.append(x)     |          |     if lst is None:       |\n",
    "  |     return lst        |          |         lst = []          |\n",
    "  +-----------------------+          |     lst.append(x)         |\n",
    "  La misma lista se reutiliza        |     return lst            |\n",
    "  en cada llamada!                   +---------------------------+\n",
    "                                     Lista nueva en cada llamada\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- El bug de argumentos mutables por defecto ---\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"VERSION CON BUG: default mutable argument\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def append_to_buggy(element, target=[]):\n",
    "    \"\"\"BUGGY: la lista target se comparte entre llamadas.\"\"\"\n",
    "    target.append(element)\n",
    "    return target\n",
    "\n",
    "# Cada llamada deberia devolver una lista con UN solo elemento...\n",
    "result1 = append_to_buggy(\"modelo_A\")\n",
    "print(f\"Llamada 1: {result1}\")        # Esperado: ['modelo_A']\n",
    "\n",
    "result2 = append_to_buggy(\"modelo_B\")\n",
    "print(f\"Llamada 2: {result2}\")        # Esperado: ['modelo_B'], Real: ['modelo_A', 'modelo_B'] !!\n",
    "\n",
    "result3 = append_to_buggy(\"modelo_C\")\n",
    "print(f\"Llamada 3: {result3}\")        # La lista sigue creciendo!\n",
    "\n",
    "print(f\"\\nresult1 is result2: {result1 is result2}\")  # Son el MISMO objeto!\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"VERSION CORREGIDA: None como default\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def append_to_fixed(element, target=None):\n",
    "    \"\"\"CORRECTO: se crea una lista nueva si no se proporciona una.\"\"\"\n",
    "    if target is None:\n",
    "        target = []\n",
    "    target.append(element)\n",
    "    return target\n",
    "\n",
    "result1 = append_to_fixed(\"modelo_A\")\n",
    "print(f\"Llamada 1: {result1}\")        # ['modelo_A']\n",
    "\n",
    "result2 = append_to_fixed(\"modelo_B\")\n",
    "print(f\"Llamada 2: {result2}\")        # ['modelo_B'] -- correcto!\n",
    "\n",
    "result3 = append_to_fixed(\"modelo_C\")\n",
    "print(f\"Llamada 3: {result3}\")        # ['modelo_C'] -- correcto!\n",
    "\n",
    "print(f\"\\nresult1 is result2: {result1 is result2}\")  # Son objetos DIFERENTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Funciones como Objetos de Primera Clase\n",
    "\n",
    "En Python, las funciones son **objetos**. Esto significa que puedes:\n",
    "\n",
    "- Asignarlas a variables\n",
    "- Pasarlas como argumentos a otras funciones (**higher-order functions**)\n",
    "- Retornarlas desde otras funciones (ya lo vimos con closures)\n",
    "\n",
    "Esto es fundamental para construir **pipelines de procesamiento** en ML:\n",
    "\n",
    "```\n",
    "  datos_crudos --> [transformar] --> [filtrar] --> [ordenar] --> resultado\n",
    "                       |                |             |\n",
    "                    map(fn)        filter(fn)    sorted(key=fn)\n",
    "```\n",
    "\n",
    "### `lambda` vs funcion nombrada\n",
    "\n",
    "| Aspecto | `lambda` | Funcion nombrada |\n",
    "|---|---|---|\n",
    "| Sintaxis | `lambda x: x * 2` | `def double(x): return x * 2` |\n",
    "| Lineas | Solo una expresion | Multiples lineas |\n",
    "| Legibilidad | Menor (logica simple) | Mayor (logica compleja) |\n",
    "| Depuracion | Sin nombre en traceback | Nombre claro en traceback |\n",
    "| Uso ideal | `sorted(key=lambda...)` | Reutilizacion, testing |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Funciones de orden superior con contexto ML ---\n",
    "\n",
    "# Predicciones de un modelo (probabilidades)\n",
    "predictions = [0.12, 0.89, 0.45, 0.67, 0.93, 0.31, 0.78, 0.55, 0.08, 0.72]\n",
    "\n",
    "# --- map(): aplicar transformacion a cada elemento ---\n",
    "print(\"--- map(): Redondear predicciones ---\")\n",
    "rounded = list(map(lambda p: round(p, 1), predictions))\n",
    "print(f\"  Original:   {predictions}\")\n",
    "print(f\"  Redondeado: {rounded}\")\n",
    "\n",
    "# --- filter(): quedarnos con elementos que cumplen condicion ---\n",
    "print(\"\\n--- filter(): Predicciones de alta confianza (>= 0.7) ---\")\n",
    "high_confidence = list(filter(lambda p: p >= 0.7, predictions))\n",
    "print(f\"  Alta confianza: {high_confidence}\")\n",
    "\n",
    "# --- sorted() con key: ordenar por criterio personalizado ---\n",
    "print(\"\\n--- sorted(): Ordenar por distancia al umbral 0.5 ---\")\n",
    "\n",
    "def distance_to_threshold(score: float, threshold: float = 0.5) -> float:\n",
    "    \"\"\"Calcula que tan lejos esta un score del umbral de decision.\"\"\"\n",
    "    return abs(score - threshold)\n",
    "\n",
    "sorted_by_uncertainty = sorted(predictions, key=lambda p: distance_to_threshold(p))\n",
    "print(f\"  Mas cercanos al umbral (mas inciertos) primero:\")\n",
    "print(f\"  {sorted_by_uncertainty}\")\n",
    "\n",
    "# --- Pipeline de transformaciones ---\n",
    "print(\"\\n--- Pipeline completo ---\")\n",
    "print(f\"  Datos crudos:     {predictions}\")\n",
    "\n",
    "# Paso 1: normalizar al rango [0, 100]\n",
    "step1 = list(map(lambda p: p * 100, predictions))\n",
    "print(f\"  Paso 1 (escalar): {step1}\")\n",
    "\n",
    "# Paso 2: filtrar scores >= 50\n",
    "step2 = list(filter(lambda s: s >= 50, step1))\n",
    "print(f\"  Paso 2 (filtrar): {step2}\")\n",
    "\n",
    "# Paso 3: ordenar de mayor a menor\n",
    "step3 = sorted(step2, reverse=True)\n",
    "print(f\"  Paso 3 (ordenar): {step3}\")\n",
    "\n",
    "# Paso 4: formatear como porcentajes\n",
    "step4 = list(map(lambda s: f\"{s:.0f}%\", step3))\n",
    "print(f\"  Resultado final:  {step4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Organizacion en Modulos\n",
    "\n",
    "Un proyecto de ML bien organizado separa responsabilidades en modulos:\n",
    "\n",
    "```\n",
    "  mi_proyecto_ml/\n",
    "  |\n",
    "  +-- main.py                  # Punto de entrada\n",
    "  +-- config.py                # Constantes y configuracion\n",
    "  |\n",
    "  +-- data/\n",
    "  |   +-- __init__.py\n",
    "  |   +-- loader.py            # Funciones de carga de datos\n",
    "  |   +-- preprocessing.py     # Limpieza y transformaciones\n",
    "  |\n",
    "  +-- models/\n",
    "  |   +-- __init__.py\n",
    "  |   +-- classifier.py        # Definicion del modelo\n",
    "  |   +-- metrics.py           # Funciones de evaluacion\n",
    "  |\n",
    "  +-- utils/\n",
    "  |   +-- __init__.py\n",
    "  |   +-- file_io.py           # Lectura/escritura de archivos\n",
    "  |   +-- logging_utils.py     # Utilidades de logging\n",
    "  |\n",
    "  +-- tests/\n",
    "      +-- test_metrics.py\n",
    "      +-- test_preprocessing.py\n",
    "```\n",
    "\n",
    "### Conceptos clave\n",
    "\n",
    "| Concepto | Que es | Para que sirve |\n",
    "|---|---|---|\n",
    "| Modulo | Un archivo `.py` | Agrupar funciones relacionadas |\n",
    "| Paquete | Carpeta con `__init__.py` | Agrupar modulos relacionados |\n",
    "| `import` | Cargar un modulo | Reutilizar codigo |\n",
    "| `__name__` | Nombre del modulo actual | Distinguir ejecucion directa vs importacion |\n",
    "| `if __name__ == \"__main__\":` | Guardia de ejecucion | Ejecutar solo si se corre directamente |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simulacion de organizacion modular ---\n",
    "# En un notebook no podemos crear archivos .py separados facilmente,\n",
    "# pero podemos demostrar los conceptos.\n",
    "\n",
    "# --- Simulacion de config.py ---\n",
    "# En un modulo real, estas serian constantes a nivel de modulo\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "MODEL_VERSION = \"1.0.0\"\n",
    "\n",
    "# --- Simulacion de metrics.py ---\n",
    "def compute_precision(true_positives: int, false_positives: int) -> float:\n",
    "    \"\"\"Calcula precision: TP / (TP + FP).\"\"\"\n",
    "    total = true_positives + false_positives\n",
    "    return true_positives / total if total > 0 else 0.0\n",
    "\n",
    "def compute_recall(true_positives: int, false_negatives: int) -> float:\n",
    "    \"\"\"Calcula recall: TP / (TP + FN).\"\"\"\n",
    "    total = true_positives + false_negatives\n",
    "    return true_positives / total if total > 0 else 0.0\n",
    "\n",
    "def compute_f1(precision: float, recall: float) -> float:\n",
    "    \"\"\"Calcula F1 score: 2 * (P * R) / (P + R).\"\"\"\n",
    "    total = precision + recall\n",
    "    return 2 * (precision * recall) / total if total > 0 else 0.0\n",
    "\n",
    "# --- Simulacion de main.py ---\n",
    "def main():\n",
    "    \"\"\"Funcion principal del pipeline de evaluacion.\"\"\"\n",
    "    print(f\"Modelo: {MODEL_VERSION}\")\n",
    "    print(f\"Config: lr={LEARNING_RATE}, batch_size={BATCH_SIZE}\")\n",
    "    print()\n",
    "    \n",
    "    # Resultados de prediccion simulados\n",
    "    tp, fp, fn = 80, 15, 20\n",
    "    \n",
    "    prec = compute_precision(tp, fp)\n",
    "    rec = compute_recall(tp, fn)\n",
    "    f1 = compute_f1(prec, rec)\n",
    "    \n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "# --- El patron if __name__ == \"__main__\" ---\n",
    "# En un notebook, __name__ siempre es \"__main__\"\n",
    "# En un modulo importado, __name__ seria el nombre del modulo\n",
    "print(f\"Valor de __name__ en este notebook: '{__name__}'\")\n",
    "print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Este bloque SOLO se ejecuta si corremos el script directamente.\n",
    "    # Si otro modulo hace 'import este_modulo', este bloque NO se ejecuta.\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. File I/O: Leer y Escribir Datos\n",
    "\n",
    "En un proyecto real de ML, constantemente leemos y escribimos archivos:\n",
    "configuraciones, datasets, logs de entrenamiento, resultados de metricas.\n",
    "\n",
    "### Modos de apertura\n",
    "\n",
    "| Modo | Descripcion | Ejemplo de uso |\n",
    "|---|---|---|\n",
    "| `'r'` | Lectura (texto) | Leer configuracion, datasets CSV |\n",
    "| `'w'` | Escritura (sobreescribe) | Guardar resultados, logs |\n",
    "| `'a'` | Append (agrega al final) | Agregar entradas a un log |\n",
    "| `'rb'` | Lectura binaria | Cargar modelo serializado |\n",
    "| `'wb'` | Escritura binaria | Guardar modelo serializado |\n",
    "\n",
    "### Context managers (`with`)\n",
    "\n",
    "```python\n",
    "# SIEMPRE usar 'with' para manejo automatico de recursos:\n",
    "with open(\"archivo.txt\", \"r\") as f:\n",
    "    contenido = f.read()\n",
    "# El archivo se cierra automaticamente al salir del bloque\n",
    "```\n",
    "\n",
    "> **Regla:** Nunca uses `f = open(...)` sin `with`. Si ocurre un error,\n",
    "> el archivo podria quedar abierto y causar problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Usamos tempfile para no dejar archivos basura en el sistema\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# --- Escritura: guardar resultados de experimentos ---\n",
    "experiment_file = os.path.join(temp_dir, \"experiment_results.csv\")\n",
    "\n",
    "experiment_data = [\n",
    "    {\"epoch\": 1, \"loss\": 2.341, \"accuracy\": 0.45},\n",
    "    {\"epoch\": 2, \"loss\": 1.892, \"accuracy\": 0.58},\n",
    "    {\"epoch\": 3, \"loss\": 1.234, \"accuracy\": 0.71},\n",
    "    {\"epoch\": 4, \"loss\": 0.876, \"accuracy\": 0.79},\n",
    "    {\"epoch\": 5, \"loss\": 0.654, \"accuracy\": 0.84},\n",
    "]\n",
    "\n",
    "print(\"--- Escribiendo datos ---\")\n",
    "with open(experiment_file, \"w\") as f:\n",
    "    # Header\n",
    "    f.write(\"epoch,loss,accuracy\\n\")\n",
    "    # Datos\n",
    "    for record in experiment_data:\n",
    "        line = f\"{record['epoch']},{record['loss']:.3f},{record['accuracy']:.2f}\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "print(f\"  Archivo escrito en: {experiment_file}\")\n",
    "\n",
    "# --- Lectura: cargar y parsear los datos ---\n",
    "print(\"\\n--- Leyendo datos ---\")\n",
    "parsed_records = []\n",
    "\n",
    "with open(experiment_file, \"r\") as f:\n",
    "    header = f.readline().strip().split(\",\")\n",
    "    print(f\"  Columnas: {header}\")\n",
    "    \n",
    "    for line in f:\n",
    "        parts = line.strip().split(\",\")\n",
    "        record = {\n",
    "            \"epoch\": int(parts[0]),\n",
    "            \"loss\": float(parts[1]),\n",
    "            \"accuracy\": float(parts[2]),\n",
    "        }\n",
    "        parsed_records.append(record)\n",
    "\n",
    "# Mostrar los datos parseados\n",
    "print(f\"  Registros cargados: {len(parsed_records)}\")\n",
    "for rec in parsed_records:\n",
    "    status = \"MEJORANDO\" if rec[\"accuracy\"] > 0.7 else \"entrenando...\"\n",
    "    print(f\"  Epoch {rec['epoch']:2d} | Loss: {rec['loss']:.3f} | Acc: {rec['accuracy']:.2f} | {status}\")\n",
    "\n",
    "# --- Append: agregar un nuevo resultado ---\n",
    "print(\"\\n--- Agregando nuevo registro (append) ---\")\n",
    "with open(experiment_file, \"a\") as f:\n",
    "    f.write(\"6,0.512,0.87\\n\")\n",
    "\n",
    "# Verificar que se agrego\n",
    "with open(experiment_file, \"r\") as f:\n",
    "    all_lines = f.readlines()\n",
    "    print(f\"  Total lineas (con header): {len(all_lines)}\")\n",
    "    print(f\"  Ultima linea: {all_lines[-1].strip()}\")\n",
    "\n",
    "# Limpiar archivos temporales\n",
    "os.remove(experiment_file)\n",
    "os.rmdir(temp_dir)\n",
    "print(f\"\\n  Archivos temporales limpiados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Buenas Practicas\n",
    "\n",
    "Escribir codigo que funciona no es suficiente. En equipos de ingenieria de ML,\n",
    "el codigo debe ser **legible, mantenible y testeable**.\n",
    "\n",
    "### Convenciones de naming\n",
    "\n",
    "| Elemento | Convencion | Ejemplo |\n",
    "|---|---|---|\n",
    "| Funciones | `snake_case` | `calculate_accuracy()` |\n",
    "| Variables | `snake_case` | `learning_rate` |\n",
    "| Constantes | `UPPER_SNAKE_CASE` | `MAX_EPOCHS = 100` |\n",
    "| Clases | `PascalCase` | `DataLoader` |\n",
    "| Modulos | `snake_case` | `data_utils.py` |\n",
    "| Variables privadas | `_prefijo` | `_internal_cache` |\n",
    "\n",
    "### Principios fundamentales\n",
    "\n",
    "| Principio | Descripcion | Ejemplo |\n",
    "|---|---|---|\n",
    "| **DRY** | Don't Repeat Yourself | Extraer logica duplicada a una funcion |\n",
    "| **SRP** | Single Responsibility Principle | Cada funcion hace UNA cosa |\n",
    "| **KISS** | Keep It Simple, Stupid | Preferir claridad sobre \"elegancia\" |\n",
    "| **Funciones pequenas** | Max 20-30 lineas | Si es mas larga, dividirla |\n",
    "| **Docstrings** | Documentar el \"por que\" | No el \"que\" (el codigo ya dice eso) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPARACION: Codigo malo vs codigo bueno ---\n",
    "\n",
    "# ============================================================\n",
    "# VERSION MALA: Una funcion monolitica que hace todo\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"VERSION MALA: Funcion monolitica\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def process_everything(raw_data):\n",
    "    \"\"\"Hace DEMASIADAS cosas en una sola funcion.\"\"\"\n",
    "    # Validar\n",
    "    clean = []\n",
    "    for d in raw_data:\n",
    "        if d is not None and isinstance(d, dict) and \"score\" in d and \"name\" in d:\n",
    "            clean.append(d)\n",
    "    # Transformar\n",
    "    for d in clean:\n",
    "        d[\"score_pct\"] = round(d[\"score\"] * 100, 1)\n",
    "        d[\"label\"] = \"APROBADO\" if d[\"score\"] >= 0.6 else \"REPROBADO\"\n",
    "    # Calcular resumen\n",
    "    total = len(clean)\n",
    "    aprobados = len([d for d in clean if d[\"label\"] == \"APROBADO\"])\n",
    "    promedio = sum(d[\"score\"] for d in clean) / total if total > 0 else 0\n",
    "    # Imprimir\n",
    "    for d in clean:\n",
    "        print(f\"  {d['name']}: {d['score_pct']}% -> {d['label']}\")\n",
    "    print(f\"  --- Resumen: {aprobados}/{total} aprobados, promedio={promedio:.2f}\")\n",
    "    return clean\n",
    "\n",
    "datos_crudos = [\n",
    "    {\"name\": \"modelo_A\", \"score\": 0.85},\n",
    "    None,\n",
    "    {\"name\": \"modelo_B\", \"score\": 0.42},\n",
    "    {\"score\": 0.9},  # falta 'name'\n",
    "    {\"name\": \"modelo_C\", \"score\": 0.73},\n",
    "    {\"name\": \"modelo_D\", \"score\": 0.55},\n",
    "]\n",
    "\n",
    "process_everything(datos_crudos)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# VERSION BUENA: Funciones pequenas con responsabilidad unica\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERSION BUENA: Funciones con responsabilidad unica\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "PASSING_THRESHOLD = 0.6\n",
    "\n",
    "def validate_record(record) -> bool:\n",
    "    \"\"\"Verifica que un registro tenga los campos requeridos.\"\"\"\n",
    "    if record is None or not isinstance(record, dict):\n",
    "        return False\n",
    "    required_fields = {\"name\", \"score\"}\n",
    "    return required_fields.issubset(record.keys())\n",
    "\n",
    "\n",
    "def enrich_record(record: dict) -> dict:\n",
    "    \"\"\"Agrega campos calculados a un registro.\"\"\"\n",
    "    return {\n",
    "        **record,\n",
    "        \"score_pct\": round(record[\"score\"] * 100, 1),\n",
    "        \"label\": \"APROBADO\" if record[\"score\"] >= PASSING_THRESHOLD else \"REPROBADO\",\n",
    "    }\n",
    "\n",
    "\n",
    "def summarize(records: list) -> dict:\n",
    "    \"\"\"Calcula estadisticas de resumen.\"\"\"\n",
    "    total = len(records)\n",
    "    aprobados = sum(1 for r in records if r[\"label\"] == \"APROBADO\")\n",
    "    promedio = sum(r[\"score\"] for r in records) / total if total > 0 else 0.0\n",
    "    return {\"total\": total, \"aprobados\": aprobados, \"promedio\": promedio}\n",
    "\n",
    "\n",
    "def display_results(records: list, summary: dict) -> None:\n",
    "    \"\"\"Muestra los resultados formateados.\"\"\"\n",
    "    for r in records:\n",
    "        print(f\"  {r['name']}: {r['score_pct']}% -> {r['label']}\")\n",
    "    print(f\"  --- Resumen: {summary['aprobados']}/{summary['total']} aprobados, \"\n",
    "          f\"promedio={summary['promedio']:.2f}\")\n",
    "\n",
    "\n",
    "# Pipeline claro y legible\n",
    "valid = [r for r in datos_crudos if validate_record(r)]\n",
    "enriched = [enrich_record(r) for r in valid]\n",
    "summary = summarize(enriched)\n",
    "display_results(enriched, summary)\n",
    "\n",
    "print(\"\\n[Ventajas: cada funcion se puede testear, reutilizar y entender por separado]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicios\n",
    "\n",
    "Pon en practica lo aprendido. Cada ejercicio incluye instrucciones detalladas.\n",
    "Completa las funciones reemplazando `pass` con tu implementacion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EJERCICIO 1: Closure - Normalizador configurable\n",
    "# ============================================================\n",
    "# Crea una funcion make_normalizer(min_val, max_val) que retorne\n",
    "# una funcion (closure) capaz de normalizar cualquier valor al\n",
    "# rango [0, 1] usando la formula:\n",
    "#\n",
    "#   normalizado = (valor - min_val) / (max_val - min_val)\n",
    "#\n",
    "# Ejemplo de uso esperado:\n",
    "#   normalizer = make_normalizer(0, 100)\n",
    "#   normalizer(50)   -> 0.5\n",
    "#   normalizer(0)    -> 0.0\n",
    "#   normalizer(100)  -> 1.0\n",
    "#\n",
    "# BONUS: Que pasa si min_val == max_val? Maneja ese caso.\n",
    "# ============================================================\n",
    "\n",
    "def make_normalizer(min_val: float, max_val: float):\n",
    "    \"\"\"Retorna una funcion que normaliza valores al rango [0, 1].\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# --- Tests (descomenta para verificar tu solucion) ---\n",
    "# normalizer = make_normalizer(0, 100)\n",
    "# assert normalizer(50) == 0.5\n",
    "# assert normalizer(0) == 0.0\n",
    "# assert normalizer(100) == 1.0\n",
    "# print(\"Ejercicio 1: OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EJERCICIO 2: Pipeline de procesamiento con funciones pequenas\n",
    "# ============================================================\n",
    "# Dado una lista de registros de experimentos (diccionarios),\n",
    "# implementa 3 funciones con responsabilidad unica:\n",
    "#\n",
    "# 1. validate_experiment(record) -> bool\n",
    "#    Verifica que el registro tenga las claves: \"name\", \"accuracy\", \"loss\"\n",
    "#    y que accuracy este entre 0 y 1, y loss sea >= 0.\n",
    "#\n",
    "# 2. transform_experiment(record) -> dict\n",
    "#    Agrega el campo \"status\":\n",
    "#    - \"excelente\" si accuracy >= 0.9\n",
    "#    - \"bueno\" si accuracy >= 0.7\n",
    "#    - \"necesita_mejora\" en otro caso\n",
    "#\n",
    "# 3. summarize_experiments(records) -> dict\n",
    "#    Retorna {\"total\", \"mejor_modelo\" (nombre con mayor accuracy),\n",
    "#             \"accuracy_promedio\"}\n",
    "#\n",
    "# Luego aplica las 3 funciones en secuencia sobre los datos de prueba.\n",
    "# ============================================================\n",
    "\n",
    "# Datos de prueba\n",
    "raw_experiments = [\n",
    "    {\"name\": \"bert_base\", \"accuracy\": 0.92, \"loss\": 0.21},\n",
    "    {\"name\": \"lstm_v1\", \"accuracy\": 0.75, \"loss\": 0.68},\n",
    "    {\"name\": \"bad_record\"},  # invalido\n",
    "    {\"name\": \"transformer_xl\", \"accuracy\": 0.88, \"loss\": 0.31},\n",
    "    {\"name\": \"logistic_reg\", \"accuracy\": 0.65, \"loss\": 0.89},\n",
    "    {\"name\": \"invalid\", \"accuracy\": 1.5, \"loss\": -0.1},  # invalido\n",
    "]\n",
    "\n",
    "\n",
    "def validate_experiment(record: dict) -> bool:\n",
    "    \"\"\"Valida que un registro de experimento tenga campos correctos.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def transform_experiment(record: dict) -> dict:\n",
    "    \"\"\"Agrega el campo 'status' basado en la accuracy.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def summarize_experiments(records: list) -> dict:\n",
    "    \"\"\"Calcula resumen: total, mejor modelo y accuracy promedio.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# --- Aplica tu pipeline aqui ---\n",
    "# valid = ...\n",
    "# transformed = ...\n",
    "# summary = ...\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# EJERCICIO 3: File I/O con context managers\n",
    "# ============================================================\n",
    "# Implementa 3 funciones para un flujo de lectura-proceso-escritura:\n",
    "#\n",
    "# 1. write_training_log(filepath, records)\n",
    "#    Escribe una lista de dicts [{\"epoch\": 1, \"loss\": 0.5, \"acc\": 0.8}, ...]\n",
    "#    como CSV con header. Usa context manager.\n",
    "#\n",
    "# 2. read_training_log(filepath) -> list[dict]\n",
    "#    Lee el CSV y retorna una lista de dicts con los tipos correctos\n",
    "#    (epoch como int, loss y acc como float). Usa context manager.\n",
    "#\n",
    "# 3. generate_report(records) -> str\n",
    "#    Genera un string con resumen: mejor epoch, peor loss, tendencia\n",
    "#    (mejorando/empeorando basado en si el ultimo loss < primer loss).\n",
    "#\n",
    "# Usa tempfile para las operaciones de archivo.\n",
    "# ============================================================\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "def write_training_log(filepath: str, records: list) -> None:\n",
    "    \"\"\"Escribe registros de entrenamiento como CSV.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_training_log(filepath: str) -> list:\n",
    "    \"\"\"Lee un CSV de entrenamiento y retorna lista de dicts.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_report(records: list) -> str:\n",
    "    \"\"\"Genera un reporte de resumen del entrenamiento.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# --- Datos de prueba ---\n",
    "# training_data = [\n",
    "#     {\"epoch\": 1, \"loss\": 2.50, \"acc\": 0.35},\n",
    "#     {\"epoch\": 2, \"loss\": 1.80, \"acc\": 0.52},\n",
    "#     {\"epoch\": 3, \"loss\": 1.20, \"acc\": 0.68},\n",
    "#     {\"epoch\": 4, \"loss\": 0.75, \"acc\": 0.81},\n",
    "#     {\"epoch\": 5, \"loss\": 0.50, \"acc\": 0.89},\n",
    "# ]\n",
    "#\n",
    "# temp_path = os.path.join(tempfile.mkdtemp(), \"training_log.csv\")\n",
    "# write_training_log(temp_path, training_data)\n",
    "# loaded = read_training_log(temp_path)\n",
    "# report = generate_report(loaded)\n",
    "# print(report)\n",
    "# os.remove(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checklist de consolidacion\n",
    "\n",
    "Antes de avanzar al Notebook 03, verifica que puedes:\n",
    "\n",
    "- [ ] Definir funciones con **type hints**, **docstrings** y **parametros por defecto**\n",
    "- [ ] Explicar la regla **LEGB** y predecir donde Python busca una variable\n",
    "- [ ] Crear y usar **closures** para encapsular configuracion\n",
    "- [ ] Identificar y corregir el **anti-patron de argumentos mutables por defecto**\n",
    "- [ ] Usar `map()`, `filter()`, `sorted()` con funciones como argumento\n",
    "- [ ] Estructurar un proyecto con **modulos** y el patron `if __name__ == \"__main__\"`\n",
    "- [ ] Leer y escribir archivos usando **context managers** (`with open(...)`)\n",
    "- [ ] Aplicar **SRP** (Single Responsibility Principle) para refactorizar funciones largas\n",
    "\n",
    "### Siguiente paso\n",
    "\n",
    "**Notebook 03** cubrira: Programacion Orientada a Objetos, clases, herencia y patrones\n",
    "de diseno aplicados a pipelines de ML.\n",
    "\n",
    "---\n",
    "\n",
    "*Curso de AI Engineering - Henry 2026*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}